---
title: "Modules"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Modules}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>"
)
```

```{r setup, message=FALSE}
library(papercheck)
library(dplyr)
```

Papercheck is designed modularly, so you can add modules to check for anything. It comes with a set of pre-defined modules, and we hope people will share more modules.

## Module List

You can see the list of built-in modules with the function below.

```{r, results='asis'}
module_list()
```

## Built-in Modules

Below, we will demonstrate the use of each built-in module, first on a single paper and then a list of papers, the `psychsci` list of 250 open-access papers from Psychological Science.

```{r}
paper <- psychsci$`0956797620955209`
```


### all_p_values

List all p-values in the text, returning the matched text (e.g., 'p = 0.04') and document location in a table.

```{r}
all_p <- module_run(paper, "all_p_values")

all_p$table # print table
```


If you run this module on all 250 papers, you will get more rows than you probably want to print in the full table one row for every p-value in each paper), so you can print the summary table, which gives you one row per paper.

```{r}
all_p_ps <- module_run(psychsci, "all_p_values")

all_p_ps$summary
```

You can still access the full table for further processing.

```{r}
all_p_ps$table |>
  count(text, sort = TRUE) |>
  head()
```


### all_urls

List all the URLs in the main text. There will, of course, be a few false positives when text in the paper is formatted as a valid URL. 

```{r}
all_urls <- module_run(paper, "all_urls")

all_urls$table
```


```{r}
all_urls_ps <- module_run(psychsci, "all_urls")

all_urls_ps$summary
```

### exact_p

List any p-values that may have been reported with insufficient precision (e.g., p < .05 or p = n.s.). 

```{r}
imprecise <- module_run(paper, "exact_p")

imprecise$table # print table
```

You can use the `expand_text()` function to get the full sentence or paragraph for context. Here you can see that "p < .025" was not an imprecisely reported p-value, but a description of the preregistered alpha threshold. 

```{r}
exp <- expand_text(imprecise$table, paper)

exp$expanded # print expanded text
```

We can investigate the most common imprecise p-values in the PsychSci set. "p < .01" and "p < .05" are probably often describing figures or tables, but what is the deal with "p > .25"?

```{r}
imprecise_ps <- module_run(psychsci, "exact_p")

imprecise_ps$table |>
  count(text, sort = TRUE) |>
  head()
```

We can expand the text to check the context for "p > .25".

```{r}
gt.25 <- imprecise_ps$table |>
  filter(grepl("\\.25", text)) |>
  expand_text(paper = psychsci)

gt.25$expanded[1:3] # look at the first 3
```

### marginal

List all sentences that describe an effect as 'marginally significant'.

```{r, results='asis'}
marginal <- module_run(paper, "marginal")

marginal # print table
```

Let's check how many are in the full set.

```{r}
marginal_ps <- module_run(psychsci, "marginal")

marginal_ps$table # print table
```



### osf_check

List all OSF links and whether they are open, closed, or do not exist. This requires an internet connection to check the status of the link.

```{r}
osf_check <- module_run(paper, "osf_check")

osf_check$summary
```

The returned table contains additional information about the links if they are found and open.

```{r}
osf_check$table[1,] |> t()
```


You can also run this module on a batch of papers. You can only make 100 API requests per hour, unless you authorise your requests, when you can make 10K requests per day. The OSF functions in papercheck often make several requests per URL to get all of the info, so it's worthwhile setting your PAT. You can authorise them by creating an OSF token at <https://osf.io/settings/tokens> and including the following line in your .Renviron file (which you can open using `usethis::edit_r_environ()`):

```
OSF_PAT="replace-with-your-token-string"
```

This would take a while to run for the 333 unique OSF links found in the PsychSci set (usually a few links per second), so we will skip it here (but can reveal we found 10 closed links).

```{r, eval = FALSE}
osf_check_ps <- module_run(psychsci, "osf_check")
```



### ref_consistency

Check if all references are cited and all citations are referenced.

```{r}
ref_consistency <- module_run(paper, "ref_consistency")

ref_consistency$table
```

It looks like there are some references with missing citations. The first one doesn't look like a reference, and grobid often parses tables oddly. You'd need to look at the original PDF to see if the others are actually missing or false positives. Here, they are all false positives, based on grobid not being able to match the in-text citation to the reference list. 


### retractionwatch

Flag any cited papers in the RetractionWatch database. It takes a substantial time to make calls to crossref for each reference, so we provide a summary of the database in papercheck, which is updated regularly (last 2025-02-28). 

```{r}
rw <- module_run(paper, "retractionwatch")

rw$table
```

In the full PsychSci set, we find three papers in the RetractionWatch database cited, one with a retraction and two with corrections.  

```{r}
rw_ps <- module_run(psychsci, "retractionwatch")

rw_ps$table
```

Check the context of the citations in the text column:

```{r}
rw_ps$table$text
```

### statcheck

Check consistency of p-values and test statistics using functions from [statcheck](https://github.com/MicheleNuijten/statcheck).

```{r}
statcheck <- module_run(paper, "statcheck")

statcheck$table
```

Here we see a false positive, where the paper reported the results of an equivalence test, which are meant to be one-tailed, but statcheck did not detect that this was one-tailed.


In the full PsychSci set, there are more than 27K sentences with numbers to check, so this takes about a minute to run. 

```{r, results='asis', eval = FALSE}
statcheck_ps <- module_run(psychsci, "statcheck")
```

```{r, echo = FALSE}
#saveRDS(statcheck_ps, "statcheck_ps.Rds")
statcheck_ps <- readRDS("statcheck_ps.Rds")
```

There will be, of course, some false positives in the full set of `r nrow(statcheck_ps$table)` flagged values. Let's look just at the flagged values where the computed p-value is about double the reported p-value, and this changes the significance decision (at an alpha of 0.05).

```{r}
statcheck_ps$table |>
  filter(decision_error, 
         round(computed_p/reported_p, 1) == 2.0) |>
  select(reported_p, computed_p, raw) |>
  mutate(computed_p = round(computed_p, 4))
```

## Chaining Modules

Modules return a `summary` table as well as the detailed results `table`, which is automatically added to the summary if you chain modules.

```{r}
ps_metascience <- psychsci[1:10] |>
  module_run("all_p_values") |>
  module_run("exact_p") |>
  module_run("marginal")

ps_metascience$summary
```



