<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affective Arousal Links Sound to Meaning</title>
				<funder ref="#_7Cs3QZg">
					<orgName type="full">National Science Foundation Graduate Research Fellowships Program</orgName>
				</funder>
				<funder>
					<orgName type="full">Direct Exchange Program of Freie Universität Berlin</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Arash</forename><surname>Aryani</surname></persName>
							<email>arash.aryani@fu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Education and Psychology</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Cognitive Neuroscience Berlin</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Isbilen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Interacting Minds Centre</orgName>
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Communication and Culture</orgName>
								<orgName type="institution">Aarhus University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Education and Psychology</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
								<address>
									<addrLine>Habelschwerdter Allee 45</addrLine>
									<postCode>D-14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affective Arousal Links Sound to Meaning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62C70C75CD83F7A5DAD81AC1B2462FE4</idno>
					<idno type="DOI">10.1177/0956797620927967</idno>
					<note type="submission">Received 7/12/19; Revision accepted 4/4/20</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>bouba-kiki effect</term>
					<term>arousal</term>
					<term>emotional mediation hypothesis</term>
					<term>language evolution</term>
					<term>affective iconicity</term>
					<term>open data</term>
					<term>open materials</term>
					<term>preregistered</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For most words, the relationship between sound and meaning appears arbitrary: The sound of a word does not typically tell us what it means <ref type="bibr" target="#b16">(De Saussure, 1916)</ref>. A growing body of work, however, has shown that the sounds of words can carry subtle cues about what they refer to (for reviews, see Dingemanse, Blasi, Lupyan, Christiansen, &amp; Monaghan, 2015; Perniss, Thompson, &amp; Vigliocco</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>between auditory and visual brain maps <ref type="bibr" target="#b34">(Ramachandran &amp; Hubbard, 2001)</ref>.</p><p>Given that the bouba-kiki effect shows up early in development <ref type="bibr" target="#b28">(Maurer, Pathman, &amp; Mondloch, 2006;</ref><ref type="bibr" target="#b32">Ozturk et al., 2013)</ref>, exists in radically different cultures <ref type="bibr" target="#b10">(Bremner et al., 2013)</ref>, is implicit <ref type="bibr" target="#b41">(Westbury, 2005</ref>; but see <ref type="bibr" target="#b42">Westbury, 2018)</ref>, and occurs even prior to conscious awareness of the visual stimuli <ref type="bibr" target="#b19">(Heyman, Maerten, Vankrunkelsven, Voorspoels, &amp; Moors, 2019;</ref><ref type="bibr" target="#b21">Hung, Styles, &amp; Hsieh, 2017)</ref>, it is plausible that the mechanism underlying this phenomenon may be fundamental to human experience. Here, we propose that emotional congruence-the similarity in the arousal elicited by auditory and visual stimuli-may mediate the association between shapes and words, with kiki-like words 1 and spiky shapes invoking a higher level of arousal than bouba-like words and rounded shapes.</p><p>Emotion has been implicated in orchestrating and regulating stimulus-response mappings. Accordingly, emotional states need to connect with different types of stimuli across different modalities, suggesting a possible role for emotion in cross-modal associations. The mediating role of affective meaning has already been observed in cross-sensory correspondences: among auditory, visual, and tactile stimuli <ref type="bibr" target="#b27">(Marks, 1975;</ref><ref type="bibr" target="#b40">Walker, Walker, &amp; Francis, 2012)</ref>; between music and color <ref type="bibr" target="#b22">(Isbilen &amp; Krumhansl, 2016)</ref>; and even between more abstract concepts such as vowels and size <ref type="bibr" target="#b5">(Auracher, 2017;</ref><ref type="bibr" target="#b20">Hoshi, Kwon, Akita, &amp; Auracher, 2019)</ref>. Furthermore, when participants are asked to rate the meaning of concepts using scales between bipolar adjectives, such as rounded-angular and soft-hard, most of the variance in ratings could be accounted for by the affective dimensions of valence and arousal <ref type="bibr" target="#b31">(Osgood, 1952)</ref>.</p><p>Within existing affective dimensions <ref type="bibr" target="#b31">(Osgood, 1952)</ref>, arousal seems to be the most well-suited candidate for mediating the bouba-kiki effect, given its dominant role in vocal communication and iconic mappings <ref type="bibr" target="#b1">(Aryani, Conrad, Schmidtke, &amp; Jacobs, 2018)</ref>. At the auditory level of processing, the group of kiki-like words used in previous studies has often involved voiceless plosives (e.g., /k/ and /t/) and short vowels (e.g., /i/), which make words feel more harsh and arousing, as shown at both the behavioral-processing level <ref type="bibr" target="#b1">(Aryani, Conrad, et al., 2018)</ref> and neural-processing level <ref type="bibr" target="#b2">(Aryani, Hsu, &amp; Jacobs, 2018</ref><ref type="bibr" target="#b35">, 2019)</ref>. Similarly, noisy and punctuated sounds as well as sounds with sharp (abrupt) onsets (e.g., kiki, takete) are generally associated with high arousal, hostility, and aggression <ref type="bibr" target="#b29">(Nielsen &amp; Rendall, 2011)</ref>. At the level of visual processing, people generally associate sharp-angled visual objects with negative valence and high arousal <ref type="bibr" target="#b7">(Bar &amp; Neta, 2006)</ref>, which can potentially stem from a similar feeling of threat triggered by the sharpness of the angles. Moreover, using models of geometric patterns, studies of human body shapes and facial expressions suggest that simple sharp shapes (e.g., a V-shaped corner) convey threat, whereas simple round shapes (e.g., an O-shaped element) convey warmth and calmness <ref type="bibr" target="#b0">(Aronoff, Woike, &amp; Hyman, 1992)</ref>. Lastly, in a widely used nonverbal technique for assessing arousal, the Self-Assessment Manikin <ref type="bibr" target="#b9">(Bradley &amp; Lang, 1994)</ref>, people are asked to rate the level of arousal of different types of stimuli by selecting from among a set of manikins that best fits their perceived feeling. The two extreme manikins on the left and on the right of the rating scale intuitively make use of spiky shapes versus rounded shapes to pictorially represent high levels of arousal versus low levels of arousal, respectively (see Fig. <ref type="figure">1</ref>, bottom).</p><p>In sum, we hypothesized that in bouba-kiki experiments, people rely on the arousal evoked by sounds and shapes to match auditory and visual stimuli to one another. To test this, we conducted three experiments. In Experiment 1, we gathered a comprehensive list of the shapes and words used in previous studies examining the bouba-kiki effect. We then collected ratings of arousal for these shapes and words and compared these rating values across rounded-spiky and bouba-kiki categories. In Experiment 2, we generated a new set of pseudowords representative of English phonology. We then developed a model-driven measure of arousal for the previously published words and compared the Ramachandran &amp; Hubbard ( <ref type="formula">2001</ref>) <ref type="bibr" target="#b25">Köhler (1929</ref><ref type="bibr" target="#b25">Köhler ( /1947) )</ref> Takete Maluma</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kiki Bouba</head><p>High Arousal Low Arousal <ref type="bibr" target="#b9">Bradley &amp; Lang (1994)</ref> Fig. <ref type="figure">1</ref>. Examples of mapping between shapes and other modalities.</p><p>The spiky and rounded shapes used in the seminal studies by <ref type="bibr" target="#b25">Köhler (1929</ref><ref type="bibr" target="#b25">Köhler ( /1947) )</ref> and <ref type="bibr" target="#b34">Ramachandran and Hubbard (2001)</ref> are shown in the top and middle rows, respectively. In a nonverbal method for assessing arousal, which was first introduced by <ref type="bibr" target="#b9">Bradley and Lang (1994)</ref>, the two manikins (bottom row) contain similar spiky and rounded shapes to represent the notion of high arousal versus low arousal.</p><p>predicted values across bouba-kiki categories. Finally, to test the generalizability of arousal as a mediating factor, we asked participants in Experiment 3 (preregistered) to match a novel set of pseudowords varying in their arousal with the rounded-spiky shapes from previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>We asked participants to rate the degree of arousal of shapes and words in two separate conditions (shape condition and word condition) to determine whether bouba-like and kiki-like words and the corresponding shapes would yield different levels of arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Two groups of native-English-speaking Cornell University undergraduates participated for course credit. A total of 24 participants (14 women; age: M = 19.5 years, range = 18-21) rated the words and 28 participants (17 women; age: M = 19.7 years, range = 18-22) rated the shapes for affective arousal. The sample size was based on those used in similar rating studies of the affective arousal of words, pseudowords, or pictorial stimuli (18-30 ratings per item; e.g., <ref type="bibr" target="#b1">Aryani, Conrad, et al., 2018;</ref><ref type="bibr" target="#b9">Bradley &amp; Lang, 1994)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>. We selected previous studies investigating the bouba-kiki phenomenon by searching for the keywords "bouba kiki" on Google Scholar (<ref type="url" target="https://scholar.google.com">https://scholar  .google.com</ref>). From these studies, we selected those that used different sets of words and shapes, yielding a total of eight studies that used 29 different pairs of shapes and 45 different pairs of words. A professional male actor who was a native-English speaker recorded the stimuli. Words were spoken in a listlike manner to prevent affective prosody and recorded in a professional sound-recording booth.</p><p>Procedure. We asked participants to rate how exciting or calming each presented image felt (shape condition) or each presented word sounded (word condition), following the instructions of previous studies <ref type="bibr" target="#b1">(Aryani, Conrad, et al., 2018;</ref><ref type="bibr" target="#b9">Bradley &amp; Lang, 1994)</ref>. The order of presentation within the lists of words and shapes was randomized across participants. Importantly, we did not use the Self-Assessment Manikin <ref type="bibr" target="#b9">(Bradley &amp; Lang, 1994)</ref> because the shapes in this instrument could potentially cause bias (see the introduction and Fig. <ref type="figure">1</ref>, bottom). Instead, a 5-point scale was shown on the screen, characterized by five bars of different heights from very calming (1) to very exciting (5). For the words, participants were instructed to give their ratings solely on the basis of the sound of the item and not its potential similarity to real words (for detailed instructions, see Figs. S1 and S2 in the Supplemental Material available online). Note that we used bars of increasing heights to visualize the intensity of arousal from low to high. This choice also provided a neutral counterpart to the Self-Assessment Manikin typically used to measure arousal and thus provides some continuity with that literature.</p><p>Analysis. Differences in the ratings of spiky shapes versus rounded shapes and kiki-like words versus boubalike words were analyzed with t tests using the statistical software JMP Pro 14 (SAS Institute, 2018). For studies with more than one item in a category, we first calculated an average rating for each category and conducted the t test on the basis of the average ratings of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A comparison of the arousal ratings in the shape condition revealed significantly higher arousal for spiky shapes than rounded shapes (spiky: M = 3.45, SE = 0.039; rounded: M = 2.43, SE = 0.037; p &lt; .0001, d = 0.92). We then compared the ratings separately for the materials used in each study. The results showed significant differences in arousal ratings between stimulus types in each study, with spiky shapes rated as having significantly higher arousal than rounded shapes (see Fig. <ref type="figure" target="#fig_0">2a</ref> and Table <ref type="table">S1</ref> in the Supplemental Material).</p><p>In the word condition, we compared arousal ratings for bouba-like and kiki-like words; kiki-like words were rated significantly higher in arousal than bouba-like words (kiki: M = 3.16, SE = 0.033; bouba: M = 2.53, SE = 0.033; p &lt; .0001, d = 0.57). Separate analyses for each of the prior studies showed that kiki-like words always elicited higher mean arousal ratings than boubalike words (see Fig. <ref type="figure" target="#fig_0">2b</ref> and Table <ref type="table">S1</ref>), similar to the shape condition.</p><p>In Experiment 1, the ratings for both shapes and words showed a consistent effect of category on arousal, with spiky shapes and kiki-like words rated higher in arousal than rounded shapes and bouba-like words across all of the studies. We took this as initial support for our hypothesis, showing that the typical shapes and words used in previous studies possess similar levels of arousal over different sensory modalities (i.e., visual vs. auditory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>In Experiment 2, we asked whether any wordlike stimulus can convey emotional information on the basis of the basic perceptual features of its sounds. Specifically, we developed a model-driven measure of the level of arousal in the sound of bouba-kiki words that can overcome some of the typical issues related to direct subjective judgments in Experiment 1 (e.g., familiarity of items).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>A new group of participants rated a large set of novel auditorily presented pseudowords for their level of arousal. We then extracted the acoustic features of these pseudowords to develop acoustic models capable of predicting the variation in the ratings. These models were applied to the word stimuli from Experiment 1 to predict their level of arousal solely on the basis of their acoustic features.</p><p>Participants. A group of native-English-speaking Cornell University undergraduates participated for course credit. This sample of participants was divided into two groups to develop two independent acoustic models to test the reliability of the results. Thus, we randomly assigned each of the 70 participants (53 women; age: M = 18.8 years, range = 17-22) to one of two groups of equal sizes (n = 35 each). The sample size was calculated on the basis of the expected effect size from a similar previous study <ref type="bibr" target="#b1">(Aryani, Conrad, et al., 2018</ref>; minimum R 2 = 52.6%, d = 0.72) and assuming a power of 80%, resulting in a sample size of 32 participants per condition.</p><p>Materials. To generate novel words representative of the phonotactics of English, we used the Wuggy algorithm <ref type="bibr" target="#b24">(Keuleers &amp; Brysbaert, 2010)</ref>. Wuggy generates pseudowords on the basis of the syllabic structure (i.e., onset, nucleus, and coda) of a given real word, thereby producing combinations of phonemes that are permissible in a given language. We chose the first 1,500 most frequent one-and two-syllable nouns from the English CELEX database <ref type="bibr" target="#b6">(Baayen, Piepenbrock, &amp; Gulikers, 1995)</ref> and adapted the program to generate five alternatives for each word. Candidate pseudowords that differed by fewer than two letters (added, deleted, or substituted) from the nearest real word were excluded (Coltheart distance = 1). For words with more than one remaining alternative, the one with the highest Levenshtein distance (i.e., the minimum number of insertions, deletions, or substitutions of a letter required to change the pseudoword into a real word) was selected. The list of words was checked for pseudohomophones, rare phonetic combinations, and similarity to real words by a native-English speaker. This resulted in 940 words, which were recorded as in Experiment 1.</p><p>Procedure. The procedure was the same as in Experiment 1. We asked participants to evaluate 940 randomly presented pseudowords for how arousing they sounded (1 = very calming, 3 = neutral, 5 = very exciting). Ten practice items were presented at the beginning of the experimental session to familiarize participants with the task. The rating instructions were the same as those used in the word condition of Experiment 1 to elicit arousal ratings for previously published pseudowords. During a second inspection round (after collecting ratings), 11 pseudowords were identified as trisyllabic and excluded from subsequent analyses, resulting in 929 pseudowords (321 monosyllabic and 608 bisyllabic) in total.</p><p>Next, we extracted the acoustic features of these words and developed acoustic models that used these features to predict the variation in arousal ratings (cf. <ref type="bibr" target="#b1">Aryani, Conrad, et al., 2018)</ref>. We used the speech-analysis software Praat (Version 6.0.25; <ref type="bibr" target="#b8">Boersma &amp; Weenik, 2017)</ref> for the acoustic modeling. For each word, we extracted 14 acoustic features known to modulate emotional vocalization <ref type="bibr" target="#b1">(Aryani, Conrad, et al., 2018;</ref><ref type="bibr" target="#b23">Juslin &amp; Laukka, 2003)</ref>: the mean of fundamental frequency f0 (time step = .01, minimum = 75 Hz, maximum = 300 Hz); the mean, the minimum, the maximum, the median, and the standard deviation of intensity (time step = .01); and the mean and the bandwidth of the first three formants, F1, F2, and F3 (time step = .01), from the spectral representation of the sound. Finally, the spectral centroid (spectral center of gravity) and the standard deviation of the spectrum were computed on the basis of fast Fourier transformations (time step = .01, minimum pitch = 75 Hz, maximum pitch = 300 Hz).</p><p>Analysis. For the regression models, standard least squares was chosen as the fitting method using the statistical software JMP Pro 14 (SAS Institute, 2018). We then applied these acoustic models to our list of 45 pairs of bouba-like and kiki-like words from the previous studies in Experiment 1 and predicted their affective arousal solely on the basis of their acoustic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Across two groups of participants (Group 1 and Group 2), average arousal ratings for the entire set of words were highly correlated, r = .73, p &lt; .0001. The results of linear regression models using the acoustic features to predict the arousal ratings showed that the acoustic features could account for 24.0% (for Group 1) and 22.9% (for Group 2) of the variance in arousal ratings (both R 2 adjusted, ps &lt; .0001; for detailed model information, see Fig. <ref type="figure" target="#fig_1">S3</ref> in the Supplemental Material). In both models, sound intensity and its related properties (e.g., minimum, maximum) were highly significant, replicating previous results on the dominant role of intensity in the arousal of different types of sounds <ref type="bibr" target="#b1">(Aryani, Conrad, et al., 2018;</ref><ref type="bibr" target="#b23">Juslin &amp; Laukka, 2003)</ref>. We next took the acoustic models (i.e., the linear equations) and applied them to the extracted acoustic features of the bouba-like and kiki-like words to predict their arousal. We will refer to these two predicted values for arousal of bouba-like and kiki-like words as "predicted arousal based on G1 and G2" (PAG1 and PAG2, respectively).</p><p>A comparison of the predicted values of arousal of the set of bouba-like and kiki-like words from previous studies revealed significantly higher arousal for kiki-like words than bouba-like words, for both PAG1 (kiki: M = 3.07, SE = 0.023; bouba: M = 2.90, SE = 0.020), t(88) = 5.91, p &lt; .0001, d = 1.24, and PAG2 (kiki: M = 3.03, SE = 0.021; bouba: M = 2.85, SE = 0.019), t(88) = 6.17, p &lt; .0001, d = 1.36. Results of single comparisons across each of the eight selected studies showed that for both PAG1 and PAG2, the predicted values for the arousal of kiki-like words were consistently higher than those of bouba-like words in all of the studies (for Group 1 and Group 2 average ratings, see Fig. <ref type="figure" target="#fig_0">2c</ref>). Because of the small number of bouba-like and kiki-like words in most of the studies, a statistical test of significance comparing the mean values at the item level within each study was not always possible (note that this was not an issue in Experiment 1, in which the analyses were at the participant level). Instead, we tested a null hypothesis that the directions of these single comparisons (i.e., kiki &gt; bouba vs. bouba &gt; kiki) were determined by chance. Thus, we considered the success or failure of the results being in the direction expected by our hypothesis as several Bernoulli trials, with the words in each study being independent events with a probability of success (i.e., kiki &gt; bouba) equal to chance (p = .50). Comparisons across all of the studies (see Table <ref type="table">S2</ref> in the Supplemental Material) showed that the predicted value of arousal for kiki-like words was higher than that for bouba-like words in all eight cases. Corresponding binomial tests thus rejected the null hypothesis: X ~ B (8, .5), p(X ≥ 8) = .003 for PAG1, and X ~ B (8, .5), p(X ≥ 8) = .003 for PAG2.</p><p>By showing that the predicted values for arousal significantly differ across kiki-like and bouba-like words, Experiment 2 provided further support for the hypothesis that arousal may mediate the bouba-kiki effect. Crucially, the present findings go beyond the limited selection of bouba-kiki words used in previous studies: Any wordlike stimulus can potentially convey emotional information solely on the basis of its perceptual acoustic characteristics (cf. <ref type="bibr" target="#b1">Aryani, Conrad, et al., 2018;</ref><ref type="bibr" target="#b2">Aryani, Hsu, &amp; Jacobs, 2018;</ref><ref type="bibr">Aryani &amp; Jacobs, 2018)</ref>, making it possible to match it with emotionally similar concepts (e.g., shapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3</head><p>To further establish the generalizability of the emotionalmediation hypothesis, we sought to replicate the classic bouba-kiki effect with a new set of words in a preregistered study (<ref type="url" target="https://aspredicted.org/mq97g.pdf">https://aspredicted.org/mq97g.pdf</ref>). We hypothesized that any word (or pseudoword) would be matched with spiky shapes versus rounded shapes depending on the level of arousal in its sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. A group of 64 native-English-speaking Cornell University undergraduates participated for course credit (49 women; age: M = 19.2 years, range = 18-22). Assuming an effect size (d) of 0.5 (a medium effect), this sample size provides adequate power of 80%.</p><p>Materials. On the basis of the ratings collected in Experiment 2, we selected 168 words and assigned them to three distinct groups of high arousal (&gt; 3.12), medium arousal (2.87-3.01), and low arousal (&lt; 2.75; see Table <ref type="table">S3</ref> in the Supplemental Material). The number of selected words was motivated by the number of available spiky and rounded shape pairs from previous studies (N = 28) and was chosen so that each pair would be presented an equal number of times across the experiment. Because of an existing confound between arousal-rating value and both the number of phonemes and the number of syllables in the words (r = .54, p &lt; .0001, and r = .53, p &lt; .0001, respectively), we controlled the number of phonemes across the three arousal categories (see Table <ref type="table">S3</ref>) and selected the same number of monosyllabic words (n = 16) and bisyllabic words (n = 40) for each of these categories.</p><p>Procedure. The order of presentation of the 168 words was randomized for each participant. During each presentation, a pair of spiky or rounded shapes from previously published studies was displayed. The position of each shape was counterbalanced; each shape in a pair appeared thrice throughout the course of the experiment on either side of the screen. Participants were given a two-alternative forced-choice task in which they used the keyboard to indicate which of the shapes best fitted a word they had heard.</p><p>Analysis. As was preregistered, responses that took longer than 5 s were excluded from analysis (230 of 10,752), resulting in 10,522 responses that were analyzed. We fitted a logistic generalized linear mixed model to our data set using the glmmTMB package <ref type="bibr" target="#b11">(Brooks et al., 2017)</ref> in the R software environment (Version 3.6.1; <ref type="bibr" target="#b35">R Core Team, 2019)</ref>. We modeled response to shapes as a binary outcome (1 = spiky, 0 = rounded), word category (high, medium, low) as the predictor, and subject and word as random effects, and we used the binomial link function (the entire R code is available at <ref type="url" target="https://osf.io/a8sd4">https://osf  .io/a8sd4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In line with our hypothesis, results showed that the average probability of selecting a spiky shape over a rounded shape (i.e., reporting a response of 1) was significantly higher for the words in the high-arousal group than for those in the medium-arousal group, which was, in turn, higher than those in the low-arousal group. A summary of the actual results is shown in Figure <ref type="figure" target="#fig_1">3</ref> (for a summary of the results of the model output and the results of pairwise comparisons between each pair of word groups, see Tables <ref type="table">S4</ref> and <ref type="table">S5</ref> in the Supplemental Material).</p><p>These results suggest that the extent to which a word in a bouba-kiki experiment is matched with a rounded shape or spiky shape depends on the level of arousal elicited by its sound. Importantly, the results generalize the mediating role of arousal to a new set of pseudowords and show that varying one factor in the characteristics of stimuli (i.e., arousal) in one modality (i.e., auditory words) can significantly impact the outcome of matching preferences in the other modality (i.e., visual images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>The present results substantiate the emotional-mediation hypothesis (e.g., <ref type="bibr" target="#b27">Marks, 1975;</ref><ref type="bibr" target="#b40">Walker et al., 2012)</ref> in the bouba-kiki paradigm, suggesting that information from different sensory domains can be linked through shared emotional associations. Our findings show that the tendency to match pseudowords with spiky shapes versus rounded shapes is associated with the level of arousal in their sound.</p><p>Although our data do not permit a direct conclusion about the causal role of arousal in driving the boubakiki effect, many of the effect's characteristics, such as its prevalence across different age groups (e.g., <ref type="bibr" target="#b32">Ozturk et al., 2013)</ref> and cultures (e.g., <ref type="bibr" target="#b10">Bremner et al., 2013)</ref>, can be explained by the emotional-mediation hypothesis. Particularly, recent findings on the occurrence of the effect even in the absence of visual awareness <ref type="bibr" target="#b19">(Heyman et al., 2019;</ref><ref type="bibr" target="#b21">Hung et al., 2017)</ref> suggest the involvement of automatic processes that bypass higher order cognitive analysis. This type of stimulus-response processing has been frequently reported for highly emotional stimuli; for instance, individuals with cortical blindness show spontaneous facial and pupillary reactions to nonconsciously perceived expressions of fear and happiness <ref type="bibr" target="#b38">(Tamietto et al., 2009)</ref>. Although further studies are needed to determine the causal role of emotion in cross-modal mappings, our data suggest that affective arousal serves as a potent mediator between these domains.</p><p>Our results may also provide an answer to the problem of grounding abstract concepts during development. Despite the valuable insights of previous investigations, it is still a matter of debate as to how abstract sound categories (i.e., phonemes) can be linked to abstract shape forms (e.g., spikiness) merely on the basis of the physical characteristics of the stimuli. In line with theories emphasizing the role of affect in the representation of abstract words <ref type="bibr" target="#b26">(Kousta, Vigliocco, Vinson, Andrews, &amp; Del Campo, 2011;</ref><ref type="bibr" target="#b39">Vigliocco, Meteyard, Andrews, &amp; Kousta, 2009)</ref>, our results suggest that the lack of direct sensorimotor information in an abstract concept may in part be compensated for by its affective relevance, which may help build links between specific abstract concepts and specific affective states. Accordingly, an abstract concept such as spikiness may be grounded in the affective system by the embodied experience of harmfulness and, hence, high arousal. Thus, learning to build abstract categories on the basis of their emotional relevance may be a crucial stepping stone in the development of abstract semantic representations during early word learning.</p><p>The present findings may moreover shed new light on the early stages of language evolution and the emergence of linguistic signs by highlighting the role of arousal. The expression and perception of affective states are fundamental aspects of human communication and, since <ref type="bibr" target="#b14">Darwin (1871)</ref>, have been viewed as a key impetus for language evolution. At the level of vocal expressions, arousal can be readily reflected in human vocalizations and thus extends to acoustic features of the speech signal. At the perception level, humans can reliably identify levels of arousal in the vocalizations of other humans <ref type="bibr" target="#b23">( Juslin &amp; Laukka, 2003)</ref> as well as in diverse vertebrate species <ref type="bibr" target="#b18">(Filippi et al., 2017)</ref>. Thus, an initial form of vocalization may have been a motor reflection of arousal in the vocal tract. Such vocalizations might gradually have been used to refer to external objects that were associated with similar affective experience (e.g., sharp rocks becoming associated with high-arousal sounds). Grounding linguistic signs in the affective system may thereby have provided human verbal communication with an essential building block that is easily learnable and, thus, likely to be retained over the course of the cultural evolution of language <ref type="bibr" target="#b12">(Christiansen &amp; Chater, 2008)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results from Experiments 1 and 2. For Experiment 1, arousal ratings are shown for each set of stimuli in the (a) shape condition and (b) word condition, separately for each of the eight previous studies. For Experiment 2 (c), model-driven predicted levels of arousal are shown for each word used in the previous studies. Asterisks indicate significant differences between stimuli (*p &lt; .05, **p &lt; .01, ***p &lt; .001). Error bars in (a) and (b) represent standard errors of the mean. The studies are referred to by the first author's initial: C = Cuskley, Simner, and Kirby (2017); D = Davis (1961); K = Köhler (1929/1947); M = Maurer, Pathman, and Mondloch (2006); N = Nielsen and Rendall (2011); O = Occelli, Esposito, Venuti, Arduino, and Zampini (2013); R = Ramachandran and Hubbard (2001); W = Westbury (2005).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results from the two-alternative forced-choice task in Experiment 3: average probability of matching a pseudoword with a spiky shape or a rounded shape as a function of the level of arousal in its sound. Blue dots represent individual data points, and red dots represent group means. The width of the gray plots indicates the density of the data. The vertical red lines indicate standard errors of the mean.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are grateful to <rs type="person">Felix Thoemmes</rs> for guidance on the statistical analyses and to <rs type="person">Stewart McCauley</rs> for generating an earlier version of the pseudowords. We also thank <rs type="person">Dante Dahabreh</rs>, <rs type="person">Emma Goldenthal</rs>, <rs type="person">Phoebe Ilevebare</rs>, <rs type="person">Eleni Kohilakis</rs>, <rs type="person">Jake Kolenda</rs>, <rs type="person">Farrah Mawani</rs>, <rs type="person">Jeanne Powell</rs>, and <rs type="person">Olivia Wang</rs> for their help in preparing the stimuli and conducting the experiments. We especially thank <rs type="person">Emma Goldenthal</rs> and <rs type="person">Eleni Kohilakis</rs> for their help with the International Phonetic Alphabet transcriptions of the stimuli from Experiment 2.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research was funded by grants to <rs type="person">A. Aryani</rs> from the <rs type="funder">Direct Exchange Program of Freie Universität Berlin</rs> and to E. S. Isbilen from the <rs type="funder">National Science Foundation Graduate Research Fellowships Program</rs> (No. <rs type="grantNumber">DGE-1650441</rs>).</p></div>
<div><head>Open Practices</head><p>All data and stimuli have been made publicly available via the Open Science Framework and can be accessed at <ref type="url" target="https://osf.io/a8sd4">https://osf.io/a8sd4</ref>. The design and analysis plans were preregistered at <ref type="url" target="https://aspredicted.org/mq97g.pdf">https://aspredicted.org/mq97g.pdf</ref>. The complete Open Practices Disclosure for this article can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/0956797620927967">http://journals.sagepub.com/doi/suppl/10.1177/  0956797620927967</ref>. This article has received the badges for Open Data, Open Materials, and Preregistration. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psychologicalscience.org/publications/  badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7Cs3QZg">
					<idno type="grant-number">DGE-1650441</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/0956797620927967">http://  journals.sagepub.com/doi/suppl/10.1177/0956797620927967</ref> </p><p>Note 1. Note that we use the term word to refer to both words with lexical meaning and pseudowords without lexical meaning (e.g., kiki, bouba).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Which are the stimuli in facial displays of anger and happiness? Configurational bases of emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aronoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Woike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1050" to="1066" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why &apos;piss&apos; is ruder than &apos;pee&apos;? The role of sound in affective meaning making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aryani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0198430</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">198430</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The sound of words evokes affective brain responses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aryani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.3390/brainsci8060094</idno>
	</analytic>
	<monogr>
		<title level="j">Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Affective iconic words benefit from additional sound-meaning integration in the left amygdala</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aryani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbm.24772</idno>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="5289" to="5300" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Affective congruence between sound and meaning of words facilitates semantic decision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aryani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.3390/bs8060056</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sound iconicity of abstract concepts: Place of articulation is implicitly associated with abstract concepts of size and social dominance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Auracher</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0187196</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">187196</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The CELEX lexical database [CD-ROM</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gulikers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Humans prefer curved visual objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="645" to="648" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Praat: Doing phonetics by computer (Version 6.0.25</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenik</surname></persName>
		</author>
		<ptr target="http://www.praat.org/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring emotion: The Self-Assessment Manikin and the semantic differential</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bouba&quot; and &quot;Kiki&quot; in Namibia? A remote culture make similar shape-sound matches, but different shape-taste matches to Westerners</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caparos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Fockert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Linnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2012.09.007</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="165" to="172" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kristensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Van Benthem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The R Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="378" to="400" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language as shaped by the brain</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X08004998</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral &amp; Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Phonological and orthographic influences in the bouba-kiki effect</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cuskley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirby</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-015-0709-2</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<title level="m">The descent of man and selection in relation to sex</title>
		<meeting><address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<publisher>John Murray</publisher>
			<date type="published" when="1871">1871</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The fitness of names to drawings. A crosscultural study in Tanganyika</title>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>De Saussure</surname></persName>
		</author>
		<title level="m">Course in general linguistics</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1916">1916</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arbitrariness, iconicity, and systematicity in language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dingemanse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Monaghan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2015.07.013</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Humans recognize emotional arousal in vocalizations across all classes of terrestrial vertebrates: Evidence for acoustic universals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Congdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Reber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pašukonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Güntürkün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2017.0990</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<date type="published" when="1859">2017. 1859</date>
		</imprint>
	</monogr>
	<note>Article 20170990</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sound-symbolism effects in the absence of awareness: A replication study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Maerten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vankrunkelsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Voorspoels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moors</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797619875482</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1638" to="1647" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic associations dominate over perceptual associations in vowel-size iconicity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Akita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Auracher</surname></persName>
		</author>
		<idno type="DOI">10.1177/2041669519861981</idno>
	</analytic>
	<monogr>
		<title level="j">i-Perception</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Can a word sound like a shape before you have seen it? Sound-shape mapping prior to conscious awareness</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Styles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The color of music: Emotion-mediated associations to Bach&apos;s Well-Tempered Clavier</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Isbilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Krumhansl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychomusicology: Music, Mind, &amp; Brain</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Communication of emotions in vocal expression and music performance: Different channels, same code?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.129.5.770</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="770" to="814" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wuggy: A multilingual pseudoword generator</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keuleers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<idno type="DOI">10.3758/BRM.42.3.627</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gestalt psychology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Köhler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1929">1947. 1929</date>
			<publisher>Liveright</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Original work</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The representation of abstract words: Why emotion matters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kousta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Del Campo</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0021446</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="14" to="34" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On colored-hearing synesthesia: Crossmodal translations of sensory dimensions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="303" to="331" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The shape of boubas: Sound-shape correspondences in toddlers and adults</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pathman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-7687.2006.00495.x</idno>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="316" to="322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The sound of round: Evaluating the sound-symbolic role of consonants in the classic Takete-Maluma phenomenon</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rendall</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0022268</idno>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="115" to="124" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Takete-Maluma phenomenon in autism spectrum disorders</title>
		<author>
			<persName><forename type="first">V</forename><surname>Occelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Arduino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zampini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="233" to="241" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The nature and measurement of meaning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Osgood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="197" to="237" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sound symbolism in infancy: Evidence for sound-shape crossmodal correspondences in 4-month-olds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ozturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vouloumanos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jecp.2012.05.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="173" to="186" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iconicity as a general property of language: Evidence from spoken and signed languages</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2010.00227</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">227</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Synaesthesia-A window into perception, thought and language</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consciousness Studies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>R Foundation for Statistical Computing</publisher>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m">JMP Pro 14</title>
		<meeting><address><addrLine>Cary, NC</addrLine></address></meeting>
		<imprint>
			<publisher>Author</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>SAS Institute</orgName>
		</respStmt>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crossmodal correspondences: A tutorial review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-010-0073-7</idno>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="971" to="995" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unseen facial and bodily expressions trigger fast emotional reactions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tamietto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vighetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geminiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weiskrantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="17661" to="17666" />
			<date type="published" when="2009">2009</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Toward a theory of semantic representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meteyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kousta</surname></persName>
		</author>
		<idno type="DOI">10.1515/LANGCOG.2009.011</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Cognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="219" to="247" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A common scheme for cross-sensory correspondences across stimulus domains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Francis</surname></persName>
		</author>
		<idno type="DOI">10.1068/p7149</idno>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1186" to="1192" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Implicit sound symbolism in lexical access: Evidence from an interference task</title>
		<author>
			<persName><forename type="first">C</forename><surname>Westbury</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2004.07.006</idno>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Implicit sound symbolism effect in lexical access, revisited: A requiem for the interference task paradigm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Westbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Articles in Support of the Null Hypothesis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
