<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI Hyperrealism: Why AI Faces Are Perceived as More Real Than Human Ones</title>
				<funder ref="#_FWFUPmJ">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Experimental Psychology Society Small</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian National University College of Health and Medicine</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elizabeth</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><forename type="middle">A</forename><surname>Steward</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zak</forename><surname>Witkower</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clare</forename><forename type="middle">A M</forename><surname>Sutherland</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution" key="instit1">King&apos;s College</orgName>
								<orgName type="institution" key="instit2">University of Aberdeen</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Psychological Science</orgName>
								<orgName type="institution">University of Western Australia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eva</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Amy</forename><surname>Dawel</surname></persName>
							<email>amy.dawel@anu.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AI Hyperrealism: Why AI Faces Are Perceived as More Real Than Human Ones</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A110D4C30A69BB82B5C05F66F249221</idno>
					<idno type="DOI">10.1177/09567976231207095</idno>
					<note type="submission">Received 4/25/23; Revision accepted 9/19/23</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence</term>
					<term>face perception</term>
					<term>face-space theory</term>
					<term>generative adversarial network</term>
					<term>StyleGAN2</term>
					<term>open data</term>
					<term>open materials</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces-a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dawn of the artificial intelligence (AI) revolution has marked an unprecedented societal shift <ref type="bibr" target="#b40">(Xie, 2023)</ref>. Prominent in this shift is the generation of realistic humanlike AI faces, twinned with public concern that AI might distort the perception of truth <ref type="bibr" target="#b8">(Devlin, 2023)</ref>. AI-generated faces are now widely available (e.g., thisperson-does-not-exist.com) and are being used for both prosocial and nefarious purposes, from finding missing children <ref type="bibr" target="#b6">(Chandaliya &amp; Nain, 2022)</ref> to transmitting political misinformation via fake social media accounts (e.g., <ref type="bibr" target="#b12">Hatmaker, 2020)</ref>. AI faces are now so realistic that people often fail to detect they are not human (e.g., <ref type="bibr" target="#b23">Nightingale &amp; Farid, 2022)</ref>. However, because this technology has advanced so rapidly <ref type="bibr" target="#b11">(Hao, 2021)</ref>, there have been few empirical tests of this ability. Here we argue that AI faces are not just indistinguishable from human faces but that, in fact, they may be perceived as more "human" than real human faces. We term this striking and counterintuitive phenomenon hyperrealism. The present research aimed to test for and explain AI hyperrealism, investigate whether people have insight into their AI detection errors, and discover visual attributes that can be used to reveal AI imposters.</p><p>Psychology offers decades of theoretical and empirical work with potential to explain AI hyperrealism. For example, the influential face-space theory (Fig. <ref type="figure" target="#fig_0">1</ref>; <ref type="bibr" target="#b36">Valentine, 1991;</ref><ref type="bibr" target="#b37">Valentine et al., 2016)</ref> proposes a hypothetical multidimensional space in which faces are coded along unspecified dimensions according to how much they differ from an average face located at the center. Human faces are supposed to be normally distributed within this space in such a way that more average features (for all dimensions) are statistically overrepresented. This bias toward average features that generative algorithms are trained on (e.g., StyleGAN2 for faces; <ref type="bibr" target="#b14">Karras et al., 2020</ref><ref type="bibr" target="#b13">Karras et al., , 2021</ref>) may be further exaggerated in the AI faces they generate, as these algorithms are biased toward the most common statistical properties of their training data <ref type="bibr" target="#b9">(Grossman et al., 2019)</ref>. Although the specific dimensions of face-space are unknown, it is possible to measure the relative location of faces indirectly via the emergent perceptual attributes of face-space, such as facial averageness. Thus, we hypothesized that StyleGAN2-generated faces would embody the attributes of average faces to a greater extent than real human faces.</p><p>A psychological analysis of AI representativeness can also help with understanding a puzzle arising from the handful of studies that have investigated people's ability to detect AI faces: Although one recent study found that people were unable to distinguish AI from human faces <ref type="bibr" target="#b23">(Nightingale &amp; Farid, 2022)</ref>, two others go further to suggest that people may overidentify AI faces as human <ref type="bibr" target="#b32">(Shen et al., 2021;</ref><ref type="bibr" target="#b35">Tucciarelli et al., 2022)</ref>. How can we explain this puzzle? All three studies used the StyleGAN2 algorithm but varied in the race of the faces they tested. These demographic differentials are critical because StyleGAN2 was trained on primarily White faces (~69% White, ~31% for all other races combined; see Supplemental File S1 in the Supplemental Material available online), potentially biasing the algorithm toward the statistical regularities of White faces. This bias may lead to White AI faces that appear especially average (indicated in Fig. <ref type="figure" target="#fig_0">1</ref>) and therefore, potentially, especially realistic. Consistent with this theory, Shen et al. ( <ref type="formula">2021</ref>) and <ref type="bibr" target="#b35">Tucciarelli et al. (2022)</ref> found preliminary evidence of AI hyperrealism to the extent they tested White faces, although <ref type="bibr" target="#b35">Tucciarelli et al.'s (2022)</ref> stimuli were also preselected to be particularly realistic, biasing them toward this finding. Intriguingly, <ref type="bibr" target="#b23">Nightingale and Farid (2022)</ref> also reported more errors for White than non-White AI face detection. However, they did not pursue this question further. If AI faces do appear more realistic for White faces than other groups, their use will confound perceptions of race with perceptions of being "human." Thus, the use of popular StyleGAN2 faces may risk misleading scientific conclusions <ref type="bibr" target="#b7">(Dawel et al., 2022)</ref> and may even perpetuate social biases in real-world outcomes, from influencing elections to finding missing children <ref type="bibr" target="#b6">(Chandaliya &amp; Nain, 2022;</ref><ref type="bibr" target="#b12">Hatmaker, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Present Research</head><p>Here we aimed to investigate the potential for AI hyperrealism and provide the first test of whether people Distinctive faces are coded further out on face-space dimensions; stand out from crowd due to lower face density, so more memorable.</p><p>Average faces cluster densely around center; more confusable with neighbors so less memorable; but more familiar and attractive; maybe Al faces are all average-ish?  <ref type="bibr">(2018), and</ref><ref type="bibr" target="#b37">Valentine et al. (2016)</ref>. For psychophysics-related work, see <ref type="bibr" target="#b0">Abudarham and Yovel (2016)</ref> and <ref type="bibr" target="#b30">Rhodes and Jeffery (2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>Artificial intelligence, or AI, can now generate faces that are indistinguishable from human faces. However, AI algorithms tend to be trained using a disproportionate number of White faces. As a result, AI faces may appear especially realistic when they are White. Here, we show that White (but not non-White) AI faces are, remarkably, judged as human more often than pictures of actual humans. We pinpoint the perceptual qualities of faces that contribute to this hyperrealism phenomenon, including facial proportions, familiarity, and memorability. Problematically, the people who were most likely to be fooled by AI faces were the least likely to detect that they were being fooled. Our results explain why AI hyperrealism occurs and show that not all AI faces appear equally realistic, with implications for proliferating social bias and for public misidentification of AI.</p><p>have insight into their AI detection errors. If people mistake AI faces as human but have low confidence in their judgment, they may respond more cautiously (e.g., investigating an online profile). However, if they are convinced their judgment is correct, their errors may be more consequential (e.g., falling for a fraudulent profile). Although <ref type="bibr" target="#b35">Tucciarelli et al. (2022)</ref> found confidence was higher for judgments of AI than for human faces overall, it is currently unknown whether people are aware of their AI detection errors. Errors are associated with lower confidence for other face judgments (e.g., face identity recognition, <ref type="bibr" target="#b25">Palermo et al., 2017;</ref><ref type="bibr">eyewitness identification, Wixted &amp; Wells, 2017)</ref>. Thus, we predicted that poorer AI detection would be associated with lower confidence.</p><p>We also aimed to identify visual attributes that distinguish AI from human faces and address the critical unanswered question of why people fail to detect AI faces. Our theorizing suggests that the emergent perceptual attributes of face-space-such as facial averageness, memorability, attractiveness, and familiarity-may play a role, given their importance for human face perception <ref type="bibr" target="#b37">(Valentine et al., 2016;</ref><ref type="bibr" target="#b38">Vokey &amp; Read, 1992)</ref>. Because little is known about which cues people use for AI detection, we augmented this theoretical perspective with a data-driven approach by asking participants what information they used to guide their judgments.</p><p>Reanalysis of <ref type="bibr" target="#b23">Nightingale and Farid (2022)</ref> We started with a proof-of-principle by reanalyzing data from a prominent recent study that included information about face race <ref type="bibr">(Nightingale &amp; Farid, 2022, Experiment 1)</ref> to investigate the potential for AI hyperrealism. Analyses showed clear evidence of AI hyperrealism for White faces, but not for non-White faces. Figure <ref type="figure" target="#fig_2">2a</ref> shows that White AI faces were judged as human significantly more often than White human faces, M White-AI = 69.5% versus M White-human = 52.2%, t(314) = 13.25, p &lt; .001, d = 0.75, 95% confidence interval (CI) = [0.62, 0.87], and significantly more often than chance (= 50% in the two-alternative forced choice task), t(314) = 16.01, p &lt; .001, d = 0.90, 95% CI = [0.77, 1.03]. In contrast, non-White AI faces (left side of Fig. <ref type="figure" target="#fig_2">2a</ref>) were judged as human at around chance levels, M non-White-AI = 50.5%, t(314) = 0.41, p = .682, d = 0.02, 95% CI = [-0.09, 0.13], which did not differ significantly from how often non-White human faces were judged to be human, versus M non-White-human = 51.3%, t(314) = 0.74, p = .461, d = 0.04, 95% CI = [-0.07, 0.15]. Unusually, d ′ -a measure of people's ability to discriminate between AI and human faces that is unaffected by response bias-was also significantly negative for White faces, M = -.59, t(314) = 13.17  result indicates that participants did discriminate between White AI and human faces, but in the wrong direction, providing clear evidence of AI hyperrealism for White faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>To investigate whether people have insight into their AI hyperrealism errors and uncover what causes this somewhat counterintuitive phenomenon, we asked a new set of participants to report how confident they felt, and what information they used, when attempting to distinguish AI from human faces. Focusing our new empirical work on the White AI faces from Nightingale and Farid (2022) enabled us to test the robustness of AI hyperrealism with a new set of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open practices statement</head><p>We report all measures and exclusions (see Supplemental File S2), along with power analyses justifying our sample sizes (see Supplemental File S3). Data, analysis scripts, and materials are available on the Open Science Framework at osf.io/sz2fe/. Stimuli are available at osf .io/ru36d/. Data were analyzed using R version 4.2.1 (R Core Team, 2021) and JASP ( JASP Team, 2023). Metad ′ was calculated in MATLAB (The MathWorks, Natick, MA; <ref type="bibr" target="#b16">Maniscalco &amp; Lau, 2012</ref><ref type="bibr" target="#b17">, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method and participants</head><p>The final data were from 124 adults (61 men, 62 women, 1 preferred another term) recruited from Prolific (<ref type="url" target="http://www.prolific.co">www.  prolific.co</ref>). Participants were White U.S. residents, aged 18 to 50 years (M age = 34.4 years, SD = 8.0 years), who had not lived outside the United States for more than 2 years before they turned 18 and who reported not having autism spectrum disorder, attention deficit disorder/attention deficit hyperactivity disorder, schizophrenia, or a major neurological condition.</p><p>We recruited only White participants because of potential out-group effects in humanness ratings (McLoughlin et al., 2018) and other-race effects (McKone et al., 2019; Meissner &amp; Brigham, 2001). 1 Participants' data were excluded if they did not complete the full study or missed &gt; 10% of experimental trials in the AI or human face conditions; used a mobile phone (because face stimuli would not appear appropriately sized); responded incorrectly on more than one attention check question; or responded incorrectly when they were asked at the end of the study what task they had performed (see Supplemental File S2). All participants in this research provided informed consent, and all experiments were approved by the Australian National University Human Research Ethics Committee (Protocol 2019/970).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimulus materials</head><p>We used the 100 AI and 100 human White faces (half male, half female) from Nightingale and Farid (2022; see osf.io/ru36d/). The AI faces were generated using StyleGAN2. The human faces were selected from the Flickr-Faces-HQ Dataset <ref type="bibr">(Karras et al., 2021, used to develop StyleGAN2)</ref> to match each of the AI faces as closely as possible (e.g., same gender, posture, and expression). All stimuli had blurred or mostly plain backgrounds, and AI faces were screened to ensure they had no obvious rendering artifacts (e.g., no extra faces in background). Screening for artifacts mimics how real-world users screen AI faces, either as scientists <ref type="bibr" target="#b26">(Peterson et al., 2022)</ref> or for public use <ref type="bibr" target="#b32">(Satter, 2021)</ref>, and therefore captures the type and range of stimuli that appear online. Participants were asked to resize their screen so that stimuli had a visual angle of 12° wide × 12° high at ~50 cm viewing distance.</p><p>Participants were assigned in counterbalanced order by gender to view either all the male or female faces (50 AI + 50 human faces = 100 trials in total) so that approximately equal numbers of men and women were assigned to each face sex. Faces were shown individually until a response was made, with order randomized across participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants were told that they would see approximately 100 faces with the task of deciding whether each face depicts a real human or is computer-generated (AI). We defined "human" as people who exist in the real world and "computer-generated" as pictures that have been made by AI technology for generating highly realistic images of people who do not exist in the real world. After deciding whether a face was AI or human, participants rated their confidence on each trial from 0 (not at all) to 100 (completely). The AI and human response options were shown horizontally on screen with the left/right position counterbalanced across participants. In five additional trials, as an attention check, participants decided whether a face was under or over 50 years of age (see Supplemental File S4). Finally, to investigate the visual attributes used by participants to judge whether faces were AI or human, we asked participants to give open-ended responses about what information they used. At the end of the experiment, participants reported their age, gender identity, time spent outside the United States, state of residence, and any clinical or neurological conditions, and they confirmed they were White.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytic strategy</head><p>First, we calculated the percentage of stimuli judged as human, the error percentages, and the mean confidence ratings for each participant (for AI and human faces separately; Supplemental File S5). Complementary stimulus-level analyses are reported in Supplemental File S6. We also calculated participant-level signal detection measures: d ′ and meta-d ′ . Meta-d ′ combines confidence ratings with response correctness to measure metacognitive sensitivity-participants' insight into whether their responses are correct or incorrect <ref type="bibr" target="#b16">(Maniscalco &amp; Lau, 2012</ref><ref type="bibr" target="#b17">, 2014)</ref> </p><formula xml:id="formula_0">(Supplemental File S7).</formula><p>To analyze the open-ended qualitative responses, we used data-driven (inductive) thematic analysis following the stages proposed by <ref type="bibr" target="#b2">Braun and Clarke (2006)</ref>. The first author initially read through the responses and formulated an initial thematic framework. The themes were then refined and finalized via detailed discussion with the second and last authors. Findings held for male and female faces separately (Supplemental File S8). Figure <ref type="figure" target="#fig_4">3</ref> shows the faces that were judged as human and AI most often: Notably, the top three most humanlike faces were actually AI-generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI hyperrealism is robust</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do people have insight into their AI detection errors?</head><p>Concerningly, we found that participants who were the worst at detecting AI faces had the poorest insight Al female 29 (93%)</p><p>Al male 45 (92%)</p><p>Five faces judged as human most often</p><p>Five faces judged as Al most often</p><p>Al male 13 (90%) Human male 40 (90%) Al male 34 (89%)</p><p>Human male 37 (90%) Human male 47 (86%) Human female 31 (84%) Al female 44 (82%) Human male 18 (79%) a b into their abilities, against our prediction from the face identification literature. However, the accuracyconfidence relationship differed by face type: Although lower error rates for classification of human faces were associated with higher confidence as predicted, r human = -.235, 95% CI = [-.395, -.061], p = .009 (Fig. <ref type="figure" target="#fig_2">2b</ref>), for AI faces more errors were unexpectedly associated with higher confidence, r AI = .385, 95% CI = [.224, .526], p &lt; .001 (Fig. <ref type="figure" target="#fig_2">2c</ref>), indicating that the tendency for AI hyperrealism is exacerbated by overconfidence.</p><p>To investigate participants' insight into their performance, free from bias in confidence ratings (e.g., reporting high confidence for all judgments), we used meta-d ′ . Half of the participants (51%) fell into the bottom left quadrant, pairing poor performance with poor insight (versus ~23% with poor performance and good insight; ~8% with good performance and poor insight; and ~18% with good performance and good insight). These findings are incongruent with prior face perception literature <ref type="bibr" target="#b25">(Palermo et al., 2017;</ref><ref type="bibr" target="#b39">Wixted &amp; Wells, 2017)</ref> but in line with other types of judgments in which people can be highly confident but incorrect (e.g., when people are unknowingly exposed to misinformation but report it with high confidence; Flowe et al., 2019), or overestimate their competence at a task, commonly referred to as the Dunning-Kruger effect <ref type="bibr" target="#b15">(Kruger &amp; Dunning, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What visual attributes do participants report using to judge faces as AI versus human?</head><p>Figure <ref type="figure" target="#fig_6">4</ref> presents the qualitative coding framework capturing the attributes that participants reported using when they judged whether faces were AI or human. The size of each segment indicates the percentage of total codes captured by each theme. The framework is composed of 21 main themes with 20 subthemes (e.g., "eyes" is a subtheme of the specific facial features theme). Responses could be coded into multiple themes, and thus each response was coded into an average of 2.29 themes. For instance, the response, "If the faces were overly symmetrical and if they [sic] eyes looked fake" was coded into the "symmetry," "eyes," and "artificial" themes. A total of 546 codes were applied to the 239 responses. Supplemental File S9 includes example quotes for each theme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>The phenomenon of AI hyperrealism implies there must be some visual differences between AI and human faces, which people misinterpret. Very little is known about what these differences might be. <ref type="bibr" target="#b35">Tucciarelli et al. (2022)</ref> found a partial negative contribution of attractiveness, which aligns with our predictions based on face-space, because faces at the core of face-space (more average faces) tend to be more attractive, all else being equal <ref type="bibr" target="#b29">(Rhodes, 2006)</ref>. Shen et al. ( <ref type="formula">2021</ref>) also found that removing background scenery made AI and human faces indistinguishable; however, background information was matched for our stimuli, rendering this latter explanation unlikely here.</p><p>Thus, in Experiment 2 we investigated the capacity of 14 attributes derived from face-space and Experiment 1 qualitative reports to explain AI hyperrealism. We also tested for the first time whether human-perceivable information can be used to accurately classify AI and human faces, using machine learning. If, as we hypothesize, StyleGAN2 is biased to produce faces toward the center of face-space, AI faces should be perceived as more average, familiar, and attractive, but as less memorable than human faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>The final data were from 610 participants (290 men, 312 women, 8 preferred another term; M age = 35.3 years, SD = 8.6 years), recruited to rate the AI and human faces on one of 14 attributes. In contrast to Experiment 1, participants were not told AI faces were present, and we excluded those who guessed that AI faces were present (N = 44, 7%). Participant screening was otherwise identical to Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>In total, 14 attributes were rated (Table <ref type="table" target="#tab_2">1</ref>). In addition to the four attributes derived from face space theory (distinctiveness/averageness, memorability, familiarity, attractiveness), we focused our analyses on attributes commonly mentioned in Experiment 1, resulting in nine attributes. We also included perceived age because we wanted to isolate the contributions of other related attributes, such as attractiveness and skin smoothness. Supplemental File S10 provides a detailed rationale. Each condition had five attention checks that asked for specific numeric ratings. Experimental stimuli and procedure were otherwise identical to Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytic strategy</head><p>We calculated the stimulus-level mean rating for each face for each of the 14 attributes separately. Then, using our data from Experiment 1, we calculated the percentage of participants who judged each face as human. Higher percentage values indicate that more participants judged the face as human. Stimulus type (i.e., AI or human faces) was dummy-coded (0 = AI faces and 1 = human faces).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which visual attributes contribute to faces being judged as human?</head><p>To determine what attributes made faces look real (even if they were AI-generated), we constructed a multiple linear regression model predicting the percentage of participants who judged each stimulus as human from the 14 stimulus-level attribute means. 2 All variables were standardized prior to model entry. The model explained the majority of observed variance (62%) in how often faces were judged as human, R 2 adj = .62, 95% CI = [.57, .72], p &lt; .001. Standardized coefficients for each individual predictor show that faces were more likely to be judged as human if they were more proportional, alive in the eyes, and familiar; and less memorable, symmetrical, attractive, and smooth-skinned (Table <ref type="table" target="#tab_3">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which attributes contribute to AI hyperrealism?</head><p>Here, we take a novel approach by applying a Brunswikian lens model <ref type="bibr" target="#b3">(Brunswik, 1956;</ref><ref type="bibr" target="#b10">Hall et al.,</ref>  2019) to reveal how each of the 14 attributes contributed to faces being (mis)judged as human (Fig. <ref type="figure">5</ref>). Constructing a stimulus-level lens model (using lavaan; Rossell, 2012) allowed us to investigate the attributes as simultaneous mediators explaining the correspondence between the AI or human status of faces and how often they were judged as human, thereby distinguishing between cue validity (differences in the visual attributes of human and AI faces) and cue utilization (the extent to which each attribute contributes to faces being judged as human). In this model, face type was the focal predictor (AI = 0 vs. human = 1), the 14 attributes were entered as simultaneous mediators, and the percentage of participants who judged each face as human was the outcome variable. All attributes were allowed to freely correlate with each other (see Supplemental File S12 for the correlation matrix), producing a fully saturated model that perfectly fit the data (comparative fit indices = 1.00, Tucker-Lewis indices = 1.00, root-mean-square errors of approximation = .00). 3  Consistent with Experiment 1, the total effect indicated that AI faces were more likely to be perceived as human than actual human faces, β = -0.41, z = -6.26, p &lt; .001.</p><p>Critically, in line with our face-space theory prediction that AI faces would be more average than human ones, AI faces were significantly more average (less distinctive), familiar, and attractive, and less memorable than human faces. Overall, AI hyperrealism was explained by larger cumulative effects for the attributes that were utilized in the wrong direction-facial proportions, familiarity, and memorability (in red, Fig. <ref type="figure">5</ref>; β = -0.67, 95% CI = [-.88, -.46], z = -6.10, p &lt; .001)-compared with those utilized in the correct direction-facial attractiveness, symmetry, and congruent lighting/shadows (in green; β = 0.37, 95% CI = [.21, .53], z = 4.47, p &lt; .001). Additionally, several valid cues were not utilized by participants-namely, facial averageness/ distinctiveness, image quality, and expressivity (in gray). There also remained a residual direct effect of face type (human vs. AI) on perceiving the face as human, over and above the 14 attributes we measured, β = -0.22, 95% CI = [-.40, -.03], z = -2.33, p = .020. Therefore, Note: See also the full experimental surveys on the Open Science Framework (osf.io/sz2fe/). a "Alive in the eyes" combines the "eyes" and "uncanny valley" themes from Experiment 1's qualitative framework. b "Proportional" combines the "features work as a whole" (are in proportion with one another) and "proportional" themes. c "Smooth-skinned" derives from "the skin or wrinkles" and "perfectness" themes.</p><p>although our identified mediators explained the majority of the AI hyperrealism effect, there are aspects still to be uncovered (we return to this in the Discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can human-perceived attributes be used to accurately classify AI and human faces?</head><p>Given that humans are unable to detect current AI faces, society needs tools that can accurately identify AI imposters. Present AI detection algorithms are limited to specific databases (e.g., the popular Google Chrome extension, V7 Fake Profile Detector, works only for StyleGAN faces). Human perception may be useful for improving algorithmic generalizability, as integrating additional parameters into algorithms has proved useful in other domains ( J. W. <ref type="bibr" target="#b22">Miller et al., 2022)</ref>. We therefore provide the first investigation of whether machine learning can leverage human-perceived attributes to accurately classify AI and human faces.</p><p>Using 10-fold cross-validation, we constructed a random forest classification model (mtry = 4; the square root of the number of predictors, rounded to the nearest whole number) predicting face type (AI vs. human) from the 14 attributes identified in Experiment 2. The model was able to accurately classify face type with 94% accuracy, 95% CI = [91%, 97%], z = 56.53, p &lt; .001, k = .88 (also see Table <ref type="table" target="#tab_5">3</ref> for the confusion matrix specifying the predicted and actual values during crossvalidation). AI faces, at least those generated by Style-GAN2, can therefore be distinguished from human faces on the basis of human-perceived attributes with extremely high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>We find that White AI faces are perceived as hyperreal and that observers are overconfident in their ability to detect them. By combining psychological theory with a novel data-driven approach and machine learning, our study significantly advances understanding of why AI hyperrealism occurs. Specifically, we were able to pinpoint perceptual attributes that accurately distinguish AI from human faces and model how people misuse this information, explaining a significant majority of the variance in humans' AI judgments. The identification of these attributes provides a critical foundation in the future for detailed psychophysics work aiming to map AI face-space. Importantly, the present findings are generalizable to the types of images used online, because AI faces are screened for image artifacts as they are selected for real-world use (e.g., when committing fraud; <ref type="bibr" target="#b32">Satter, 2021)</ref>. Also, artifact screening cannot explain the White specificity of hyperrealism in our reanalysis of <ref type="bibr">Nightingale and Farids's (2022)</ref> data, as the same screening criteria were applied across face race.</p><p>Our study highlights two separate, and critical, biases. First, generative adversarial networks (GANs) are biased toward the statistical regularities of their most common inputs, which we argue produces AI hyperrealism. Although we demonstrate this point in the context of AI faces, the foundational idea may generalize to other important types of AI outputs, including text and artwork from ChatGPT and DALL-E. Here, as this argument predicts, we found AI faces appeared more average than their human counterparts (see Supplemental Fig. <ref type="figure" target="#fig_6">S14</ref>). Notably, participants failed to utilize facial distinctiveness/averageness for AI detection and inappropriately utilized several other associated cues (facial proportions, memorability, familiarity), producing hyperrealism. Attractiveness was correctly used as a distinguishing feature, confirming Tucciarelli et al.'s (2022) initial finding. The minority of variance left to be explained suggests that other cues, such as those mentioned less often in Experiment 1 (e.g., "ears," "glasses"), may also play a small but cumulative role. Second, we found evidence of White racial bias in algorithmic training that produces racial differentials in the presence of AI hyperrealism, with significant implications for the use of AI faces online and in science. Previously, less realistic computer-generated faces have been used as stand-ins for human faces when it was inappropriate to do so <ref type="bibr" target="#b7">(Dawel et al., 2022;</ref><ref type="bibr" target="#b21">E. J. Miller et al., 2023)</ref>, and there is concern that the same will happen with AI faces, with implications for inequality. We recommend that studies using AI faces should verify that they are perceived as equally natural across races. On a related note, a pressing question is how to address racial differentials in GANs. It is unclear in face-space theory whether there is one face-space or separate spaces for different demographic groups (e.g., <ref type="bibr" target="#b37">Valentine et al., 2016)</ref>. Future research could fruitfully test these theoretical questions by comparing a GAN trained on equal numbers of faces of each race with GANs trained separately on different demographic groups.</p><p>Importantly, and in contrast to standard AI detection algorithms (which are "black boxes"), the present work makes known the perceptual attributes that lead to accurate AI detection in machine learning. Human accuracy may also be improved by training people to utilize attributes appropriately, though this strategy risks exacerbating overconfidence as technologies progress and certain attributes become outdated. Currently, most algorithms produce only single images of each identity, but soon multiple images of AI products are likely to be available <ref type="bibr" target="#b5">(Chan et al., 2023)</ref>. We likewise drew on a theoretical account of face-space that focuses on variation between single images; when multiple within-identity AI images are commonplace, future work could apply more nuanced face-space theories (e.g., <ref type="bibr" target="#b4">Burton et al., 2016;</ref><ref type="bibr" target="#b24">O'Toole et al., 2018)</ref>. Regardless, because AI technology is advancing so rapidly <ref type="bibr" target="#b1">(Bond, 2023)</ref>, training focused on metacognition and education may be more helpful. For example, <ref type="bibr" target="#b34">Szpitalak et al. (2021)</ref> found that people who were advised about the unreliability of human memory were more resistant to misinformation than naive individuals. Educating people about the perceived realism of AI faces could likewise reduce risks by making the public appropriately skeptical.</p><p>We also found individual differences in the accuracy of AI face detection (Fig. <ref type="figure" target="#fig_2">2c</ref>), opening new lines of research. Participants were selected to have normalrange face perception, yet the best performer achieved only 80% accuracy. However, people with exceptional face recognition abilities (super recognizers; <ref type="bibr" target="#b27">Ramon et al., 2019)</ref> may possess superior AI detection skills. A further intriguing question is whether individual differences in the utilization of specific attributes can shed light on why certain individuals are more vulnerable to deception by AI faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The present study demonstrates a robust AI hyperrealism effect: Remarkably, White AI faces can convincingly pass as more real than human faces-and people do not realize they are being fooled. We believe psychology has a critical role to play in holding AI technologies accountable to the public good. Society has faced many large-scale, seemingly unsolvable challenges that have subsequently become a normal, and manageable, part of life (e.g., automobile safety). We remain hopeful that social and regulatory responses will reduce potential risks as society adjusts to the inevitable presence of AI in our world.</p><p>Transparency Action Editor: Rachael Jack Editor: Jennifer L. Tackett Author Contributions Elizabeth J. Miller: Conceptualization; Data curation; Formal analysis; Methodology; Visualization; Writingoriginal draft; Writing -review &amp; editing. Ben A. Steward: Formal analysis; Methodology; Visualization; Writing -original draft; Writing -review &amp; editing. Zak Witkower: Formal analysis; Methodology; Visualization; Writing -original draft; Writing -review &amp; editing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic illustration of face-space theory: A potential explanation for AI hyperrealism. Orange dots show sample distribution of human faces; purple dots show hypothesized distribution of AI faces. We focus on relevant abstract principles of face-space theory (e.g., relating to single images of faces in human perception). For more nuanced discussions, see Burton et al. (2016), O'Toole et al.(2018), and<ref type="bibr" target="#b37">Valentine et al. (2016)</ref>. For psychophysics-related work, see<ref type="bibr" target="#b0">Abudarham and Yovel (2016)</ref> and<ref type="bibr" target="#b30">Rhodes and Jeffery (2006)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>, p &lt; .001, d = 0.74, 95% CI = [0.62, 0.87]. The d ′</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Reanalysis of data from Experiment 1 of Nightingale and Farid (2022) and results for current Experiment 1. Error bars represent 95% confidence intervals. N&amp;F E1 = data from Nightingale and Farid (2022), Experiment 1; n.s. = nonsignificant. **p &lt; .01. ***p &lt; .001.</figDesc><graphic coords="3,273.41,235.40,74.78,65.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><figDesc>Figure2ashows that the hyperrealism found for White AI faces in our reanalysis of<ref type="bibr" target="#b23">Nightingale and Farid (2022)</ref> was fully replicated in our new sample, indicating that this effect is robust. White AI faces were judged as human significantly more often than White human faces, M AI = 65.9% versus M human = 51.1%, t(123) = 7.14, p &lt; .001, d = 0.64, 95% CI = [0.45, 0.83], and significantly more often than chance (50% in 2AFC task), t(123) = 7.82, p &lt; .001, d = 0.70, 95% CI = [0.51, 0.90]. In contrast, performance for White human faces did not differ significantly from chance, t(123) = 0.60, p = .550, d = 0.05, 95% CI [-0.12, 0.23]. As in our reanalysis of Nightingale and Farid (2022), d ′ was significantly negative, d ′ = -0.49 (vs. 0 = no sensitivity), t(123) = 5.20, p &lt; .001, d = 0.68, 95% CI = [0.49, 0.88].Findings held for male and female faces separately (Supplemental File S8). Figure3shows the faces that were judged as human and AI most often: Notably, the top three most humanlike faces were actually AI-generated.</figDesc><graphic coords="5,48.86,72.62,504.14,270.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Faces judged most often as (a) human and (b) AI. The stimulus type (AI or human; male or female), the stimulus ID (Nightingale &amp; Farid, 2022), and the percentage of participants who judged the face as (a) human or (b) AI are listed below each face.</figDesc><graphic coords="5,48.86,72.62,504.14,270.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Positive meta-d ′ values indicate participants have insight into whether their responses are correct or incorrect, whereas negative meta-d ′ values indicate participants have misplaced confidence in the correctness of their responses. Participants' meta-d ′ values in the present study were frequently negative (59% of participants), indicating poor insight. Moreover, Figure 2d shows lower insight (meta-d ′ ) was associated with poorer performance (d ′ ) on the AI versus human judgment task, r = .479, 95% CI = [.330, .604], p &lt; .001, indicating the poorest performers were the least aware of their AI detection errors. We divided Figure 2d into quadrants at d ′ = 0 and meta-d ′ = 0 to identify groups of participants with each combination of good and poor performance (d ′ ) and insight (meta-d ′ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative responses from Experiment 1: percentage of codes (N = 546) in each theme. Subthemes are shown at the outside edge of the main theme.</figDesc><graphic coords="7,99.00,69.00,405.00,336.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Experiment 2 Visual-Attribute Rating Conditions</figDesc><table><row><cell></cell><cell>N raters for</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attribute</cell><cell>M/F faces</cell><cell>Rating question</cell><cell>Low anchor = 0</cell><cell>High anchor = 100</cell></row><row><cell>Age</cell><cell>23/21</cell><cell>How old is this person?</cell><cell>0 years</cell><cell>100 years</cell></row><row><cell>Alive in the eyes/</cell><cell>22/21</cell><cell>When you look at this</cell><cell>Definitely not</cell><cell>Definitely alive</cell></row><row><cell>uncanny valley a</cell><cell></cell><cell>person's eyes, how alive do</cell><cell>alive</cell><cell></cell></row><row><cell></cell><cell></cell><cell>they seem?</cell><cell></cell><cell></cell></row><row><cell>Attractive</cell><cell>20/22</cell><cell>How attractive is this face?</cell><cell>Not at all</cell><cell>Very attractive</cell></row><row><cell>Congruent lighting</cell><cell>21/21</cell><cell>How much would this face</cell><cell>Not at all</cell><cell>Very much</cell></row><row><cell></cell><cell></cell><cell>stand out in a crowd?</cell><cell></cell><cell></cell></row><row><cell>Distinctive/average</cell><cell>22/20</cell><cell>How congruent are the</cell><cell>Not at all</cell><cell>Very congruent</cell></row><row><cell></cell><cell></cell><cell>lighting and shadows</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>across this picture?</cell><cell></cell><cell></cell></row><row><cell>Expressive</cell><cell>21/22</cell><cell>How emotionally expressive</cell><cell>Not at all</cell><cell>Very expressive</cell></row><row><cell></cell><cell></cell><cell>is this face?</cell><cell></cell><cell></cell></row><row><cell>Eye contact</cell><cell>22/22</cell><cell>Is this person making eye</cell><cell>No, they are</cell><cell>Yes, they definitely are</cell></row><row><cell></cell><cell></cell><cell>contact with you?</cell><cell>definitely not</cell><cell></cell></row><row><cell>Familiar</cell><cell>21/23</cell><cell>How familiar is this face?</cell><cell>Not at all</cell><cell>Very familiar</cell></row><row><cell>Genuinely happy</cell><cell>21/21</cell><cell>How happy is this person</cell><cell>Not at all</cell><cell>Very much</cell></row><row><cell></cell><cell></cell><cell>genuinely feeling?</cell><cell></cell><cell></cell></row><row><cell>Image quality</cell><cell>21/21</cell><cell>How good is the quality of</cell><cell>Poor</cell><cell>Excellent</cell></row><row><cell></cell><cell></cell><cell>this picture?</cell><cell></cell><cell></cell></row><row><cell>Memorable</cell><cell>21/22</cell><cell>How memorable is this face?</cell><cell>Not at all</cell><cell>Very memorable</cell></row><row><cell>Proportional/features</cell><cell>21/22</cell><cell>How proportional is this face?</cell><cell>Not at all</cell><cell>Very proportional</cell></row><row><cell>work as a whole b</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smooth-skinned/</cell><cell>24/23</cell><cell>How smooth is this person's</cell><cell>Not at all</cell><cell>Very smooth</cell></row><row><cell>perfectness c</cell><cell></cell><cell>skin?</cell><cell></cell><cell></cell></row><row><cell>Symmetrical</cell><cell>22/27</cell><cell>How symmetrical is this face?</cell><cell>Not at all</cell><cell>Very symmetrical</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Standardized Coefficients for Each Attribute (Ordered by β Weight) in Our Linear Regression Model Predicting Experiment 1 Stimulus-Level Percentage Judged as Human Note: CI = confidence interval; boldface type indicates p &lt; .05. *p &lt; .05. **p &lt; .01. ***p &lt; .001. †p &lt; .10.Lens model testing contributions of each attribute to (mis)judgment of faces as human (ordered by indirect effect size). Red boxes show significant negative indirect effectsattributes that were utilized in the wrong direction to judge AI/human status. Green boxes show significant positive indirect effects (attributes that contributed to accurate AI/human judgments). Gray boxes show attributes that are useful for detecting AI faces but were not utilized by human observers. Dashed lines indicate nonsignificant effects (see TableS13 in</figDesc><table><row><cell>Attribute</cell><cell>β</cell><cell>SE</cell><cell>t</cell><cell>p</cell><cell>95% CI</cell></row><row><cell>Proportional</cell><cell>0.67</cell><cell>0.11</cell><cell>6.05</cell><cell>&lt; .001***</cell><cell>[0.45, 0.89]</cell></row><row><cell>Alive in the eyes</cell><cell>0.37</cell><cell>0.08</cell><cell>4.56</cell><cell>&lt; .001***</cell><cell>[0.21, 0.53]</cell></row><row><cell>Expressive</cell><cell>0.23</cell><cell>0.14</cell><cell>1.68</cell><cell>.096 †</cell><cell>[-0.04, 0.51]</cell></row><row><cell>Familiar</cell><cell>0.20</cell><cell>0.08</cell><cell>2.69</cell><cell>.008**</cell><cell>[0.05, 0.35]</cell></row><row><cell>Eye contact</cell><cell>-0.01</cell><cell>0.05</cell><cell>-0.17</cell><cell>.864</cell><cell>[-0.11, 0.09]</cell></row><row><cell>Distinctive/average</cell><cell>-0.04</cell><cell>0.08</cell><cell>-0.45</cell><cell>.654</cell><cell>[-0.19, 0.12]</cell></row><row><cell>Image quality</cell><cell>-0.08</cell><cell>0.07</cell><cell>-1.10</cell><cell>.274</cell><cell>[-0.22, 0.06]</cell></row><row><cell>Congruent lighting</cell><cell>-0.10</cell><cell>0.06</cell><cell>-1.63</cell><cell>.104</cell><cell>[-0.22, 0.02]</cell></row><row><cell>Age</cell><cell>-0.14</cell><cell>0.10</cell><cell>-1.42</cell><cell>.156</cell><cell>[-0.33, 0.05]</cell></row><row><cell>Memorable</cell><cell>-0.17</cell><cell>0.08</cell><cell>-2.17</cell><cell>.031*</cell><cell>[-0.32, -0.02]</cell></row><row><cell>Symmetrical</cell><cell>-0.21</cell><cell>0.09</cell><cell>-2.33</cell><cell>.021*</cell><cell>[-0.39, -0.03]</cell></row><row><cell>Attractive</cell><cell>-0.27</cell><cell>0.09</cell><cell>-2.90</cell><cell>.004**</cell><cell>[-0.45, -0.09]</cell></row><row><cell>Genuinely happy</cell><cell>-0.28</cell><cell>0.15</cell><cell>-1.81</cell><cell>.071 †</cell><cell>[-0.58, 0.02]</cell></row><row><cell>Smooth-skinned</cell><cell>-0.61</cell><cell>0.10</cell><cell>-6.07</cell><cell>&lt; .001***</cell><cell>[-0.81, -0.41]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Confusion Matrix of Correct and Incorrect Machine Classifications</figDesc><table><row><cell></cell><cell>Actual: AI</cell><cell>Actual: Human</cell></row><row><cell>Predicted: AI</cell><cell>93</cell><cell>5</cell></row><row><cell>Predicted: Human</cell><cell>7</cell><cell>95</cell></row><row><cell cols="2">Note: Correct classifications are in boldface.</cell><cell></cell></row></table><note><p>Clare A. M. Sutherland: Conceptualization; Funding acquisition; Methodology; Writing -review &amp; editing. Eva G. Krumhuber: Conceptualization; Funding acquisition; Methodology; Writing -review &amp; editing. Amy Dawel: Conceptualization; Data curation; Formal analysis; Funding acquisition; Methodology; Project administration; Supervision; Visualization; Writing -original draft; Writing -review &amp; editing.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We thank <rs type="person">Sophie J. Nightingale</rs> and <rs type="person">Hany Farid</rs> for providing open access to their stimuli and data.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research is supported by the <rs type="funder">Australian Government</rs> through the <rs type="funder">Australian Research Council</rs>'s <rs type="programName">Discovery Projects funding scheme</rs> (Project No. <rs type="grantNumber">DP220101026</rs>), a <rs type="grantName">TRANSFORM Career Development Fellowship</rs> to A. Dawel from the <rs type="funder">Australian National University College of Health and Medicine</rs>, and an <rs type="funder">Experimental Psychology Society Small</rs> Grant to <rs type="person">C. A. M. Sutherland</rs>. The funders had no role in developing or conducting this research.</p></div>
<div><head>Declaration of Conflicting Interests</head><p>This article has received the badges for Open Data and Open Materials. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psychologicalscience  .org/publications/badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FWFUPmJ">
					<idno type="grant-number">DP220101026</idno>
					<orgName type="grant-name">TRANSFORM Career Development Fellowship</orgName>
					<orgName type="program" subtype="full">Discovery Projects funding scheme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/09567976231207095">http://  journals.sagepub.com/doi/suppl/10.1177/09567976231207095</ref> Notes 1. <ref type="bibr" target="#b23">Nightingale and Farid's (2022)</ref> Experiment 1 sample was 73% White. We ran a mixed analysis of variance on the percentage of faces judged as human with face type (White AI, White human) within subjects and participant race (White, non-White) between subjects and found no significant main effect of participant race, F(1, 313) = 1.23, MSE = 267.22, p = .268, or interaction with face type, F(1, 313) = 1.01, MSE = 581.35, p = .316. However, the other-race effect arises from a lack of early-life exposure to other-race faces <ref type="bibr" target="#b18">(McKone et al., 2019;</ref><ref type="bibr" target="#b33">Singh et al., 2022)</ref>, which may be unevenly distributed across non-White participants. Therefore, we took a conservative approach and recruited only White participants in the current studies. 2. Constructing a binomial regression model instead of a linear one yielded a nearly identical pattern of results (Supplemental File S11). 3. We were interested in whether the attributes differed for human versus AI faces and whether each attribute relates to perceptions of faces being human rather than to how these attributes relate to each other. By allowing all attributes to correlate, we excuse model-fit issues generated by these expected covariances.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse engineering the face space: Discovering the critical features for face identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abudarham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yovel</surname></persName>
		</author>
		<idno type="DOI">10.1167/16.3.40</idno>
		<ptr target="https://doi.org/10.1167/16.3.40" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bond</surname></persName>
		</author>
		<ptr target="https://www.npr.org/2023/03/23/1165146797/it-takes-a-few-dollars-and-8-minutes-to-create-a-deepfake-and-thats-only-the-sta" />
		<title level="m">It takes a few dollars and 8 minutes to create a deepfake. And that&apos;s only the start</title>
		<imprint>
			<date type="published" when="2023-03-23">2023, March 23</date>
		</imprint>
	</monogr>
	<note type="report_type">NPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using thematic analysis in psychology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1191/1478088706qp063oa</idno>
		<ptr target="https://doi.org/10.1191/1478088706qp063oa" />
	</analytic>
	<monogr>
		<title level="j">Quantitative Research in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Brunswik</surname></persName>
		</author>
		<title level="m">Perception and the representative design of psychological experiments</title>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identity from variation: Representations of faces derived from multiple instances</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12231</idno>
		<ptr target="https://doi.org/10.1111/cogs.12231" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="202" to="223" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generative novel view synthesis with 3D-aware diffusion models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.02602v1" />
		<imprint>
			<date type="published" when="2023-04-05">2023, April 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ChildGAN: Face aging and rejuvenation to find missing children</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chandaliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2022.108761</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2022.108761" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">108761</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic survey of face stimuli used in psychological research 2000-2020</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horsburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ford</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01705-3</idno>
		<ptr target="https://doi.org/10.3758/s13428-021-01705-3" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1889" to="1901" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental examination of the effects of alcohol consumption and exposure to misleading postevent information on remembering a hypothetical rape scenario</title>
		<author>
			<persName><forename type="first">H</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Flowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Takarangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karoğlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hope</surname></persName>
		</author>
		<idno type="DOI">10.1002/acp.3531</idno>
		<ptr target="https://doi.org/10.1002/acp.3531" />
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="413" />
			<date type="published" when="2019">2023, May 3. 2019</date>
		</imprint>
	</monogr>
	<note>AI &apos;could be as transformative as Industrial Revolution The Guardian</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convergent evolution of face spaces across human face-selective neuronal groups and deep convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gaziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Yeagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mégevand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Groppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khuvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Herrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-12623-6</idno>
		<ptr target="https://doi.org/10.1038/s41467-019-12623-6" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonverbal Communication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-010418-103145</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-010418-103145" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="294" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">These creepy fake humans herald a new age in AI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hao</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/2021/06/11/1026135/ai-synthetic-data/" />
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2021-06-11">2021, June 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Hatmaker</surname></persName>
		</author>
		<ptr target="https://jasp-stats.org/" />
	</analytic>
	<monogr>
		<title level="m">Chinese propaganda network on Facebook used AI-generated faces</title>
		<imprint>
			<date type="published" when="2017">2020, September 22. 2023. 0.17</date>
		</imprint>
	</monogr>
	<note type="report_type">TechCrunch</note>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.2970919</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2020.2970919" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4217" to="4228" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html" />
		<title level="m">Analyzing and improving the image quality of StyleGAN</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unskilled and unaware of it: How difficulties in recognizing one&apos;s own incompetence lead to inflated self-assessments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunning</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.77.6.1121</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.77.6.1121" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1121" to="1134" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.concog.2011.09.021</idno>
		<ptr target="https://doi.org/10.1016/j.concog.2011.09.021" />
	</analytic>
	<monogr>
		<title level="j">Consciousness and Cognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="422" to="430" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Signal detection theory analysis of type 1 and type 2 data: Meta-d ′ , responsespecific meta-d ′ , and the unequal variance SDT model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Maniscalco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-45190-4_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-45190-4_3" />
	</analytic>
	<monogr>
		<title level="m">The cognitive neuroscience of metacognition</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Frith</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A critical period for faces: Other-race face recognition is improved by childhood but not adult social contact</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mckone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pidcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fiorentini</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-019-49202-0</idno>
		<ptr target="https://doi.org/10.1038/s41598-019-49202-0" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12820</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Young children perceive less humanness in outgroup faces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Tipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Over</surname></persName>
		</author>
		<idno type="DOI">10.1111/desc.12539</idno>
		<ptr target="https://doi.org/10.1111/desc.12539" />
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12539</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Brigham</surname></persName>
		</author>
		<idno type="DOI">10.1037/1076-8971.7.1.3</idno>
		<ptr target="https://doi.org/10.1037/1076-8971.7.1" />
	</analytic>
	<monogr>
		<title level="j">Psychology, Public Policy, and Law</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="35" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How do people respond to computer-generated versus human faces? A systematic review and meta-analyses</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mewton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chbr.2023.100283</idno>
		<ptr target="https://doi.org/10.1016/j.chbr.2023.100283" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">100283</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Azencot</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.12086v2" />
		<title level="m">Eigenvalue initialisation and regularisation for Koopman autoencoders</title>
		<imprint>
			<date type="published" when="2022-12-23">2022, December 23</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AI-synthesized faces are indistinguishable from real faces and more trustworthy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2120481119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2120481119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2120481119</biblScope>
			<date type="published" when="2022">2022</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face space representations in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Parde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2018.06.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="794" to="809" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do people have insight into their face recognition abilities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rossion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laguesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albonico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malaspina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Irons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Janabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rivolta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mckone</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2016.1161058</idno>
		<ptr target="https://doi.org/10.1080/17470218.2016.1161058" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep models of superficial face judgments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uddenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suchow</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2115228119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2115228119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">2115228119</biblScope>
			<date type="published" when="2022">2022</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Superrecognizers: From the lab to the world and back again</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bobak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1111/bjop.12368</idno>
		<ptr target="https://doi.org/10.1111/bjop.12368" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="479" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<title level="m">R: A language and environment for statistical computing (4.2.1</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Computer software R Foundation for Statistical Computing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The evolutionary psychology of facial beauty</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.57.102904.190208</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.57.102904.190208" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="226" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive norm-based coding of facial identity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeffery</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2006.03.002</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2006.03.002" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2977" to="2987" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">lavaan: An R package for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v048.i02</idno>
		<ptr target="https://doi.org/10.18637/jss.v048.i02" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A study of the human perception of synthetic faces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Satter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richardwebster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG52635.2021.9667066</idno>
		<ptr target="https://doi.org/10.1109/FG52635.2021.9667066" />
	</analytic>
	<monogr>
		<title level="m">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<imprint>
			<date type="published" when="2021-04-20">2021, April 20. 4fffaa00de6770b8. 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Experts: Spy used AI-generated face to connect with targets</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does cross-race contact improve cross-race face perception? A meta-analysis of the cross-race deficit and contact</title>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Earls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Correll</surname></persName>
		</author>
		<idno type="DOI">10.1177/01461672211024463</idno>
		<ptr target="https://doi.org/10.1177/01461672211024463" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="865" to="887" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memory training as a method for reducing the misinformation effect</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szpitalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woltmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kękuś</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12144-019-00490-9</idno>
		<ptr target="https://doi.org/10.1007/s12144-019-00490-9" />
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5410" to="5419" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the realness of people who do not exist: The social processing of artificial faces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tucciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsakiris</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2022.105441</idno>
		<ptr target="https://doi.org/10.1016/j.isci.2022.105441" />
	</analytic>
	<monogr>
		<title level="j">IScience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">105441</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified account of the effects of distinctiveness, inversion, and race in face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Valentine</surname></persName>
		</author>
		<idno type="DOI">10.1080/14640749108400966</idno>
		<ptr target="https://doi.org/10.1080/14640749108400966" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section A</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="204" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face-space: A unifying concept in face recognition research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Valentine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hills</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2014.990392</idno>
		<ptr target="https://doi.org/10.1080/17470218.2014.990392" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1996" to="2019" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Familiarity, memorability, and the effect of typicality on the recognition of faces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Vokey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Read</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03199666</idno>
		<ptr target="https://doi.org/10.3758/BF03199666" />
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="302" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The relationship between eyewitness confidence and identification accuracy: A new synthesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wixted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wells</surname></persName>
		</author>
		<idno type="DOI">10.1177/1529100616686966</idno>
		<ptr target="https://doi.org/10.1177/1529100616686966" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="65" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The promising future of cognitive science and artificial intelligence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1038/s44159-023-00170-3</idno>
		<ptr target="https://doi.org/10.1038/s44159-023-00170-3" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
