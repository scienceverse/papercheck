<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Researchers&apos; Intuitions About Power in Psychological Research</title>
				<funder ref="#_fPR3gbb">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_9mqWspY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marjan</forename><surname>Bakker</surname></persName>
							<email>m.bakker_1@uvt.nl</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Methodology and Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Methodology and Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jelte</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Methodology and Statistics</orgName>
								<orgName type="department" key="dep2">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="institution">Tilburg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology, Psychological Methods</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tilburg School of Social and Behavioral Sciences</orgName>
								<orgName type="department" key="dep2">Department of Methodology and Statistics</orgName>
								<address>
									<postBox>P. O. Box 90153</postBox>
									<postCode>5000 LE</postCode>
									<settlement>Tilburg</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Researchers&apos; Intuitions About Power in Psychological Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE9EC13C4A8294E7C187909A43BC25B3</idno>
					<idno type="DOI">10.1177/0956797616647519pss.sagepub.</idno>
					<note type="submission">Received 10/10/15; Revision accepted 4/11/16</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>power</term>
					<term>survey</term>
					<term>methodology</term>
					<term>sample size</term>
					<term>effect size</term>
					<term>open data</term>
					<term>open materials</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many psychology studies are statistically underpowered. In part, this may be because many researchers rely on intuition, rules of thumb, and prior practice (along with practical considerations) to determine the number of subjects to test. In Study 1, we surveyed 291 published research psychologists and found large discrepancies between their reports of their preferred amount of power and the actual power of their studies (calculated from their reported typical cell size, typical effect size, and acceptable alpha). Furthermore, in Study 2, 89% of the 214 respondents overestimated the power of specific research designs with a small expected effect size, and 95% underestimated the sample size needed to obtain .80 power for detecting a small effect. Neither researchers' experience nor their knowledge predicted the bias in their self-reported power intuitions. Because many respondents reported that they based their sample sizes on rules of thumb or common practice in the field, we recommend that researchers conduct and report formal power analyses for their studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Article</head><p>Despite the existence of alternative analytical techniques <ref type="bibr" target="#b29">(Rouder, Speckman, Sun, &amp; Morey, 2009;</ref><ref type="bibr" target="#b35">Wagenmakers, Wetzels, Borsboom, &amp; van der Maas, 2011)</ref>, and notwithstanding criticism (e.g., <ref type="bibr" target="#b23">Nickerson, 2000)</ref>, null-hypothesis significance testing (NHST) remains the main statistical tool in the analysis of psychological research data <ref type="bibr" target="#b3">(Bakker &amp; Wicherts, 2011;</ref><ref type="bibr" target="#b24">Nuijten, Hartgerink, van Assen, Epskamp, &amp; Wicherts, 2015;</ref><ref type="bibr" target="#b38">Wetzels et al., 2011)</ref>. Much recent debate on how researchers use NHST in practice has concerned the inflation of the number of Type I errors, or rejection of the null hypothesis when it is in fact true <ref type="bibr" target="#b1">(Asendorpf et al., 2013;</ref><ref type="bibr" target="#b2">Bakker, van Dijk, &amp; Wicherts, 2012;</ref><ref type="bibr" target="#b31">Simmons, Nelson, &amp; Simonsohn, 2011;</ref><ref type="bibr" target="#b35">Wagenmakers et al., 2011)</ref>. Reducing the possibility of Type II errors is another important consideration in improving the quality of studies, however: Studies should be well powered <ref type="bibr" target="#b11">(Fiedler, Kutzner, &amp; Krueger, 2012;</ref><ref type="bibr" target="#b31">Simmons et al., 2011)</ref>.</p><p>It has long been argued that researchers should conduct formal power analyses before starting data collection <ref type="bibr" target="#b7">(Cohen, 1965</ref><ref type="bibr" target="#b8">(Cohen, , 1990</ref>), yet it continues to be the case that many studies in the psychological literature are statistically underpowered <ref type="bibr" target="#b2">(Bakker et al., 2012;</ref><ref type="bibr" target="#b8">Cohen, 1990;</ref><ref type="bibr" target="#b20">Maxwell, 2004)</ref>. Specifically, given the typical effect sizes (ESs) and sample sizes reported in the psychological literature, the statistical power of a typical two-group between-subjects design has been estimated to be less than .50 <ref type="bibr" target="#b8">(Cohen, 1990)</ref> or even .35 <ref type="bibr" target="#b2">(Bakker et al., 2012)</ref>. These low power estimates appear to contradict the finding that more than 90% of published studies in the literature have p values below the typical threshold for significance (i.e., Î± = .05; <ref type="bibr" target="#b10">Fanelli, 2010;</ref><ref type="bibr" target="#b32">Sterling, Rosenbaum, &amp; Weinkam, 1995)</ref>. This apparent discrepancy is often attributed to the combination of publication bias (i.e., the nonreporting of nonsignificant results; <ref type="bibr" target="#b28">Rosenthal, 1979)</ref> and the use of questionable research practices in the collection and analysis of data <ref type="bibr" target="#b17">( John, Loewenstein, &amp; Prelec, 2012;</ref><ref type="bibr" target="#b31">Simmons et al., 2011)</ref>. Despite the centrality of power in NHST <ref type="bibr" target="#b13">( Gigerenzer, 2004)</ref>, formal power analyses are rarely reported in the literature. <ref type="bibr" target="#b30">Sedlmeier and Gigerenzer (1989)</ref> found that none of the 54 articles published in the 1984 volume of the Journal of Abnormal Psychology reported the power of the statistical tests that were presented. In a more recent and fairly representative sample of 271 psychological articles that involved the use of NHST <ref type="bibr" target="#b3">(Bakker &amp; Wicherts, 2011)</ref>, only 3% of the authors explicitly discussed power as a consideration in designing their studies. Thus, it appears that sample-size decisions are hardly ever based on formal and explicitly reported (a priori) power considerations.</p><p>Here, we consider another explanation of the common failure to conduct sufficiently powerful studies, namely, researchers' intuitions about statistical power. In a classic study, <ref type="bibr" target="#b34">Tversky and Kahneman (1971)</ref> showed that even quantitatively oriented psychologists underestimated the randomness in small samples. In addition, when <ref type="bibr" target="#b15">Greenwald (1975)</ref> asked social psychologists what the acceptable Type II error rate was, the average response was around .27, which means that an acceptable level of power would be .73, which again is markedly higher than the overall power estimates for published studies, as reported by <ref type="bibr" target="#b8">Cohen (1990)</ref> and <ref type="bibr" target="#b2">Bakker et al. (2012)</ref>. These results suggest that researchers may intuitively overestimate the power associated with their own research and that of others (i.e., in their role as reviewers).</p><p>Given the centrality of power in the debate regarding reproducibility and replicability of research in psychology and beyond (e.g., <ref type="bibr" target="#b1">Asendorpf et al., 2013;</ref><ref type="bibr" target="#b5">Button et al., 2013;</ref><ref type="bibr" target="#b14">Gilbert, King, Pettigrew, &amp; Wilson, 2016;</ref><ref type="bibr">Open Science Collaboration, 2015)</ref>, we surveyed psychology researchers on their practices, intuitions, and goals related to statistical power. In our first study, respondents assumed the role of either researcher (reporting on their own studies) or reviewer (assessing their peers' studies) in answering questions about typical and acceptable cell sizes (ns), ESs, power levels, and alpha levels. In addition, respondents in the researcher condition indicated how they typically determined their sample size in planning studies, and those in the reviewer condition indicated how they assessed sample sizes in other researchers' studies. This survey informed us about the typical study from the viewpoints of both researchers and reviewers. In our second study, respondents estimated the actual power of several research designs and the sample size that would be required to achieve a power of .80 in various research designs.</p><p>Study 1 Method Subjects. We collected all e-mail addresses of the corresponding authors of the 1,304 articles published in 2012 in Cognitive, Affective, &amp; Behavioral Neuroscience; Cognitive Psychology; Developmental Psychology; European Journal of Work and Organizational Psychology; Health Psychology; Journal of Consulting and Clinical Psychology; Journal of Experimental Social Psychology; Personality and Individual Differences; Psychological Methods; and Psychological Science. After removing 80 duplicate e-mail addresses and 5 physical addresses, we invited 1,219 researchers from various subdisciplines in psychology to participate in our online survey on the Qualtrics Web site in September 2013. Eighty-four e-mails bounced; thus, we assume that we were able to contact 1,135 researchers. We expected this sample to be sufficiently large for the (mostly descriptive) analyses that we had planned.</p><p>Of all the contacted researchers, 499 (44%) started the survey. Note that respondents would have been counted twice in this number if they started the survey, did not complete it, and then started it a second time after we sent a reminder. We could not send a personalized reminder because we did not want to be able to connect contact information with the responses given. Seven respondents who started the survey chose not to give informed consent and therefore did not complete it. A total of 291 (26%) respondents finished the survey. Respondents were randomly assigned to complete the reviewer's version or the researcher's version of the questionnaire. One hundred sixty-nine respondents completed the latter version, and 122 respondents completed the former version. We focus our discussion on the results obtained in analyses including only those respondents with complete data, except as noted.</p><p>Survey. We developed two versions of a short survey containing 10 questions (available at Open Science Framework, <ref type="url" target="https://osf.io/5t0b7/">https://osf.io/5t0b7/</ref>). The first version contained questions to be answered from a researcher's perspective, and the second version contained questions to be answered from a reviewer's perspective. The last 3 questions (concerning respondents' research field, statistical knowledge, and number of publications) were the same for the two versions. Results for 8 of the questions are discussed in this article, and results for the other 2 are presented in the Supplemental Material available online. Specifically, here we discuss the respondents' descriptions of how they generally determined their sample size (researcher condition only, because answers to the corresponding question in the reviewer condition were hard to classify) and their assessments of the acceptable Type I error rate, the power level regarded as satisfactory, the cell size typically considered sufficient, and the typically expected ES (in Cohen's d ) for an independent-samples t test. The design did not involve any additional dependent or independent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Deciding on sample size. A total of 197 respondents answered the open question on determination of sample size from a researcher's perspective (note that for this analysis, we included answers from respondents who did not finish the survey). Two independent raters scored whether the answers could be assigned to one or more of five different categories. The raters agreed in 93% of the cases (Cohen's Îº = .80). A power analysis was mentioned by 93 (47%) of the respondents (although 20 of these respondents, or 22%, also mentioned practical constraints, such as available time and money). Overall, 40 respondents (20%) stated that practical constraints determined their sample size. Furthermore, 45 respondents (23%) mentioned some rule of thumb (e.g., 20 subjects per condition), 41 respondents (21%) based sample sizes on the common practice in their field of research, and 18 respondents (9%) wanted as many subjects as possible, to have the highest possible power to detect an effect.</p><p>The typical study. Because responses in the researcher condition were very similar to those in the reviewer condition, we present results for the two conditions combined (see the Supplemental Material for results separated by condition). As the distributions were not normal and included outliers (histograms and medians are presented in the Supplemental Material), we report the trimmed means (M t s; 20% trimming) and used robust statistics to increase power and to protect against an incorrect estimation of the Type I error rate <ref type="bibr" target="#b4">(Bakker &amp; Wicherts, 2014;</ref><ref type="bibr" target="#b37">Welch, 1938;</ref><ref type="bibr" target="#b39">Wilcox, 2012;</ref><ref type="bibr" target="#b40">Yuen, 1974)</ref>.</p><p>The average expected ES was 0.39, which is somewhat lower than the estimated mean ES obtained in large-scale meta-analyses of psychological research (d = 0.5 on average; <ref type="bibr" target="#b0">Anderson, Lindsay, &amp; Bushman, 1999;</ref><ref type="bibr" target="#b18">Lipsey &amp; Wilson, 1993;</ref><ref type="bibr" target="#b22">Meyer et al., 2001;</ref><ref type="bibr" target="#b27">Richard, Bond, &amp; Stokes-Zoota, 2003;</ref><ref type="bibr" target="#b33">Tett, Meyer, &amp; Roese, 1994)</ref>. However, these meta-analyses probably overestimated the mean ES because of the publication bias often present in metaanalyses <ref type="bibr" target="#b2">(Bakker et al., 2012)</ref>. Note that the average expected ES found in Study 1 is comparable to the (original) mean ES (d = 0.402) of 100 studies in psychology that were recently subjected to replication <ref type="bibr">(Open Science Collaboration, 2015)</ref>. The average acceptable cell size reported by our respondents was 34.6, which is somewhat higher than previous estimates of mean cell sizes based on the published literature (20-24 subjects; <ref type="bibr" target="#b19">Marszalek, Barber, Kohlhart, &amp; Holmes, 2011;</ref><ref type="bibr" target="#b38">Wetzels et al., 2011)</ref>. The average reported acceptable levels for Î± and power were .05 and .80, respectively. Responses to these questions, in particular, seemed to reflect a common standard, as 83% of our respondents reported that the acceptable Î± level is .05, and 69% reported that power of .80 is sufficient.</p><p>In answer to our question about determining sample size, one respondent indicated, "I usually aim for 20-25 subjects per cell of the experimental design, which is typically what it takes to detect a medium effect size with .80 probability." However, for an independent-samples t test with 20 to 25 subjects in each condition and d of 0.5 (medium ES), the actual power lies between .34 and .41, which is approximately half the power that the respondent mentioned. Considering that 53% of the respondents in the researcher condition indicated that they did not generally conduct power analyses and 23% reported using some rule of thumb, we wondered whether respondents' intuitive power analyses were accurate. To investigate this, we calculated the power of a study with Î±, ES, and cell size equal to the trimmed means obtained in Study 1, using the pwr package in R <ref type="bibr" target="#b6">(Champely, 2009)</ref>. Such a study would have power of .35. (When we calculated power separately for each respondent's reported values of Î±, ES, and n, we found that the trimmed mean power across respondents was .40.) We also calculated the required cell size given the trimmed means for Î±, ES, and power, and found that it would be 105 subjects, which is 3 times as many subjects as respondents' trimmed mean for n.</p><p>A robust within-subjects Yuen t test <ref type="bibr" target="#b39">(Wilcox, 2012;</ref><ref type="bibr" target="#b40">Yuen, 1974)</ref> indicated that respondents' reported acceptable power levels differed significantly from the calculated power based on their responses to the other questions, t(171) = 19.38, p &lt; .001, Î¾ = .82, 95% confidence interval (CI) for the difference = <ref type="bibr">[.36, .44</ref>]. We also calculated the bias for each respondent individually (calculated power -reported power). The trimmed mean bias was -.34; 80% of the respondents showed a negative bias (calculated power lower than desired power), and 33% showed a negative bias with an absolute value larger than .5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2</head><p>A majority of the respondents in Study 1 reported that power of .80 is satisfactory, and this is the common standard advised by <ref type="bibr" target="#b7">Cohen (1965)</ref> and other researchers. Hence, it might be that our respondents gave the normative answer even though they knew that it was not in accordance with the other values they reported. The goal of Study 2 was to measure researchers' power intuitions more directly, by asking them to estimate the power of research designs with Î±, ES, and N specified. Additionally, we presented examples of research designs with Î± and ES specified, and asked respondents to estimate the number of subjects needed to reach a power of .80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Subjects. We collected all e-mail addresses of the corresponding authors of articles published in 2014 in the same journals as used in Study 1. After removing 1 duplicate e-mail address and 1 e-mail address from a lab member familiar with the hypotheses, we invited 1,625 researchers to participate in our online survey on "statistical intuitions" in February 2015. We did not conduct a formal power analysis because we considered this sample sufficiently large for the purposes of estimation.</p><p>Of all the contacted researchers, 404 (24.9%) started the survey, and 214 (53.0%) of those who started the survey completed it. Respondents were randomly assigned to one of three sample-size versions of the survey. Sixtyseven respondents completed the small-N version (N = 40), 81 completed the medium-N version (N = 80), and 66 completed the large-N version (N = 160). We report the results obtained when our analyses included only the respondents with complete data.</p><p>Survey. Our short survey contained 10 questions (available at Open Science Framework, <ref type="url" target="https://osf.io/5t0b7/">https://osf.io/5t0b7/</ref>). The first 3 asked the respondents to estimate the power of independent-samples (two-tailed) t tests in three research situations, which differed in the ES (Cohen's d = 0.20, 0.50, or 0.80); Î± was set at .05 throughout. Depending on the condition to which respondents were assigned, the total N specified was 40, 80, or 160. In the next 3 questions, we asked the respondents to estimate the sample sizes required for an independent-samples t test to have a power of .80 given expected ESs (Cohen's d ) of 0.20, 0.50, and 0.80; each ES was accompanied by the corresponding correlation (.10, .24, or .37, respectively), and Î± was again set at .05. Next, we tested respondents' understanding of what power is with a single multiple-choice question. Finally, we asked them to indicate how often they conducted a power analysis (7-point Likert scale), to assess their own statistical knowledge (10-point scale), and to indicate their main subfield of psychological research. The design did not involve any additional dependent or independent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Intuitions about power and sample size. We calculated the true power of the research designs presented to the respondents using the pwr package in R <ref type="bibr" target="#b6">(Champely, 2009)</ref>; these values are presented in Table <ref type="table" target="#tab_1">1</ref> and Figure <ref type="figure" target="#fig_0">1</ref>, along with the 20% trimmed means and 95% CIs for the respondents' estimates. Most respondents were not able to estimate the true power values well. The true power lay within the 95% CI for only one scenario in the medium-N condition (when d = 0.50) and one scenario in the small-N condition (when d = 0.80). The vast majority of respondents (89%) overestimated power for the small-ES scenario. This is especially worrisome given that small ESs are typically found in psychological research (Open Science Collaboration, 2015) and were reported as typical by respondents in Study 1. When the ES was large and N was greater than 80, respondents underestimated the power of the t test in the design we presented to them.</p><p>A comparable pattern was found when respondents estimated the sample sizes required to obtain a power of .80 in an independent-samples t test, given a specific expected ES. Table <ref type="table" target="#tab_2">2</ref> gives the true sample sizes needed in these cases along with respondents' estimates (20% trimmed means). When the expected ES was large, respondents overestimated the required sample size by about 25 subjects, on average. Respondents' mean estimate was quite close to the actual value when the expected ES was medium. When the expected ES was small, however, the required sample size was underestimated by 95% of the respondents. Whereas respondents estimated, on average, that 216 subjects were needed, 788 subjects would actually be needed to obtain sufficient power in the case of such a small effect. Given that respondents in Study 1 indicated that their typical ES was around 0.4, on average, our results suggest that researchers typically underestimate the sample sizes needed for studying effects that they deem to be typical. Unexpectedly, we did find a difference in sample-size estimates among the three conditions: Respondents in the large-N condition gave the highest estimates. This might be a carryover effect from the questions asking respondents to estimate the power of research designs (e.g., effect of anchoring and adjustment; <ref type="bibr" target="#b9">Epley &amp; Gilovich, 2006)</ref>.</p><p>Other factors. To explore possible influences on respondents' power intuitions, we looked at the data from both studies. We focused especially on the small-ES situations, because these are common in psychology and also because respondent's intuitions were the least accurate for these situations. First, we found that respondents who reported doing power analyses to determine their sample sizes did not estimate power better than those who did not report conducting power analyses. Almost half of the respondents in the researcher condition in Study 1 indicated that they generally used a power analysis to determine their sample size (although they might not conduct a power analysis for every single study). The average calculated power for this group of respondents (M t = .46, 95% CI = [.37, .55]) was not significantly higher than that for the remaining respondents in the researcher condition (M t = .42, 95% CI = [.34, .51]). Furthermore, the amount of bias did not differ significantly between respondents who mentioned typically doing power analyses (M t = -.31, 95% CI = <ref type="bibr">[-.40, -.22]</ref>) and those who did not (M t = -.30, 95% CI = [-.39, -.22]).</p><p>Next, for Study 2, we used a principal components analysis to summarize respondents' answers to the questions regarding their understanding of what power means (question correctly answered by 168 respondents, or 78.5%), how often they conducted power analyses, and how good their statistical knowledge was. The first component explained 50% of the variance, and we used hierarchical regression analyses to investigate whether scores on this component predicted estimates of power and required sample sizes. (Separate results for the three questions, including full regression tables, are available in the Supplemental Material.) The dependent variables were the power and sample-size estimates for each presented research design. In the first model for each dependent variable, we included only component score as a predictor; in the second model, we added condition; and in the third model, we added the interaction between component score and condition. We report results based on Model 2, which was always selected on the basis of the change in R 2 except for predicting sample size in the small-ES scenario, in which case none of the models fitted the data. No interactions between condition and component score were found. We did not find a significant effect of component score on power estimates for the small-ES scenario (b = -0.01, t = -0.98, p = .329). However, when the ES was medium or large, respondents with higher component scores had higher (and hence more accurate) power estimates (b = 0.02, t = 2.26, p = .025, and b = 0.04, t = 3.94, p &lt; .001, respectively). Furthermore, when the specified ES was large, respondents with higher component scores gave smaller estimates of the sample size required to achieve a power of  .80 (b = -12.89, t = -2.56, p = .011), which again resulted in estimates closer to the true value. Component score did not significantly predict sample-size estimates when the ES was small or medium (b = 16.54, t = 1.33, p = .185, and b = -6.30, t = -1.05, p = .296, respectively).</p><p>In Study 1, respondents' self-reported statistical knowledge correlated with neither calculated power nor bias. In addition, robust regression analyses revealed that number of publications did not significantly predict either calculated power or bias.</p><p>Finally, we did not find any significant differences between research fields in Study 1 respondents' calculated power (full results are presented in the Supplemental Material). For Study 2, we combined cognitive psychology and neuroscience, and added the 2 respondents from forensic psychology to the "other" category, because of the small number of respondents in these categories. With a robust 3 (condition) Ã 9 (research field) two-way analysis of variance using the trimmed means, we tested for differences between research fields in power and sample-size estimates. We found that the research fields differed only in sample-size estimates for the situation in which the ES was small (F = 41.43, p = .006). These estimates were lowest for respondents from the fields of cognitive psychology and neuroscience and highest for respondents from personality and developmental psychology. However, the highest mean sample-size estimate (by the respondents from personality psychology) was 276, which is still far removed from the true required sample size of 788 in that scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>It has long been noted that the statistical power of studies in the psychological literature is typically too low <ref type="bibr" target="#b2">( Bakker et al., 2012;</ref><ref type="bibr" target="#b8">Cohen, 1990;</ref><ref type="bibr" target="#b20">Maxwell, 2004)</ref>. The results of the current studies, involving more than 500 psychology researchers, offer insight into why this may be so. Specifically, for studies of effects expected to have the most typical magnitude, respondents overestimated power and consequently underestimated the required sample size. When asked about how they normally determined sample sizes in their own studies, more than half of our respondents indicated that they did not use a power analysis, which may explain why such analyses are presented in fewer than 3% of psychological articles <ref type="bibr" target="#b3">(Bakker &amp; Wicherts, 2011)</ref>. Much research in psychology appears to be planned without formal power analysis, and many researchers appear to use rather intuitive approaches in determining their sample sizes.</p><p>In our first study, the calculated power based on respondents' reported acceptable sample sizes and expected ESs was only half of the power respondents indicated they wanted to achieve. The power intuitions of more than 75% of respondents resulted in calculated power that was lower than desired. Results were similar for respondents who answered as researchers and those who answered as reviewers. In our second study, 89% of respondents overestimated the power of studies with small expected ESs, and 95% underestimated the sample size required for sufficient power when the ES was small. When the expected ES was small, the true sample size needed to reach a power of .80 was more than 3 times the respondents' mean estimate of the required sample size. This is worrisome, as the results of our first study and replication studies show that ESs are often quite small in psychology <ref type="bibr">(Open Science Collaboration, 2015)</ref>. In combination with publication bias, the (strategic) use of small sample sizes and research designs that are underpowered results in inflated Type I error rates, biased ES estimates, distorted meta-analytical results, and nonreplicable findings <ref type="bibr" target="#b2">(Bakker et al., 2012;</ref><ref type="bibr">Open Science Collaboration, 2015)</ref>.</p><p>Even researchers who stated that they typically used formal power analyses had poor power intuitions. In line with earlier work showing the same poor statistical intuitions among general and mathematical psychologists <ref type="bibr" target="#b34">(Tversky &amp; Kahneman, 1971)</ref>, our studies indicate that greater self-reported statistical knowledge and experience are not related to better power intuitions in the most common cases (when the ES is small). Only when the underlying ES was large did we see some apparent advantage of knowledge and experience. In our second study, we found a small difference between research fields in the estimates of required sample sizes when the ES was small. However, the true required sample size is more than 2.5 times the mean estimate of respondents from the research field with the highest sample-size estimate (personality psychology).</p><p>We focused on a between-subjects experimental design because it is a common and basic design in psychology. Nevertheless, it is possible that some of our respondents were more familiar with other research designs that have different associations between sample size and power (e.g., within-subjects designs are typically more powerful). However, if experience with research designs had influenced our results, power intuitions should have differed more between subfields that typically use different research designs. Future research could focus on power intuitions related to other research designs, such as within-subjects and correlational designs. We also found some evidence for carryover effects. However, the questions calling for power estimates and the questions calling for sample-size estimates showed the same pattern of results: large discrepancies between estimated and actual values in all conditions when the ES was small. Furthermore, the response rate in both studies was quite low (26% and 13%, respectively), and researchers who are knowledgeable about power are probably overrepresented in this sample because of their interest in the subject. Therefore, we expect that a more balanced sample would show even larger overestimation of power and underestimation of required sample sizes in research designs.</p><p>Poor intuitions about power may lead to incorrect inferences concerning nonsignificant results. Researchers often conduct multiple small (and therefore likely underpowered) studies of the same underlying phenomenon <ref type="bibr" target="#b12">(Francis, 2014;</ref><ref type="bibr">Hartgerink, Wicherts, &amp; van Assen, 2015)</ref>. Given the flawed power intuitions we observed, it is quite likely that researchers dismiss nonsignificant outcomes in such studies as due to methodological flaws (i.e., "failed studies") or feel inclined to interpret nonsignificant outcomes as reflecting a true null effect, although in fact these outcomes might be false negatives <ref type="bibr">( Hartgerink et al., 2015;</ref><ref type="bibr" target="#b21">Maxwell, Lau, &amp; Howard, 2015)</ref>. These small (often exploratory rather than confirmatory; <ref type="bibr" target="#b36">Wagenmakers, Wetzels, Borsboom, van der Maas, &amp; Kievit, 2012)</ref> studies should be combined within a meta-analysis to estimate an underlying mean effect (and confidence interval) and to ascertain whether there is heterogeneity in the underlying ES <ref type="bibr" target="#b2">(Bakker et al., 2012)</ref>.</p><p>Our results lead us to the following recommendations for using NHST. First, researchers should always conduct a formal power analysis when planning a study (preferably, such an analysis would be part of an institutional review board's approval or part of preregistration of the study), and they should report this power analysis in their manuscript, together with a description of their sample. This will force researchers to explicate their samplesize decisions and will likely lead to better-powered studies. Second, considering that often no appropriate ES estimation is available and that our results indicate that intuitions for exponential power functions are often suboptimal and potentially linear, we recommend that power analyses be accompanied by inspection of the implications of a range of ES estimates, especially at the lower end of this range. This will help researchers understand the exponential relations involved in statistical power and the considerable impact of seemingly small changes in ES estimates (see also <ref type="bibr" target="#b26">Perugini, Gallucci, &amp; Costantini, 2014)</ref>. Third, reviewers should check whether indeed a formal power analysis has been conducted <ref type="bibr" target="#b1">(Asendorpf et al., 2013)</ref> and whether it is sound. Fourth, confirmatory studies, or core studies in a research line, should be sufficiently powerful and preregistered <ref type="bibr" target="#b1">(Asendorpf et al., 2013;</ref><ref type="bibr" target="#b36">Wagenmakers et al., 2012)</ref>. If researchers conduct exploratory studies or analyses, these should be presented as such and possibly combined in a meta-analysis to provide estimates of the mean effect and possible heterogeneity of effects <ref type="bibr" target="#b2">(Bakker et al., 2012)</ref>.</p><p>In the current climate of debate about replicability, reproducibility, and reporting standards, researchers and reviewers should collaborate in assessing the reliability of research results <ref type="bibr" target="#b1">(Asendorpf et al., 2013)</ref>. Both parties may misestimate the power of studies, regardless of their self-assessed statistical expertise. There is really only one way to improve studies: power them up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Editor</head><p>D. Stephen Lindsay served as action editor for this article.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Results from Study 2: respondents' mean estimate (20% trimmed mean) of the power of the presented research design for each combination of sample size and expected effect size. The error bars represent 95% confidence intervals, and the lines indicate the true power of studies with the three expected effect sizes as a function of total sample size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>ResultsFrom Study 2: Respondents' Estimates of Power and the True Power for the Research Designs</figDesc><table><row><cell></cell><cell cols="2">d = 0.20 (small ES)</cell><cell cols="2">d = 0.50 (medium ES)</cell><cell cols="2">d = 0.80 (large ES)</cell></row><row><cell>N</cell><cell>True power</cell><cell>Estimated power</cell><cell>True power</cell><cell>Estimated power</cell><cell cols="2">True power Estimated power</cell></row><row><cell>40</cell><cell>.09</cell><cell>.240 [.177, .303]</cell><cell>.34</cell><cell>.459 [.414, .503]</cell><cell>.69</cell><cell>.660 [.612, .709]</cell></row><row><cell>80</cell><cell>.14</cell><cell>.344 [.302, .386]</cell><cell>.60</cell><cell>.578 [.534, .622]</cell><cell>.94</cell><cell>.768 [.726, .811]</cell></row><row><cell>160</cell><cell>.24</cell><cell>.504 [.439, .570]</cell><cell>.88</cell><cell>.736 [.690, .782]</cell><cell>&gt; .99</cell><cell>.876 [.842, .909]</cell></row></table><note><p>Note: The table presents the 20% trimmed means of the power estimates, with 95% confidence intervals inside brackets. ES = effect size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>ResultsFrom Study 2: Respondents' Estimates of the Required Sample Size and the True Required Sample Size to Reach a Power of .8</figDesc><table><row><cell>Required sample size</cell><cell>d = 0.20 (small ES)</cell><cell>d = 0.50 (medium ES)</cell><cell>d = 0.80 (large ES)</cell></row><row><cell>True</cell><cell>788</cell><cell>128</cell><cell>52</cell></row><row><cell>Estimated</cell><cell>216 [196, 236]</cell><cell>124 [114, 134]</cell><cell>77 [72, 83]</cell></row></table><note><p>Note: The table presents the 20% trimmed means of the sample-size estimates, with 95% confidence intervals inside brackets. ES = effect size.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Marcel van Assen</rs> for his comments on an early version of the first survey, and <rs type="person">Linda Dominguez Alvarez</rs> for rating the first question of the first survey.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>The preparation of this article was supported by Grants <rs type="grantNumber">400-08-214</rs> and <rs type="grantNumber">452-11-004</rs> from the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9mqWspY">
					<idno type="grant-number">400-08-214</idno>
				</org>
				<org type="funding" xml:id="_fPR3gbb">
					<idno type="grant-number">452-11-004</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>M. Bakker developed the study concept. All the authors contributed to the study design. Data collection was performed by M. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://pss.sagepub.com/content/by/supplemental-data">http://pss  .sagepub.com/content/by/supplemental-data</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices</head><p>All data and materials have been made publicly available via Open Science Framework and can be accessed at osf.io/5t0b7. The complete Open Practices Disclosure for this article can be found at <ref type="url" target="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ref>. This article has received badges for Open Data and Open Materials. More information about the Open Practices badges can be found at <ref type="url" target="https://osf.io/tvyxz/wiki/1.%20View%20the%20">https://osf.io/tvyxz/wiki/1.%20View%20the%20</ref> Badges/ and <ref type="url" target="http://pss.sagepub.com/content/25/1/3.full">http://pss.sagepub.com/content/25/1/3.full</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Research in the psychological laboratory</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Bushman</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-8721.00002</idno>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recommendations for increasing replicability in psychology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Asendorpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Conner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Fruyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Houwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J A</forename><surname>Denissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1002/per.1919</idno>
	</analytic>
	<monogr>
		<title level="j">European Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The rules of the game called psychological science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612459060</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="543" to="554" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The (mis)reporting of statistical results in psychology journals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-011-0089-5</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="666" to="678" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Outlier removal, sum scores, and the inflation of the Type I error rate in independent samples t tests: The power of alternatives and recommendations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000014</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="409" to="427" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Power failure: Why small sample size undermines the reliability of neuroscience</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mokrysz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>MunafÃ²</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn3475</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">pwr: Basic functions for power analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Champely</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/pwr/pwr.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Version 1.1.1 Computer software</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Some statistical issues in psychological research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of clinical psychology</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Wolmann</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1965">1965</date>
			<biblScope unit="page" from="95" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Things I have learned (thus far)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.45.12.1304</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The anchoring-and-adjustment heuristic: Why the adjustments are insufficient</title>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2006.01704.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Positive&quot; results increase down the hierarchy of the sciences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0010068</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10068</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The long way from Î±-error control to validity proper: Problems with a shortsighted false-positive debate</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kutzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Krueger</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612462587</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="661" to="669" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The frequency of excess success for articles in Psychological Science</title>
		<author>
			<persName><forename type="first">G</forename><surname>Francis</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-014-0601-x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1180" to="1187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mindless statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.socec.2004.09.033</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Socio-Economics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="587" to="606" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comment on &quot;Estimating the reproducibility of psychological science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pettigrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aad7243</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="page">1037</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Consequences of prejudice against the null hypothesis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0076157</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<ptr target="osf.io/qpfnw" />
		<title level="m">Too good to be false: Non-significant results revisited</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Project files Retrieved from</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring the prevalence of questionable research practices with incentives for truth telling</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prelec</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611430953</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="524" to="532" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The efficacy of psychological, educational, and behavioral treatment: Confirmation from meta-analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lipsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.48.12.1181</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1181" to="1209" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sample size in psychological research over the past 30 years</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.2466/03.11.pms.112.2.331-348</idno>
	</analytic>
	<monogr>
		<title level="j">Perceptual and Motor Skills</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="331" to="348" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The persistence of underpowered studies in psychological research: Causes, consequences, and remedies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
		<idno type="DOI">10.1037/1082-989X.9.2.147</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is psychology suffering from a replication crisis? What does &quot;failure to replicate&quot; really mean</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0039400</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Psychological testing and psychological assessment: A review of evidence and issues</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Eyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Dies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.56.2.128</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="128" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Null hypothesis significance testing: A review of an old and continuing controversy</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nickerson</surname></persName>
		</author>
		<idno type="DOI">10.1037//1082-989x.5.2.241</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="301" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The prevalence of statistical reporting errors in psychology (1985-2013)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H J</forename><surname>Hartgerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-015-0664-2</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Advance online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating the reproducibility of psychological science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
		<idno type="DOI">10.1126/science.aac4716</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page">943</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Safeguard power as a protection against imprecise power estimates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perugini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gallucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Costantini</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691614528519</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="319" to="332" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">One hundred years of social psychology quantitatively described</title>
		<author>
			<persName><forename type="first">F</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Stokes-Zoota</surname></persName>
		</author>
		<idno type="DOI">10.1037/1089-2680.7.4.331</idno>
	</analytic>
	<monogr>
		<title level="j">Review of General Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="331" to="363" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The file drawer problem and tolerance for null results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.86.3.638</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="638" to="641" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian t tests for accepting and rejecting the null hypothesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<idno type="DOI">10.3758/PBR.16.2.225</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do studies of statistical power have an effect on the power of studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sedlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno type="DOI">10.1037//0033-2909.105.2.309</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="309" to="316" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Falsepositive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611417632</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1359" to="1366" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Publication decisions revisited: The effect of the outcome of statistical tests on the decision to publish and vice versa</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinkam</surname></persName>
		</author>
		<idno type="DOI">10.1080/00031305.1995.10476125</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="108" to="112" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Applications of meta-analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Tett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Roese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1987" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Belief in the law of small numbers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0031322</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="105" to="110" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Why psychologists must change the way they analyze their data: The case of psi: Comment on Bem</title>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0022790</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="426" to="432" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An agenda for purely confirmatory research</title>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612463078</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="632" to="638" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The significance of the difference between two means when the population variances are unequal</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="350" to="362" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Statistical evidence in experimental psychology: An empirical comparison using 855 t tests</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611406923</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Modern statistics for the social and behavioral sciences: A practical introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Wilcox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The two-sample trimmed t for unequal population variances</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yuen</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/61.1.165</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="165" to="170" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
