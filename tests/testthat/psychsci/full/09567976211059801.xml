<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Language of Social Touch Is Intuitive and Quantifiable</title>
				<funder>
					<orgName type="full">Swedish Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sarah</forename><surname>Mcintyre</surname></persName>
							<email>sarah.mcintyre@liu.se</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">C</forename><surname>Hauser</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<addrLine>3 Facebook</addrLine>
									<settlement>Redmond</settlement>
									<region>Washington;</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anikó</forename><surname>Kusztor</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rebecca</forename><surname>Boehme</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Athanasia</forename><surname>Moungou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peder</forename><surname>Mortvedt Isager</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lina</forename><surname>Homman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Novembre</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saad</forename><forename type="middle">S</forename><surname>Nagi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Israr</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ellen</forename><forename type="middle">A</forename><surname>Lumpkin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Physiology and Cellular Biophysics</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Freddy</forename><surname>Abnousi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Gerling</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<addrLine>3 Facebook</addrLine>
									<settlement>Redmond</settlement>
									<region>Washington;</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Håkan</forename><surname>Olausson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Social and Affective Neuroscience</orgName>
								<orgName type="department" key="dep2">Department of Biomedical and Clinical Sciences</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Social and Affective Neuroscience</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Language of Social Touch Is Intuitive and Quantifiable</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B33B08660FC82538C9E1A361E136CC6C</idno>
					<idno type="DOI">10.1177/09567976211059801</idno>
					<note type="submission">Received 1/30/20; Revision accepted 10/26/21</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>touch</term>
					<term>social interaction</term>
					<term>emotions</term>
					<term>facial expressions</term>
					<term>communication</term>
					<term>open data</term>
					<term>open materials</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Touch is a powerful communication tool, but we have a limited understanding of the role played by particular physical features of interpersonal touch communication. In this study, adults living in Sweden performed a task in which messages (attention, love, happiness, calming, sadness, and gratitude) were conveyed by a sender touching the forearm of a receiver, who interpreted the messages. Two experiments (N = 32, N = 20) showed that within close relationships, receivers could identify the intuitive touch expressions of the senders, and we characterized the physical features of the touches associated with successful communication. Facial expressions measured with electromyography varied by message but were uncorrelated with communication performance. We developed standardized touch expressions and quantified the physical features with 3D hand tracking. In two further experiments (N = 20, N = 16), these standardized expressions were conveyed by trained senders and were readily understood by strangers unacquainted with the senders. Thus, the possibility emerges of a standardized, intuitively understood language of social touch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpersonal touch is important for social interaction, precedes language as a form of communication in infants <ref type="bibr" target="#b10">(Hertenstein, 2002)</ref>, and is linked to the evolutionary development of language <ref type="bibr" target="#b4">(Dunbar, 1993)</ref>. In nonhuman primates, touch in the form of grooming is used for reconciliation after conflicts and promotes group cohesion <ref type="bibr" target="#b5">(Dunbar, 2010)</ref>. The way that adult humans use interpersonal touch may be governed by this evolutionary and developmental heritage. That is, there may exist neural infrastructure that is related to how the particular features of interpersonal touch (e.g., tapping, stroking, holding) are used for different purposes (e.g., getting someone's attention, soothing, expressing affection). However, it is currently unknown whether humans communicate via a common repertoire of touch gestures or what determines successful communication.</p><p>There are challenges to investigating interpersonal touch in humans: Touch usually happens in private and complex social contexts <ref type="bibr" target="#b3">(Cekaite &amp; Kvist Holm, 2017)</ref> and is governed by the relationship between the people involved <ref type="bibr" target="#b35">(Suvilehto et al., 2015</ref><ref type="bibr" target="#b36">(Suvilehto et al., , 2019))</ref>. Nonetheless, several studies have shown that touch alone effectively communicates emotions such as anger, fear, disgust, love, gratitude, sympathy, happiness, and sadness <ref type="bibr" target="#b11">(Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b13">Hertenstein, Verkamp, et al., 2006)</ref>. Indeed, touch is the preferred channel over body language and facial expressions for expressing both love and sympathy <ref type="bibr" target="#b0">(App et al., 2011)</ref>. Happiness, sadness, anger, and disgust are also successfully communicated, despite touch not being the preferred communication channel, likely because redundancy improves communication success <ref type="bibr" target="#b0">(App et al., 2011)</ref>.</p><p>An outstanding question is to what extent interpersonal touch strategies are universal, culturally determined, or idiosyncratic. One study showed that couples are more effective at touch communication than strangers <ref type="bibr" target="#b38">(Thompson &amp; Hampton, 2011)</ref>, suggesting that couples may develop an idiosyncratic set of touch expressions, or touch language. Other studies described the physical features of social touch <ref type="bibr" target="#b11">(Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b16">Jung et al., 2015;</ref><ref type="bibr" target="#b20">Masson &amp; Op de Beeck, 2018)</ref> but did not evaluate the different strategies for communication effectiveness. A recent study showed that the speed of stroking and the touched body location influence the touch receiver's judgments about the emotional state and intentions of the person touching them, even when they are not trying to communicate a specific message <ref type="bibr" target="#b17">(Kirsch et al., 2018)</ref>. These findings suggest that some physical features of interpersonal touch could be reliably mapped to different messages.</p><p>Observing facial expressions can be a source of additional information for the receiver of a touch communication <ref type="bibr" target="#b40">(Tsalamlal et al., 2015</ref><ref type="bibr" target="#b39">(Tsalamlal et al., , 2018))</ref>, and facial muscle activity changes in a way that is consistent with positive affect when people are gently stroked by a soft brush <ref type="bibr" target="#b22">(Mayo et al., 2018;</ref><ref type="bibr" target="#b25">Pawling et al., 2017;</ref><ref type="bibr" target="#b31">Ree et al., 2020)</ref>. It is not known whether there is a link between the affective state as measured by facial muscle activity and the capacity to either convey or interpret messages via touch.</p><p>In the current study, we investigated whether it is possible to optimize touch strategies by selecting specific physical features, so that touch messages can be understood broadly within one cultural context, outside of a close relationship. We first identified, within close relationships, the physical features of interpersonal touch (e.g., slow stroking, tapping with a finger) that were most associated with success and failure to communicate cued touch messages. We also investigated facial expressions that accompany touch communication, using facial electromyography (EMG), and tested whether variations in facial expressions were related to performance on the touch-communication task.</p><p>We then developed standardized touch expressions on the basis of the most successful touch features that we identified for each message. These were delivered by trained experimenters to strangers, and performance on the communication task was compared with that between people in a preexisting relationship. We also present a quantitative description in terms of motion and contact area characteristics for the standardized touch expressions. Data sets, analysis scripts, and other materials associated with this study are available on OSF (<ref type="url" target="https://osf.io/cnj68/">https://osf.io/cnj68/</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We recruited pairs of participants with an existing close relationship (Table <ref type="table" target="#tab_0">1</ref>) to participate in our touch-communication task (Fig. <ref type="figure">1a</ref>). The participants assigned to the role of sender were required to communicate a series of messages using touch alone, making any gestures they felt were appropriate. The receiver then had to identify the message from a list of options. Because we gave no instructions about the touch strategies to use, we refer to this as "intuitive" touch communication to contrast it with the standardized touch strategies that we later developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>For the intuitive-touch experiments (Experiments 1 and 2), we recruited pairs of adult participants with a preexisting relationship in which they felt emotionally close and were comfortable touching each other in a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>Communicating via touch is a precursor of spoken language-both in an evolutionary sense and in human infant development. In adult life, touch is used to complement verbal communication, adding intimacy and emotion. It has been suggested that people in close relationships may use idiosyncratic techniques for communicating emotions via touch. This is because of research showing that romantic couples performed better at a touchcommunication task than strangers did. In this work, we found that strangers can communicate using touch just as well as, or even better than, people in close relationships when appropriate touch strategies are used. We employed a standard set of touch strategies for expressing nonverbal social messages such as love, attention, happiness, sadness, gratitude, and calming. Using 3D hand tracking, we made precise measurements of the contact area, velocity, and duration of our standard set of touch expressions. These expressions were intuitively understood by strangers, similar to the way that emojis rely on a common understanding of facial expressions. normal social context. For the standardized-touch experiments (Experiments 3 and 4), we recruited participants individually. Table <ref type="table" target="#tab_0">1</ref> provides a detailed description of our sample and an overview of the experiments. Participants were recruited from online and poster advertisements targeting the general community and the university community at Linköping University, Sweden. This is an international community, but we did not collect any data on the cultural background of our participants. All participants provided informed consent, and the study was conducted in accordance with the regulations of the regional ethics committee that approved the study. In addition to completing the touch-communication task, participants completed a number of questionnaires, detailed in the Supplemental Material available online.</p><p>In Experiments 1, 3, and 4, the target number of participants was 15 to 20, based on previous research <ref type="bibr" target="#b23">(McIntyre et al., 2019)</ref> in which 95% confidence intervals (CIs) of approximately ±10% performance were achieved at the cue level. For Experiment 2, the target was set at 20 participants, the recommended minimal sample size for functional-imaging studies <ref type="bibr" target="#b24">(Murphy &amp; Garavan, 2004)</ref>. The number of trials per participant was based on trying to maximize the number of trials while staying within an acceptable duration for the participants. This was smaller for Experiment 1 because of the time taken to apply facial EMG electrodes. For Experiment 2, we initially recruited 22 pairs, but one pair had to be excluded from the final analysis because they did not follow the task instructions. Another pair ended the experiment early because the person in the scanner felt claustrophobic. In Experiments 3 and 4, some trials were not performed because of experimenter or technical error. In Experiment 3, there were three participants for which only 70, 72, and 78 of the intended 90 trials were obtained; in Experiment 4, there were two participants for which only 118 of the intended 120 trials were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intuitive-touch-communication task</head><p>We chose six messages to cue: attention, love, calming, happiness, sadness, and gratitude (see Table <ref type="table" target="#tab_2">2</ref> for fullsentence cues). We included a mix of touch messages and required only that participants were sufficiently acquainted to be comfortable touching each other. Our goal was to try to capture a variety of different interpersonal touch interactions that may occur in natural settings. Some of the cues communicate the emotional state of the sender (happiness, sadness), some attempt to elicit a response from the receiver (attention, calming), and some refer to an existing social context (love, gratitude, calming). These are messages that people generally want to communicate using touch and were adapted from those used in a previous study <ref type="bibr" target="#b23">(McIntyre et al., 2019)</ref> to reduce conceptual overlap between messages. Gender 15 female, 17 male 9 female, 11 male; partners: 11 female, 9 male 9 female, 11 male 5 female, 11 male Relationships 10 romantic, (9 female-male, 1 male-male), 5 friendships, (2 female-female, 2 malemale, 1 female-male), 1 family (female-male)</p><p>9 romantic (1 femalefemale, 8 female-male), 11 friendships (5 female-female, 6 male-male) 20 strangers (1 experimenter, 20 participants) 16 strangers (2 experimenters, 16 participants) Note: EMG = electromyography. a Data for this measurement are presented in a separate article. b There were small deviations from the intended number of trials (see the text for details).</p><p>72 3 3 1 0 1 0 2 39 6 1 12 1 19 13 15 24 9 9 6 4 22 4 4 46 1 2 1 0 24 2 0 43 0 11 0 17 14 1 10 1 37 Oth Gra Sad Cal Hap Lov Att Cued Word Receiver Response % 0 25 50 75 100 Exp. 1: Initial Roles b 69 0 1 8 0 2 0 0 51 7 0 19 0 3 7 12 39 7 1 2 12 10 3 6 58 0 0 3 0 7 7 0 46 0 20 1 21 8 0 9 3 38 Oth Gra Sad Cal Hap Lov Att Cued Word Receiver Response % 0 25 50 75 100 Exp. 1: Swapped Roles c d 370 4 6 10 2 5 3 2 176 27 11 126 46 12 25 57 175 61 30 40 12 38 19 65 230 13 28 7 4 82 40 35 193 34 12 6 92 51 8 61 161 21 Tim Gra Sad Cal Hap Lov Att Cued Word Receiver Response % 0 25 50 75 100 Experiment 2 e f</p><p>Cue Sender Receiver Curtain Experimental Setup a 0.00 0.25 0.50 0.75 1.00 0.00 Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra 0.25 0.50 0.75 1.00 Performance F1 -Initial Roles -Swapped Roles Experiment 1 Performance F1</p><p>Experiment 2 chance chance Fig. <ref type="figure">1</ref>. Touch communication between people in a close relationship (intuitive touch). The experimental setup is illustrated in (a). The sender was presented with a cue and then touched the receiver to communicate the message. The receiver used a separate interface to record the message they thought their partner was sending. Participants could not see each other's monitors or faces. Group-level confusion matrixes for Experiment 1 are shown separately for trials (b) in which participants within pairs were in their initially assigned roles (sender and receiver) and (c) after the participants swapped roles. Each square represents a unique cue/response combination.</p><p>The number in the square indicates the total number of times that unique combination occurred, pooled across participants. The shading indicates the percentage of occurrences that response was made out of the total number of times that cue was presented (columns sum to 100% shading). A dark diagonal indicates good performance on the communication task, and shading off the diagonal indicates consistent mistakes. F1 performance score (minimum = 0, maximum = 1; given by the harmonic mean of recall and precision) is shown Participants were assigned roles: one as the sender, the other as the receiver. The sender's task was to send messages by touching the receiver's arm. The receiver's task was to guess what the message was. The receiver sat in a chair (Experiment 1) or lay in the MRI scanner (Experiment 2; MRI data will be presented in a separate article; <ref type="bibr" target="#b1">Boehme et al., 2017)</ref> with one arm resting passively, accessible to the sender. The sender and receiver could not see each other's faces, which were obscured by a curtain (Experiment 1) or the MRI scanner (Experiment 2). Participants were instructed to remain quiet throughout the touch-communication task, not talking or making any other noise such as laughing or sighing. The touch-communication task was guided by a custom Python script using PsychoPy (Version 1.83.04; <ref type="bibr" target="#b28">Peirce, 2007)</ref>. On each trial, the sender was presented with one of the sender cues shown in Table <ref type="table" target="#tab_2">2</ref>. The sender was told to touch only the forearm and that they could perform any kind of touch that they felt was appropriate. After each touch was performed, the receiver was presented with buttons on a screen labeled with all of the receiver response options shown in Table <ref type="table" target="#tab_2">2</ref> and asked to select one to indicate what they thought the sender was communicating. The cues were presented in a pseudorandom order, and the locations of the buttons on the screen were shuffled on every trial. No feedback was provided.</p><p>There were some minor differences in how the task was conducted in Experiments 1 and 2. In Experiment 1, the sender could take as long as they wanted to perform the touch and controlled the pace of the experiment. After performing the touch-communication task, the participants switched roles so that the sender became the receiver and vice versa, and the touch-communication task was performed with the new roles. The experimental session consisted of two blocks, one for initial roles and one for swapped roles, of 30 trials each. Additionally, each cue was also presented once at the beginning of the first block (initial roles), in which the receivers could type in open-ended responses instead of being offered the forced-choice buttons. In addition to the touch-communication task, facial EMG responses of the participants were measured, and the touches were recorded on video (details below).</p><p>In Experiment 2, the task was conducted while the receiver was lying in an MRI scanner, and so the task was modified slightly. In addition to being given the instructions described above, the senders were told not to perform any shaking movements in order to avoid movement artifacts. Additionally, they were required to perform each touch for exactly 10 s to facilitate analysis of the functional MRI data. To control the timing of the experiment, we provided the sender with the single word cues (e.g., "attention") via headphones, followed by a countdown to "go" and "stop" signals to start and stop touching. So the senders had time to consider how to perform the touch, as they did in Experiment 1, the senders were familiarized with the full cues (Table <ref type="table" target="#tab_0">1</ref>) Your partner is trying to get your ATTENTION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Love</head><p>Think of all the wonderful qualities that your partner has and how they enrich your life. Try to express LOVE through touch.</p><p>before entering the scanner. The receiver was not given the "other" option, but if they failed to answer within 7 s, a "time-out" response was recorded and the next trial was presented. The participants did not swap roles.</p><p>The experimental session consisted of two blocks of 60 trials each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardized-touch-communication task</head><p>In the expert-touch experiments, the participant was always the receiver and a trained experimenter was the sender. On the basis of the video observations of the common physical features of touch-communication behavior between people in a close relationship (Experiment 1), we developed a set of 10-s standardized touch gestures for each of the six emotional messages (see Movies S1-S6 in the Supplemental Material). These standardized gestures were applied to the forearm by trained experimenters, who received spoken cues via headphones.</p><p>In Experiment 3, one trained experimenter performed all the gestures (Expert 1), and the experimental session consisted of one block of 90 trials. In Experiment 4, two different trained experimenters performed the gestures (Experts 2 and 3), and the experimental session consisted of two blocks of 60 trials each, usually with a different expert performing the touches in the two blocks. Because people tend to find touch from women more acceptable <ref type="bibr" target="#b36">(Suvilehto et al., 2019)</ref>, the trained experimenters were all female.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis for touch communication</head><p>To evaluate performance on the touch-communication task, we used F1 as the outcome measure. F1 is a measure of performance that is suited to classification tasks in which there are more than two categories and is appropriate when both misses and false alarms are equally undesirable. F1 is the harmonic mean of recall and precision, F1 = 2 × (precision × recall)/(precision + recall). Recall is the rate of cues correctly identified when present (also called hit rate), and precision is the rate of cues correctly identified when the receiver chose that cue. A separate set of analyses was also conducted using recall as the dependent variable, which yielded similar results and patterns of significance (not reported).</p><p>We conducted separate analyses for each experiment using a generalized linear mixed-effects model with a logit link function for F1 performance score as the dependent variable. For Experiment 1, we used fixed effects for cued word and roles (initial or swapped) and a random intercept term for participant. For Experiments 2 to 4, we used a fixed effect for cued word and a random intercept term for participant. When comparing intuitive and standardized touch, we used fixed effects for touch type (intuitive or expert) and cued word and a random intercept term for participant. To test for significance, we used parametric bootstrapping with 1,000 samples to produce likelihood ratios for full versus reduced models. Because we did not have the same number of judgments for receivers in different experiments, the weight term supplied to the model reflected the number of trials for each receiver-cue combination. Bonferroni-Holm correction was used for all post hoc tests comparing overall performance and performance of individual cues against chance, and for all post hoc pairwise comparisons, to obtain a familywise α of .05 separately for each model. Analyses were performed in the R programming environment (Version 4.0.3; R Core Team, 2018) using the package afex (Version 1.0-1; Singmann &amp; Kellen, 2019), and figures were created using the packages ggplot2 <ref type="bibr">(Version 3.3.3;</ref><ref type="bibr" target="#b43">Wickham, 2016)</ref> and patchwork (Version 1.1.1; <ref type="bibr" target="#b26">Pedersen, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial EMG</head><p>In Experiment 1, emotional expression in the face during the touch-communication task was assessed by measuring corrugator and zygomatic reactivity in response to the touch associated with each cue from both participants using a dual wireless EMG system (Dual Wireless EMG BioNomadix Pair; Biopac Systems, Goleta, CA). The corrugator supercilii muscle, which furrows the eyebrows, is active during frowning; stimuli with negative valence increase its activity, whereas stimuli with positive valence decrease its activity <ref type="bibr" target="#b19">(Larsen et al., 2003)</ref>. The zygomaticus major muscle, which lifts the cheeks and lips, is active during smiling and is associated primarily with positive affect <ref type="bibr" target="#b19">(Larsen et al., 2003)</ref> and particularly with high-arousal positive affect <ref type="bibr" target="#b7">(Fujimura et al., 2010)</ref>. Facial EMG of the corrugator and zygomatic muscles was measured in microvolts. EMG was measured over the left brow and cheek and a ground sensor on the forehead <ref type="bibr" target="#b6">(Fridlund &amp; Cacioppo, 1986)</ref> with 4-mm Ag/AgCl electrodes. Impedances were kept below 20 kΩ. EMG signals were amplified, bandpass filtered at 10 Hz to 500 Hz, digitized at 1,000 Hz, band-stop filtered at 50 Hz, rectified, and integrated over 20 ms by using EMG100C amplifiers and MP160 Data Acquisition System and AcqKnowledge software (Version 5.0.5; Biopac Systems, 2015). The script guiding the touch-communication task simultaneously delivered triggers to the EMG data-capture system via parallel port and played an audio signal that was captured by the video recording. This synchronizing signal was sent each time the sender pressed a button on the keyboard to indicate that they were about to start performing the touch. Research assistants then manually inspected the video recordings of every trial to find the frame following this signal on which the touch actually started. We then used the time delay from the synchronization signal to align the touch onset with the facial EMG recording. The video recordings were made at 25 frames per second, giving us a precision of 40 ms for identifying the onset of the touch (the audio was sampled at 44.1 kHz and the facial EMG at 1 kHz, meaning the visual onset of the touch had the lowest temporal resolution). The exact timing of the onset and offset for the touches was determined in this way for all participants, except for two blocks that lacked video data. In this case, the onset of touch was instead determined by the timing of the button press made by the sender to indicate they were about to start touching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis for facial EMG</head><p>For each trial, we extracted a 200-ms baseline preceding touch onset and a 4-s stimulation segment from touch onset. This time window was chosen on the basis of inspection of the data plotted over time (see Fig. <ref type="figure" target="#fig_3">S2</ref> in the Supplemental Material), which revealed that this window was long enough for effects to stabilize while still allowing sufficient data from shorter trials to be included. For the attention cue, which provoked the shortest touch durations, 35% of trials lasted longer than 4 s. The baseline period was between 1,200 ms and 1,000 ms before touch onset. This was because the sender's facial EMG responses had already started to change immediately before onset because they read the cue before starting to perform the touch (see Fig. <ref type="figure" target="#fig_3">S2</ref>). Automatic artifact rejection was applied to the data within the selected baseline and stimulus periods using a custom R script and based on a procedure previously validated in a large sample by comparison with manual inspection <ref type="bibr" target="#b18">(Künecke et al., 2014)</ref>. Data were flagged if the range within a sliding 50-ms window exceeded 3 times the standard deviation of the participant's full data set. Data within the selected time window were down-sampled to 100-ms bins, and a bin was rejected if it contained any flagged data samples. Trials were included in the analysis if after artifact rejection they had at least 10 bins remaining (i.e., a total of 1 s), including at least one from the baseline period. Out of 971 trials recorded, the following number of trials remained in the analysis after artifact rejection: 939 trials for the zygomaticus activity of the sender (attention: 155, love: 162, happiness: 148, calming: 161, sadness: 158, gratitude: 155), 954 trials for the zygomaticus activity of the receiver (attention: 151, love: 158, happiness: 162, calming: 162, sadness: 161, gratitude: 160), 924 trials for the corrugator activity of the sender (attention: 152, love: 160, happiness: 150, calming: 156, sadness: 154, gratitude: 152), and 935 trials for the corrugator activity of the receiver (attention: 149, love: 160, happiness: 156, calming: 157, sadness: 158, gratitude: 155).</p><p>The data were z scored within each participant for each muscle separately across all trials, thus resulting in the standardized activity of zygomaticus and corrugator for every participant. For each trial, the mean z score during the baseline phase was subtracted from the mean z score during the touch phase to obtain the difference scores, reflecting the muscle activity in response to the touch relative to the baseline period preceding touch. We used a linear mixed model with difference score as the dependent variable, a fixed effect for cued word, and a random intercept term for participant. To test for significance, we used the Kenward-Roger approximation based on a modified F test <ref type="bibr" target="#b32">(Singmann &amp; Kellen, 2019)</ref>.</p><p>For the machine-learning analysis, we used a randomforest classifier <ref type="bibr" target="#b14">(Ho, 1995)</ref> with 1,000 estimators (decision trees) using the Scikit-learn package (Version 0.24.2; <ref type="bibr" target="#b27">Pedregosa et al., 2011)</ref> with Python 2.7. The estimators are constructed in randomly selected subspaces defined by the input features (from the EMG recordings), and the classifier automatically finds the most important features for predicting the cue. As input to the classifier, six features were selected from the cleaned data set for each muscle, resulting in 12 features total for either sender or receiver. These six features were chosen as described by <ref type="bibr" target="#b15">Jerritta et al. (2014)</ref> and <ref type="bibr" target="#b29">Picard et al. (2001)</ref>  ). To determine whether classification accuracy was better than random, we compared the number of correct classifications for each cued emotion with a random classifier in a contingency table and then used a χ 2 test of independence of variables to determine statistical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video recording</head><p>In Experiment 1, the touch-communication task was recorded on video so we could capture the features of touch, such as duration, speed, target area, intensity, and type. Care was taken to record a small area focused on the arm of the receiver to avoid recording the participants' faces. On the rare occasion when a participant's face entered the frame, this was edited to obscure the face, and the original recording was destroyed. Videos were coded by three research assistants who were unaware of the cue presented to the sender. The timing of each touch event was recorded, and they were coded for target location, intensity, and type.</p><p>The coding scheme was adapted from that reported by <ref type="bibr" target="#b12">Hertenstein, Keltner, et al. (2006)</ref>. Locations were categorized as the hand, wrist, lower forearm (distal), and upper forearm (proximal). Intensity was coded as light (skin indentation or arm movement minimal; e.g., slow stroking, holding), moderate (moderate skin indentation or arm movement; e.g., fast stroking, squeezing), or strong (considerable skin indentation or arm movement; e.g., shaking). Touch type was selected from a long list of descriptors: holding, pressing, lifting, interlocking, hugging, tossing, pulling and pushing, stroking, squeezing, swinging and massaging, pinching, rubbing, tapping, shaking, patting, poking, hitting, picking, scratching, slapping, tickling, and trembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis for video recordings</head><p>Video data were annotated with ELAN (Version 5.1; Max Planck Institute for Psycholinguistics, 2017). For two sessions (one half of the experiment for each of two different pairs), recordings were not made because of equipment failure. Annotation text was automatically matched in R against the keywords included in the coding scheme for touch type, location, and intensity. Trials with missing data were checked for typos and other labeling errors, which were manually corrected. Of the recorded sessions, 53 trials (2.5%) were excluded because of missing data, obscured touches, and rare touch events that did not fit the coding scheme. The touched locations were simplified for presentation in Figure <ref type="figure">1</ref>. The wrist was included in the "distal arm" category, and the "whole arm" category included touches that covered at least two different location categories.</p><p>To determine the most effective touch strategies, we evaluated the type, location, and intensity of the touches with respect to performance on the touch-communication task, for every trial. The combination of type, location, and intensity gave the full description of each touch action. Some senders used multiple touch actions on a single trial, including in some cases swapping back and forth between different touch actions. For this reason, we did not want to simply count the number of times that a touch action appeared because these trials would mean that these repeated touch actions would be overrepresented. Furthermore, on some trials in which senders used multiple touch actions, one action appeared to be the dominant strategy, with more time spent on it than other actions. For this reason, we did not want to simply count whether a touch action appeared in a trial. We decided to capture how prominent a particular touch action was for a given trial by using the proportion of trial time spent on it. This also accounts for differences in trial duration.</p><p>To identify the best and worst touch actions for conveying each cued message, for each touch action, we calculated the difference between the proportion of time that action appeared in all correct trials and the proportion of time that it appeared in all incorrect trials for a given cue. This score was then used to rank the touch actions for each cue. We then selected the top four touch actions (or all actions that appeared more in correct than incorrect trials if there were fewer than four; i.e., with a positive score) and the bottom four touch actions (or all with a negative score if there were fewer than four).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand tracking</head><p>In Experiment 4, the physical attributes of standardized touch gestures from two expert senders were measured and characterized using a motion-tracking system <ref type="bibr" target="#b9">(Hauser et al., 2019)</ref>. Measurements were taken via custom software using a Leap Motion camera system (Leap Motion, San Francisco, CA) to track the sender's hands and a Flock of Birds six-degrees-of-freedom sensor system (Trakstar and Model 800 sensors; Ascension, Shelburne, VT) to track the receiver's forearm. Six contact metrics were considered: (a) velocity of the sender's hand normal to the receiver's arm, (b) velocity of the sender's hand tangential to the receiver's arm, (c) the total contact area between the sender's hand and the receiver's skin, (d) the percentage of the sender's palm area contacting the receiver's skin, (e) the number of the sender's fingers contacting the receiver, and (f) the proportion of time in which the sender's palm was in contact with the receiver. To compare across gestures, we normalized mean values between 0 and 1 for each contact metric on the basis of the population standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Touch communication within close relationships</head><p>In two experiments involving pairs in close relationships, we found that the participants assigned to the role of receiver were able to interpret the gestures on the basis of touch alone and select the correct message at well above the chance rate (Fig. <ref type="figure">1</ref>), despite neither participant receiving any feedback or training. Additionally, the correct response was the most popular one, for every cue (Figs. 1b, 1c, and 1e), although this was not true for all individuals (see Fig. <ref type="figure">S1</ref> in the Supplemental Material). This demonstrates that social touch messages are readily communicated within emotionally close pairs, consistent with previous reports <ref type="bibr" target="#b11">(Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b23">McIntyre et al., 2019)</ref>.</p><p>To evaluate touch-communication performance, we used the F1 metric, which reflects performance that minimizes both misses and false positives. The chance rate was set at .29, which is the maximum possible F1 score for a single cue when giving identical answers on all trials (for further details, see Data Analysis for Touch Communication). In the first experiment (Figs. <ref type="figure">1a-1d</ref>), overall performance was significantly better than chance (estimated marginal mean [EMM] F1 score = .59, 95% CI = [.49, .68], z = 18.00, p &lt; .001), as was performance for every individual cue (Fig. <ref type="figure">1d</ref>; all ps &lt; .001, Bonferroni-Holm correction). Performance varied significantly by cue, χ 2 (5) = 432.02, p &lt; .001 (mixed-effects model), was significantly better after swapping roles, χ 2 (1) = 39.64, p &lt; .001, and varied with the cued word, χ 2 (5) = 24.20, p &lt; .001. In this experiment only, the pairs swapped roles halfway through the testing session so that the sender became the receiver and vice versa. Swapping roles significantly improved identification of happiness, calming, and gratitude (attention: odds ratio [OR] = 0.81, 95% CI = [0.58, 1.12], z ratio = -1.32, p = .190; love: OR = 0.89, 95% CI = [0.68, 1.17], z = -0.84, p = .403; happiness: OR = 0.60, 95% CI = [0.45, 0.80], z ratio = -3.4, p &lt; .001; calming: OR = 0.47, 95% CI = [0.36, 0.62], z ratio = -5.44, p &lt; .001; sadness: OR = 1.01, 95% CI = [0.68, 1.17], z ratio = -0.836, p = .403; gratitude: OR = 0.52, 95% CI = [0.39, 0.68], z ratio = -4.7, p &lt; .001; post hoc pairwise comparisons with Bonferroni-Holm correction).</p><p>In a second experiment (Figs. <ref type="figure">1e</ref> and <ref type="figure">1f</ref>), the communication task was performed with the receiver in an MRI scanner (data are presented in a separate article), and the pairs did not swap roles. Similar to Experiment 1, overall performance in Experiment 2 was significantly better than chance (EMM F1 score = .58, 95% CI = [.47, .68], z = 17.8, p &lt; .001), as was performance for every individual cue (Fig. <ref type="figure">1f</ref>; all ps &lt; .001, Bonferroni-Holm correction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial expressions accompanying touch communication</head><p>To measure facial expressions during touch communication within close relationships in Experiment 1, we recorded facial muscle activity in the zygomatic (smiling) and the corrugator (frowning) muscles, in both the sender and the receiver, using facial EMG (Fig. <ref type="figure" target="#fig_3">2</ref>). For the outcome measure, we used the participant-level z score of the activity measured on each muscle and took the difference in z score measured during the first 4 s of touch relative to a baseline period preceding the touch (see the Method section and Fig. <ref type="figure" target="#fig_3">S2</ref>).</p><p>Although the senders and receivers could not see each other's faces (Fig. <ref type="figure">1a</ref>), we found that both the senders' zygomatic and corrugator activity and the receivers' zygomatic activity varied according to the cued message that was being communicated through touch.</p><p>Mixed-effects models revealed that the senders' zygomatic activity during touch was significantly greater than baseline (EMM = .19, 95% CI = [.10, .28]), t(28) = 4.13, p &lt; .001, and varied with the cue, F(5, 906) = 49.69, p &lt; .001 (model estimates and CIs are shown in boxplots in Fig. <ref type="figure" target="#fig_3">2a</ref>). The receivers' zygomatic activity during touch was not overall significantly greater than baseline (EMM = .03, 95% CI = [-.03, .10]), t(28) = 1.02, p = .316, but did vary significantly with the cue, F(5, 920) = 7.04, p &lt; .001 (Fig. <ref type="figure" target="#fig_3">2a</ref>).</p><p>The senders' overall corrugator activity was not significantly different during touch compared with baseline (EMM = .06, 95% CI = [-.04, .157]), t(28) = 1.14, p = .264, but did vary significantly with cue, F(5, 891) = 8.58, p &lt; .001 (Fig. <ref type="figure" target="#fig_3">2b</ref>). The receivers' overall corrugator activity was not significantly different during touch compared with baseline (EMM = -.04, 95% CI = [-.10, .03]), t(28) = -1.15, p = .262, nor did it vary with cue, F(5, 901) = 1.50, p = .188 (Fig. <ref type="figure" target="#fig_3">2b</ref>).</p><p>We predicted that if the variations in the senders' facial expressions were related to their capacity to convey social messages, the facial muscle activity of the sender should be related to the successful communication of messages. Similarly, if the variations in the receivers' facial expressions reflected their ability to decode the touch messages, their facial muscle activity would also be related to performance on the communication task. To test this, we applied machine-learning methods to train random-forest classifiers on the facial muscle activity of the senders and the receivers separately, to predict which message was cued (see the Method section). When the classifier was trained on either the senders' or the receivers' facial muscle activity, it could predict the cued message significantly better than chance (chance hit rate = 16.7%; sender: M = 24.8%, SD = 5.9% across the tenfold cross-validation procedure; p &lt; .001, n = 914 trials with data available from both muscles; Fig. <ref type="figure" target="#fig_3">2c</ref>; receiver: M = 21.0%, SD = 4.4%; p &lt; .05, n = 931 trials; Fig. <ref type="figure" target="#fig_3">2d</ref>). Contrary to our prediction, results showed that neither classifier's performance was significantly correlated with performance on the touch-communication task-sender: r = -.05, 95% CI = [-.41, .33], t(27) = -0.24, p = .8153; receiver: r = -.19, 95% CI = [-.52, .19], t(27) = -1.02, p = .3166 (including only those individuals with classifier data for at least 10 trials; Fig. <ref type="figure" target="#fig_3">2e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sender</head><formula xml:id="formula_0">-2 -1 -0.5 0 0.5 1 2 -2 -1 -0.5 0 0.5 1 2 -2 -1 -0.5 0 0.5 1 2 -2 -1 -0.5 0 0.</formula><p>5 1 2 Muscle Activity -Baseline (z) Zygomaticus Receiver Sender Corrugator Receiver 34 19 28 31 21 21 13 45 19 6 37 31 29 15 30 32 19 20 20 11 18 56 22 13 21 49 23 19 28 19 20 36 14 14 35 30 Cued Word Classified % 10 20 30 40 50 Sender Classifier 40 26 14 17 26 26 29 29 19 15 25 36 21 26 23 19 27 33 20 19 12 35 31 36 29 22 20 18 23 39 19 21 23 17 34 42 Cued Word % Receiver Classifier 20 40 60 80 100 0 2 0 4 0 6 0 Classifier Hit Rate (%) Receiver Hit Rate (%) a b c d e Att Lov Hap Cal Sad Gra Att Att Lov Lov Hap Hap Cal Cal Sad Sad Gra Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra Att Lov Hap Cal Sad Gra 10 20 30 40 50 Gratitude, Oth = Other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Development of standardized touch gestures</head><p>To characterize the intuitive touch gestures, we videorecorded the touch-communication task in the same experiment in which we recorded facial EMG activity (Experiment 1). The touching activity was coded by research assistants for location, intensity, and type from a list of descriptors adapted from the work by <ref type="bibr" target="#b12">Hertenstein, Keltner, et al. (2006)</ref>. We found that successful strategies for touch communication in a close relationship shared common features across senders (Fig. <ref type="figure" target="#fig_5">3</ref>). For attention, the best features were moderate tapping and shaking. For calming, it was light holding at multiple locations on the arm as well as moderately stroking the whole arm or lightly stroking part of the arm. Gratitude was associated with light stroking or holding, or squeezing the arm. Happiness was best conveyed by light tapping across the whole arm. Love was successfully signaled by light stroking across the whole arm. Successful communication of sadness was most strongly associated with lightly holding or stroking one part of the arm. Touching the hand and using high intensity were generally unsuccessful strategies.</p><p>To validate whether our characterizations of intuitive touch meaningfully reflect the features of touches that are important for communicating emotional cues, we developed standardized touch gestures (see Movies S1-S6). These were made up of the common features we observed in successful touch communication, and we avoided features found in unsuccessful communication (Fig. <ref type="figure" target="#fig_5">3</ref>). When the touch expressions were delivered by three trained experimenters to strangers who were unacquainted with the experimenters (Fig. <ref type="figure" target="#fig_6">4</ref>), overall identification of the touch messages was significantly better than chance (Experiment 3: EMM F1 score = .73, 95% CI = [.62, .81], z = 7.57, p &lt; .001; Experiment 4: EMM F1 score = .65, 95% CI = [.59, .71], z = 19.20, p &lt; .001), as was performance for every individual cue (Experiment 3: all ps &lt; .001, Bonferroni-Holm correction; Experiment 4: all ps ≤ .02, Bonferroni-Holm correction). As in the intuitive-touch task, the correct response was generally the most popular one (except Experiment 4 gratitude, which was more often labeled calming), and individuals showed more variation (see Fig. <ref type="figure" target="#fig_5">S3</ref> in the Supplemental Material).</p><p>Hand tracking with a 3D motion-tracking system <ref type="bibr" target="#b9">(Hauser et al., 2019)</ref> was used to characterize the different gestures in more detail, breaking them down into physical primitives (Fig. <ref type="figure" target="#fig_7">5a</ref>; see Figs. S4 and S5 in the Supplemental Material). Both attention and happiness were characterized by high normal velocity, a small contact area, and a brief duration, but happiness had a high tangential velocity, whereas attention involved almost no lateral movement. Love and calming were both characterized by slow tangential movement, but compared with calming, love had a longer contact duration, a smaller overall contact area, and a higher ratio of finger to palm contact. Sadness was static with a large contact area and long contact duration. Gratitude involved slow normal and tangential movements with moderate contact area and duration.</p><p>We directly compared the standardized touches performed by experts with the intuitive touches performed by someone in a close relationship in a separate analysis (Fig. <ref type="figure" target="#fig_7">5b</ref>). We found that identification of the standardized gestures was similar to or even slightly better than the intuitive touches, χ 2 (1, N = 1,000 bootstrapped samples) = 4.52, p = .027. Specifically, standardized touches produced significantly better recognition than intuitive touches for love, happiness, and calming (attention: OR = 0.80, 95% CI = [0.52, 1.22], z ratio = -1.04, p = .301; calming: OR = 1.73, 95% CI = [1.1, 2.6], z ratio = 2.58, p = .010; gratitude: OR = 0.76, 95% CI = [0.50, 1.15], z ratio = -1.288, p = .198; happiness: OR = 2.32, 95% CI = [1.53, 3.53], z ratio = 3.94, p &lt; .001; love: OR = 4.44, 95% CI = [2.92, 6.76], z ratio = 6.97, p &lt; .0001; sadness: OR = 1.35, 95% CI = [0.89, 2.04], z ratio = 1.40, p = .161; post hoc pairwise comparisons with Bonferroni-Holm correction). Furthermore, without feedback, the receivers were able to quickly learn the meaning of the gestures and reached a stable performance level after only 10 to 12 presentations (Fig. <ref type="figure" target="#fig_7">5c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We identified touch expressions for which the social and emotional content are broadly understood between strangers, at least within one cultural context. Indeed, skillful strangers can communicate emotional touch messages as effectively or better than people in close relationships. We have ruled out a large role for an idiosyncratic touch language developed within couples or other close relationships in touch-communication efficacy. Communication performance without feedback was well above chance, and our standardized touch expressions reflected their basis in real interpersonal touch interactions and a common understanding. However, our study recruited adults living in Sweden, and an important open question is whether the standardized touch expressions are universally understood across cultural contexts.</p><p>That people can understand social and emotional touch messages confirms earlier reports <ref type="bibr" target="#b0">(App et al., 2011;</ref><ref type="bibr" target="#b11">Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b17">Kirsch et al., 2018)</ref>. That strangers are as good as or better than people in a close relationship at communicating via touch provides an apparent contrast with earlier work showing that romantic couples communicate more effectively than stranger pairs <ref type="bibr">(Thompson &amp;</ref> Hampton, Attention Calming Gratitude Happiness L ove Sadness Correct Incorrect 1 3 10 30 1 3 10 30 1 3 10 30 1 3 10 30 1 3 10 30 1 3 10 30</p><p>P o k i n g S h a k i n g S q u e e z i n g T a p p i n g H o l d i n g R u b b i n g S t r o k i n g H o l d i n g S h a k i n g S q u e e z i n g S t r o k i n g H o l d i n g S q u e e z i n g S t r o k i n g T a p p i n g H o l d i n g P i n c h i n g S q u e e z i n g S t r o k i n g H o l d i n g S q u e e z i n g S t r o k i n g Time Difference (%) 5 10 15 20 Incorrect Correct Intensity Light Moderate Strong Touch Duration (s) time for each combination of features was calculated as a percentage of the total touching time for each cue (see the Method section). The top row of graphs shows the best combinations of touch features, up to four for each cue, that were observed most in trials with correct responses (time in correct trials minus time in incorrect trials).</p><p>The bottom row shows the worst combinations that were observed most in trials with incorrect responses (time in incorrect trials minus time in correct trials). The size of each circle reflects the difference in touch duration between correct and incorrect trials. Intensity is visualized by color. The y-axis shows the touch locations (the top line in which the whole arm is shaded indicates touches that were applied to at least two individual locations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2011</head><p>). We reconcile this by considering that the earlier work used untrained strangers, whereas we trained our experimenters to use effective touch strategies with clear and distinct touch gestures. This training may compensate for a greater reluctance to touch a stranger compared with someone close <ref type="bibr" target="#b35">(Suvilehto et al., 2015</ref><ref type="bibr" target="#b36">(Suvilehto et al., , 2019))</ref>. We went beyond the efforts used in previous studies <ref type="bibr" target="#b8">(Gaus et al., 2015;</ref><ref type="bibr" target="#b11">Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b16">Jung et al., 2015;</ref><ref type="bibr" target="#b20">Masson &amp; Op de Beeck, 2018;</ref><ref type="bibr" target="#b33">Supratman et al., 2020)</ref> to characterize the physical features of touch communication. We did this by first evaluating their contribution to communication efficacy and then validating this by showing that standardized touch strategies based on the features identified as effective were indeed capable of conveying the intended message. Additionally, we used a 3D handtracking approach to obtain quantitative descriptors of the standardized touch expressions, including normal and tangential velocity, contact area and duration, and palm and finger contact.</p><p>A previous study evaluated the relationship between motion energy obtained from filmed interpersonal touch interactions and observers' judgments of their valence and arousal <ref type="bibr" target="#b20">(Masson &amp; Op de Beeck, 2018)</ref>. More positively viewed interactions tend to have lower motion energy, whereas more (emotionally) arousing interactions have higher motion energy. Our results are consistent with this finding. The highest measured velocities were observed in our attention and happiness expressions, which were likely to have relatively high arousal. Interestingly, these are the same two cues that in the receiver provoked a significant increase in zygomatic activity, which has been associated with high-arousal positive stimuli <ref type="bibr" target="#b7">(Fujimura et al., 2010)</ref>. Additionally, our sadness expression, being the only negative-valence message, had near-zero velocity.</p><p>Cued Word Experiment 4 Experiment 4 Experiment 3 Experiment 3 Oth Gra Sad Cal Hap Lov Att Receiver Response Gra Sad Cal Hap Lov Att Receiver Response Att Lov Hap Cal Sad Gra Cued Word Att Lov Hap Cal Sad Gra 0.00 0.25 0.50 0.75 1.00 Att Lov Hap Cal Sad Gra Performance F1 0.00 0.25 0.50 0.75 1.00 Att Lov Hap Cal Sad Gra Performance F1 Although measuring motion energy is quantitative and informative, we have shown that more detailed physical measurements can further discriminate specific touch messages; for example, happiness and attention had similar normal velocities but very different tangential velocities, whereas contact duration and area discriminated love and calming, which had similar tangential velocities. This is consistent with studies of human-delivered touch to pressure-sensor surfaces <ref type="bibr" target="#b8">(Gaus et al., 2015;</ref><ref type="bibr" target="#b33">Supratman et al., 2020)</ref> and robot-delivered touch to humans <ref type="bibr" target="#b37">(Teyssier et al., 2020)</ref> showing that a variety of contact area and motiondynamics measures is required to distinguish different touch actions.</p><p>In our touch-communication task, the sender and receiver could not see each other's expressions, but we asked whether either role's facial expressions might interact with touch communication. The senders' zygomatic and corrugator activity as well as the receivers' zygomatic activity varied according to the touch message. However, when we looked for a relationship between facial muscle activity and communication success on an individual level, we did not find it. This suggests that the varying facial muscle activity may simply reflect the participants' adoption of an existing set of strategies for communicating emotion that would normally involve both touch and facial expression. An alternative explanation for these findings is that some aspects of facial muscle activity that we did not measure are relevant to touch communication or that there is a weak relationship (small effect size) that we failed to detect.</p><p>The senders showed increased zygomatic activity when communicating attention, happiness, and gratitude, suggesting that expressing these messages may be accompanied by smiling. The sender also showed increased corrugator activity when communicating happiness, possibly because of high arousal produced by its high intensity (Fig. <ref type="figure" target="#fig_5">3</ref>) and speed (Fig. <ref type="figure" target="#fig_7">5</ref>). The receivers also showed increased zygomatic activity when feeling the touch expressions for attention and happiness, but not gratitude. The relatively high intensity and speed of the attention and happiness may have provoked a positive emotional response from the receiver <ref type="bibr" target="#b7">(Fujimura et al., 2010)</ref>. The result contrasts with previously observed increased zygomatic activity in response to a gentle stroking with a brush that most resembled our calming expression <ref type="bibr" target="#b25">(Pawling et al., 2017)</ref>. The touch evoked no significant changes in the corrugator activity of the receivers, in contrast to previous studies showing reduced corrugator activity <ref type="bibr" target="#b22">(Mayo et al., 2018;</ref><ref type="bibr" target="#b31">Ree et al., 2020)</ref> in response to pleasant touch.</p><p>Our standardized touches closely resembled authentic interpersonal touch but were optimized for communicating specific messages. This is because they were developed on the basis of the successful communication strategies that people in close relationships used when given minimal instructions. Although we cued a specific message to communicate and limited the area of the body that could be touched, the participants were otherwise free to choose their own touch strategy. Some previous studies on touch communication systematically investigated different types of messages. For example, <ref type="bibr" target="#b0">App et al. (2011)</ref> investigated which nonverbal communication channels are used for messages that correspond to different social functions (status conveying, survival focused, and intimacy focused). <ref type="bibr" target="#b17">Kirsch et al. (2018)</ref> investigated how the same touches were interpreted differently if the receivers were given different tasks-interpreting either the emotion or the intention of the person performing the touches. In our study, we did not try to systematically vary details of the messages to be communicated because we did not have any specific hypotheses relating to them. Rather, our goal was simply to capture a variety of touch behaviors. Similarly, we did not control the exact type of relationship or demographics of our participants <ref type="bibr" target="#b11">(Hertenstein et al., 2009;</ref><ref type="bibr" target="#b12">Hertenstein, Keltner, et al., 2006;</ref><ref type="bibr" target="#b38">Thompson &amp; Hampton, 2011)</ref>, the investigation of which would have required different sample-size considerations.</p><p>The interpretation of touch messages is likely to be influenced by a large number of contextual factors <ref type="bibr" target="#b2">(Cekaite &amp; Bergnehr, 2018;</ref><ref type="bibr" target="#b42">Weiss, 1986)</ref>. We found that when pairs swapped roles, performance improved (Figs. <ref type="figure">1b-1d</ref>). Experiencing multiple roles may have aided communication in a way that is similar to more interactive, natural contexts. One example of a realworld situation that has similar contextual information to our communication task is text communication. In principle, text alone can convey all information, analogous to speech. However, the widespread use of emojis to convey facial expressions illustrates that there is a clear desire to use analogs of nonverbal communication. Emojis rely on a preexisting broad consensus about the mapping between the visual elements of a facial expression and an emotional meaning. Similarly, our standardized touches did not require specific training or any feedback but were able to capitalize on a preexisting broad understanding, while also benefiting from additional contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>When people in a close relationship engaged in emotional touch communication, the senders intuitively produced distinct gestures to effectively communicate different messages of attention, love, happiness, calming, sadness, and gratitude. The touch messages were associated with the senders' facial expressions, but we found no evidence that this was related to the ability of the receivers to correctly interpret the touch message. In contrast, choosing the right touch strategies with appropriate physical features was sufficient for effective communication. We demonstrated this with our set of tactile standardized gestures delivered to strangers unacquainted with the senders: These gestures were identified even more successfully than the intuitive gestures delivered within the context of a close relationship. We have thus presented a lexicon of gestures, with distinct physical characteristics, that can be used for social communication to enrich emotional content. These findings inform the development and improvement of interpersonal communication mediated by haptic interfaces. It could also inform development of training for people in any context in which it is desirable that communication is enhanced and emotional touch communication is appropriate. It may also provide a useful tool for exploring social differences associated with psychiatric disorders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head><p>Action Editor: Steven W. Gangestad Editor: Patricia J. Bauer</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Fig.1. Touch communication between people in a close relationship (intuitive touch). The experimental setup is illustrated in (a). The sender was presented with a cue and then touched the receiver to communicate the message. The receiver used a separate interface to record the message they thought their partner was sending. Participants could not see each other's monitors or faces. Group-level confusion matrixes for Experiment 1 are shown separately for trials (b) in which participants within pairs were in their initially assigned roles (sender and receiver) and (c) after the participants swapped roles. Each square represents a unique cue/response combination. The number in the square indicates the total number of times that unique combination occurred, pooled across participants. The shading indicates the percentage of occurrences that response was made out of the total number of times that cue was presented (columns sum to 100% shading). A dark diagonal indicates good performance on the communication task, and shading off the diagonal indicates consistent mistakes. F1 performance score (minimum = 0, maximum = 1; given by the harmonic mean of recall and precision) is shown (d) for each cue and role in Experiment 1. Circles show scores for each pair. Horizontal lines show model estimates (error bars indicate 95% confidence intervals). Chance-level performance (F1 = .29) is indicated by the dashed line. For Experiment 2, in which participants did not swap roles, the group-level confusion matrix (e) follows the same conventions used in (b) and (c), and the graph of F1 performance scores (f) follows the same conventions used in (d). Att = Attention, Lov = Love, Hap = Happiness, Cal = Calming, Sad = Sadness, Gra = Gratitude, Oth = Other, Tim = Time-out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>and consist of the (a) mean of the raw signal, (b) standard deviation of the raw signal, (c) mean of the absolute values of the first differences of the raw signals, (d) mean of the absolute values of the first differences of the normalized signals, (e) mean of the absolute values of the second differences of the raw signals, and (f) mean of the absolute values of the second differences of the normalized signals. Tenfold cross-validation was performed to determine classifier accuracy and uncertainty and to prevent overfitting. Statistical analysis of classifier accuracy was performed (SciPy statistics package, Version 1.7.1; Virtanen et. al., 2020; Python 2.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .Attention</head><label>2</label><figDesc>Fig. 2. The role of facial expressions in touch communication in Experiment 1. Mean facial electromyography activity of the (a) zygomaticus muscle and (b) corrugator muscle during the first 4 s of touch during the touch-communication task is shown for each cued word, separately for senders and receivers. Values on the y-axis are participant-level z scores during touch relative to baseline (0). The y-axis is on a pseudo log scale to emphasize differences between the different cues rather than individual data points. Circles show the average for each participant-cue combination. Box plots give the estimated marginal mean (central horizontal line) and 95% confidence interval (top and bottom edge of box) for each participant role and cue. Data in (a) are from a total of 939 (sender) and 954 (receiver) trials after artifact rejection, and data in (b) are from a total of 924 (sender) and 935 (receiver) trials after artifact rejection (see the Method section). The matrixes show performance of a random-forest classifier on the facial muscle activity (from both muscles) of (c) the sender to predict the cue and (d) the receiver to predict the cue. Each square represents a unique cue/response combination. The number in the square indicates the total number of times that unique combination occurred, pooled across participants. The shading indicates the percentage of occurrences that response was made out of the total number of times that cue was presented (columns sum to 100% shading). A dark diagonal indicates good performance on the communication task, and shading off the diagonal indicates consistent mistakes. The scatterplot (e) shows the relation between the receivers' accuracy in interpreting the messages as a function of the classifier's accuracy in interpreting the messages, separately for senders (purple) and receivers (orange). Solid lines indicate best-fitting regressions. Att = Attention, Lov = Love, Hap = Happiness, Cal = Calming, Sad = Sadness, Gra =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Location, intensity, and touch descriptors that best discriminated correct and incorrect interpretation of the messages in Experiment 1. Density plots of the touch durations for each cue are shown along the top of the figure, separately for correct and incorrect responses. Because the overall touch duration for different cues varied,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Touch communication between a trained expert sender and a receiver unacquainted with the sender (standardized touch). Group-level confusion matrixes show results for the communication task involving (a) Expert Sender 1 (Experiment 3) and (c) both Expert Senders 2 and 3 (Experiment 4). Each square represents a unique cue/response combination. The number in the square indicates the total number of times that unique combination occurred, pooled across participants. The shading indicates the percentage of occurrences that response was made out of the total number of times that cue was presented (columns sum to 100% shading). A dark diagonal indicates good performance on the communication task, and shading off the diagonal indicates consistent mistakes. F1 performance score (minimum = 0, maximum = 1) is shown for each cue in (b) for Experiment 3 and (d) Experiment 4. Circles show scores for each pair. Horizontal lines show model estimates (error bars indicate 95% confidence intervals). Chance-level performance (F1 = .29) is indicated by the dashed line. Att = Attention, Lov = Love, Hap = Happiness, Cal = Calming, Sad = Sadness, Gra = Gratitude, Oth = Other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Characterization and evaluation of the standardized touches. Physical features of each standardized touch delivered by trained experimenters and measured using an automated hand-tracking system (Experiment 4, Experts 2 and 3) are shown in (a). Mean values for each contact metric (green lines) were normalized between 0 and 1 on the basis of the population standard deviation (shaded areas). Estimates of F1 performance score from a mixed-effects model comparing intuitive and standardized touch strategies are shown (b) for each cue type. Error bars show 95% confidence intervals. Chance-level performance (F1 = .29) is indicated by the dashed line. Estimates of F1 performance score are shown (c) for each number of presentation and standardized cue. The lines were fitted with a loess function with a span of 1; the shaded area is the 95% confidence interval based on the standard error (combined data from Experiments 3 and 4). Att = Attention, Lov = Love, Hap = Happiness, Cal = Calming, Sad = Sadness, Gra = Gratitude, Oth = Other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of Experiments and Participants</figDesc><table><row><cell>Variable</cell><cell>Experiment 1</cell><cell>Experiment 2</cell><cell>Experiment 3</cell><cell>Experiment 4</cell></row><row><cell>Touch type</cell><cell>Intuitive</cell><cell>Intuitive</cell><cell>Standardized</cell><cell>Standardized</cell></row><row><cell></cell><cell>(both roles)</cell><cell>(one role)</cell><cell>(Expert 1)</cell><cell>(Experts 2 &amp; 3)</cell></row><row><cell>Measurements</cell><cell>Communication task, facial</cell><cell>Communication task,</cell><cell>Communication</cell><cell>Communication task,</cell></row><row><cell></cell><cell>EMG, video recording</cell><cell>functional MRI a</cell><cell>task</cell><cell>hand tracking</cell></row><row><cell>N participants</cell><cell>32 (16 pairs)</cell><cell>20 (plus partners)</cell><cell>20</cell><cell>16</cell></row><row><cell>N trials</cell><cell>960 (6 cues × 5 repeats ×</cell><cell>2,400 (6 cues × 20</cell><cell>1,750 b (6 cues ×</cell><cell>1,916 b (6 cues ×</cell></row><row><cell></cell><cell>32 participants)</cell><cell>repeats × 20 receivers)</cell><cell>15 repeats ×</cell><cell>20 repeats ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell>20 participants)</cell><cell>16 participants)</cell></row><row><cell>Age range in</cell><cell>19-46 (Mdn = 24)</cell><cell>19-32 (Mdn = 22);</cell><cell>20-40 (Mdn = 25)</cell><cell>20-35 (Mdn = 24)</cell></row><row><cell>years</cell><cell></cell><cell>partners: 19-40</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(Mdn = 22)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Cues for the Intuitive-Touch-Communication Task</figDesc><table><row><cell>Cue word</cell><cell>Sender cue</cell><cell>Receiver choice</cell></row><row><cell>Attention</cell><cell>You just heard about something that your partner</cell><cell></cell></row><row><cell></cell><cell>might find interesting. Try to get their ATTENTION</cell><cell></cell></row><row><cell></cell><cell>through touch.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Note:The "other" option was presented only to the receiver, and only in Experiment 1. In Experiment 2, the receiver had 7 s to respond; otherwise, the response was labeled "time-out." Participants were informed that "your partner" referred to their partner in the task.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Leah Mayo</rs> and <rs type="person">Anna Asratian</rs> for advising us on facial electromyography methods, <rs type="person">Juulia Suvilehto</rs> for suggesting the use of the F1 score, an anonymous reviewer for suggesting improvements to the analysis of the facial electromyography data, and <rs type="person">India Morrison</rs> and <rs type="person">Marcus Heilig</rs> for reading and giving feedback on an early version of the manuscript. The experiments were initiated and designed by researchers at <rs type="institution">Linköping University in Sweden</rs> as well as the <rs type="institution">University of Virginia and Columbia University</rs> in the United States. All experiments were carried out at <rs type="institution">Linköping University, Sweden</rs>.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This study was funded by <rs type="funder">Facebook</rs> and the <rs type="funder">Swedish Research Council</rs>.</p></div>
<div><head>Open Practices</head><p>All data have been made publicly available via figshare and can be accessed at <ref type="url" target="https://doi.org/10.6084/m9.figshare.c.5630842.v2">https://doi.org/10.6084/m9.fig  share.c.5630842.v2</ref>. All analysis scripts and materials have been made publicly available via Zenodo and can be accessed at <ref type="url" target="https://doi.org/10.5281/zenodo.6855921">https://doi.org/10.5281/zenodo.6855921</ref>. The design and analysis plans for the study were not preregistered. This article has received the badges for Open Data and Open Materials. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psychologi  calscience.org/publications/badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>Facebook influenced the broad topic of the research and encouraged publication without influencing the interpretation of the results. The coauthors employed by Facebook contributed scientifically and were not under any specific directive about the reporting or interpretation of the results. The authors declared that there were no other potential conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/09567976211059801">http://  journals.sagepub.com/doi/suppl/10.1177/095679762</ref> 11059801</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonverbal channel use in communication of emotion: How may depend on why</title>
		<author>
			<persName><forename type="first">B</forename><surname>App</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hertenstein</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0023164</idno>
		<ptr target="https://www.biopac.com/" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="603" to="617" />
			<date type="published" when="2011">2011. 2015</date>
		</imprint>
	</monogr>
	<note>AcqKnowledge (Version 5.0.5 Computer software</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Boehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Novembre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Olausson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Linköping University</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished raw functional MRI data during touch communication</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Affectionate touch and care: Embodied intimacy, compassion and control in early childhood education</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cekaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bergnehr</surname></persName>
		</author>
		<idno type="DOI">10.1080/1350293X.2018.1533710</idno>
		<ptr target="https://doi.org/10.1080/1350293X.2018.1533710" />
	</analytic>
	<monogr>
		<title level="j">European Early Childhood Education Research Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="940" to="955" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The comforting touch: Tactile intimacy and talk in managing children&apos;s distress</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cekaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holm</surname></persName>
		</author>
		<idno type="DOI">10.1080/08351813.2017.1301293</idno>
		<ptr target="https://doi.org/10.1080/08351813.2017.1301293" />
	</analytic>
	<monogr>
		<title level="j">Research on Language and Social Interaction</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="127" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coevolution of neocortical size, group size and language in humans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X00032325</idno>
		<ptr target="https://doi.org/10.1017/S0140525X00032325" />
	</analytic>
	<monogr>
		<title level="j">Behavioral &amp; Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="681" to="694" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The social role of touch in humans and primates: Behavioural function and neurobiological mechanisms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2008.07.001</idno>
		<ptr target="https://doi.org/10.1016/j.neubiorev.2008.07.001" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="268" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guidelines for human electromyographic research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.1986.tb00676.x</idno>
		<ptr target="https://doi.org/10.1111/j.1469-8986.1986.tb00676.x" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="567" to="589" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial expression arousal level modulates facial mimicry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijpsycho.2010.02.008</idno>
		<ptr target="https://doi.org/10.1016/j.ijpsycho.2010.02.008" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social touch gesture recognition using random forest and boosting on distinct feature sets</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F A</forename><surname>Gaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Olugbade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<idno type="DOI">10.1145/2818346.2830599</idno>
		<ptr target="https://doi.org/10.1145/2818346.2830599" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction-ICMI &apos;15</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Chairs</surname></persName>
		</editor>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction-ICMI &apos;15</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uncovering human-to-human physical interactions that underlie emotional and affective touch communication</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Israr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Olausson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gerling</surname></persName>
		</author>
		<idno type="DOI">10.1109/WHC.2019.8816169</idno>
		<ptr target="https://doi.org/10.1109/WHC.2019.8816169" />
	</analytic>
	<monogr>
		<title level="m">IEEE World Haptics Conference (WHC)</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Otaduy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gerling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-H</forename><surname>Ryu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>O'malley</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Touch: Its communicative functions in infancy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hertenstein</surname></persName>
		</author>
		<idno type="DOI">10.1159/000048154</idno>
		<ptr target="https://doi.org/10.1159/000048154" />
	</analytic>
	<monogr>
		<title level="j">Human Development</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="70" to="94" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The communication of emotion via touch</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hertenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccullough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0016108</idno>
		<ptr target="https://doi.org/10.1037/a0016108" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="566" to="573" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Touch communicates distinct emotions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hertenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>App</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Bulleit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jaskolka</surname></persName>
		</author>
		<idno type="DOI">10.1037/1528-3542.6.3.528</idno>
		<ptr target="https://doi.org/10.1037/1528-3542.6.3.528" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="528" to="533" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The communicative functions of touch in humans, nonhuman primates, and rats: A review and synthesis of the empirical research</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hertenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Verkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Kerestes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.3200/MONO.132.1.5-94</idno>
		<ptr target="https://doi.org/10.3200/MONO.132.1.5-94" />
	</analytic>
	<monogr>
		<title level="j">Genetic, Social, and General Psychology Monographs</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="94" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.1995.598994</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.1995.598994" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Document Analysis and Recognition</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kavanaugh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Storms</surname></persName>
		</editor>
		<meeting>the 3rd International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion recognition from facial EMG signals using higher order statistics and principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jerritta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yaacob</surname></persName>
		</author>
		<idno type="DOI">10.1080/02533839.2013.799946</idno>
		<ptr target="https://doi.org/10.1080/02533839.2013.799946" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Chinese Institute of Engineers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="394" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Touch challenge &apos;15: Recognizing social touch gestures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Cang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Maclean</surname></persName>
		</author>
		<idno type="DOI">10.1145/2818346.2829993</idno>
		<ptr target="https://doi.org/10.1145/2818346.2829993" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multi modal Interaction-ICMI &apos;15</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</editor>
		<meeting>the 2015 ACM on International Conference on Multi modal Interaction-ICMI &apos;15</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="387" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reading the mind in the touch: Neurophysiological specificity in the communication of emotions by touch</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Krahé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Crucianelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fotopoulou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2017.05.024</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2017.05.024" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="136" to="149" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial EMG responses to emotional expressions are related to emotion perception ability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Künecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wilhelm</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0084053</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0084053" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">84053</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<idno type="DOI">10.1111/1469-8986.00078</idno>
		<ptr target="https://doi.org/10.1111/1469-8986.00078" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="776" to="785" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Socio-affective touch expression database</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Op De Beeck</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0190921</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0190921" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Article e0190921</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ELAN (Version 5.1) [Computer software</title>
		<ptr target="https://archive.mpi.nl/tla/elan" />
	</analytic>
	<monogr>
		<title level="m">Max Planck Institute for Psycholinguistics, The Language Archive</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Max Planck Institute for Psycholinguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Putting a good face on touch: Facial expression reflects the affective valence of caress-like touch across modalities</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Olausson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heilig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Morrison</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2018.07.001</idno>
		<ptr target="https://doi.org/10.1016/j.biopsycho.2018.07.001" />
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Affective touch communication in close adult relationships</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moungou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Isager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Israr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lumpkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abnousi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Olausson</surname></persName>
		</author>
		<idno type="DOI">10.1109/WHC.2019.8816093</idno>
		<ptr target="https://doi.org/10.1109/WHC.2019.8816093" />
	</analytic>
	<monogr>
		<title level="m">IEEE World Haptics Conference (WHC)</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Otaduy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gerling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-H</forename><surname>Ryu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>O'malley</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical investigation into the number of subjects required for an eventrelated fMRI study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garavan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2004.02.005</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2004.02.005" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="879" to="885" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">C-tactile afferent stimulating touch carries a positive affective value</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pawling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Mcglone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0173457</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0173457" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">173457</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">patchwork: The composer of ggplots (R Package Version 0.0.1)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Pedersen</surname></persName>
		</author>
		<ptr target="https://github.com/thomasp85/patchwork" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Computer software</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PsychoPy-Psychophysics software in Python</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Peirce</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2006.11.017</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2006.11.017" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward machine emotional intelligence: Analysis of affective physiological state</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.954607</idno>
		<ptr target="https://doi.org/10.1109/34.954607" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1175" to="1191" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Version 4.0.3 Computer software</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Right between the eyes: Corrugator muscle activity tracks the changing pleasantness of repeated slow stroking touch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bendas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Croy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sailer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physbeh.2020.112903</idno>
		<ptr target="https://doi.org/10.1016/j.physbeh.2020.112903" />
	</analytic>
	<monogr>
		<title level="j">Physiology &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">112903</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An introduction to mixed models for experimental psychology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Singmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kellen</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780429318405-2</idno>
		<ptr target="https://doi.org/10.4324/9780429318405-2" />
	</analytic>
	<monogr>
		<title level="s">New methods in cognitive psychology</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Spieler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Schumacher</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2019">2019</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recognizing human emotion based on applied force</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Supratman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashibara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<idno type="DOI">10.1109/HSI49210.2020.9142652</idno>
		<ptr target="https://doi.org/10.1109/HSI49210.2020.9142652" />
		<title level="m">13th International Conference on Human System Interaction (HSI)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Muramatsu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Melendez</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topography of social touching depends on emotional bonds between humans</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Suvilehto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Glerean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nummenmaa</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1519231112</idno>
		<ptr target="https://doi.org/10.1073/pnas.1519231112" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="13811" to="13816" />
			<date type="published" when="2015">2015</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crosscultural similarity in relationship-specific social touching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Suvilehto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nummenmaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kitada</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2019.0467</idno>
		<ptr target="https://doi.org/10.1098/rspb.2019.0467" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<date type="published" when="1901">2019. 1901</date>
		</imprint>
	</monogr>
	<note>Article 20190467</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conveying emotions through device-initiated touch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lecolinet</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.3008693</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2020.3008693" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The effect of relationship status on communicating emotions through touch</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hampton</surname></persName>
		</author>
		<idno type="DOI">10.1080/02699931.2010.492957</idno>
		<ptr target="https://doi.org/10.1080/02699931.2010.492957" />
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining facial expression and touch for perceiving emotional valence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Tsalamlal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Amorim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2016.2631469</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2016.2631469" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="449" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Affective handshake with a humanoid robot: How do participants perceive and combine its facial and haptic expressions?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Tsalamlal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tapus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Amorim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACII.2015.7344592</idno>
		<ptr target="https://doi.org/10.1109/ACII.2015.7344592" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="334" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SciPy 1.0: Fundamental algorithms for scientific computing in Python</title>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
		<ptr target="https://doi.org/10.1038/s41592-019-0686-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>SciPy 1.0 Contributors</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Psychophysiologic effects of caregiver touch on incidence of cardiac dysrhythmia</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart &amp; Lung: The Journal of Critical Care</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="505" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<ptr target="https://ggplot2.tidyverse.org" />
		<title level="m">ggplot2: Elegant graphics for data analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
