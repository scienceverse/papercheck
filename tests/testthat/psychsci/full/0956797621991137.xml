<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Long Does It Take for a Voice to Become Familiar? Speech Intelligibility and Voice Recognition Are Differentially Sensitive to Voice Training</title>
				<funder ref="#_KJfpuMs">
					<orgName type="full">Canadian Institutes of Health Research</orgName>
				</funder>
				<funder ref="#_Yac7s9e">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Emma</forename><surname>Holmes</surname></persName>
							<email>emma.holmes@ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">The Brain and Mind Institute</orgName>
								<orgName type="institution">The University of Western Ontario</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grace</forename><surname>To</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Brain and Mind Institute</orgName>
								<orgName type="institution">The University of Western Ontario</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ingrid</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Brain and Mind Institute</orgName>
								<orgName type="institution">The University of Western Ontario</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Communication Sciences and Disorders</orgName>
								<orgName type="institution">The University of Western Ontario</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Wellcome Centre for Human Neuroimaging</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Long Does It Take for a Voice to Become Familiar? Speech Intelligibility and Voice Recognition Are Differentially Sensitive to Voice Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">93B27CF3227D02B643A1949306F9A2B2</idno>
					<idno type="DOI">10.1177/0956797621991137</idno>
					<note type="submission">Received 1/31/20; Revision accepted 11/13/20</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-05-20T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>attention</term>
					<term>speech perception</term>
					<term>auditory perception</term>
					<term>memory</term>
					<term>learning</term>
					<term>open data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We encounter familiar people every day. Most commonly, these are friends, partners, and family members-but we also encounter people whom we know less well, such as work colleagues or television and radio presenters. As we get to know someone new, we develop the ability to recognize their identity from their voice. We are also better able to understand words spoken by familiar people than people we have never met (Domingo et al.,   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>participant's mother <ref type="bibr" target="#b1">(Barker &amp; Newman, 2004)</ref>. Even friends are substantially more intelligible than unfamiliar people <ref type="bibr" target="#b11">(Domingo et al., 2020;</ref><ref type="bibr" target="#b17">Holmes et al., 2018)</ref>. In fact, <ref type="bibr" target="#b11">Domingo et al. (2020)</ref> found no significant difference in the magnitude of the intelligibility benefit for the voices of friends known for at least 1.5 years and the voices of long-term spouses (&gt; 5 years). People who had known each other for less than 1.5 years were not included in the study, which raises the question of how much training on a voice is required to derive the maximum intelligibility benefit.</p><p>Several experiments have shown that when presented in conjunction with a masker, voices that participants have been trained on in the lab over 2 to 9 days have better intelligibility than unfamiliar voices <ref type="bibr" target="#b22">(Kreitewolf et al., 2017;</ref><ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994;</ref><ref type="bibr" target="#b41">Yonan &amp; Sommers, 2000)</ref>. However, the magnitude of the speech-intelligibility benefit for artificially trained voices seems to be smaller than for naturally familiar voices: Voice training improves participants' ability to report words by up to 10% <ref type="bibr" target="#b30">(Nygaard et al., 1994)</ref> when speech intelligibility is measured in terms of percentage of correct responses or by 0.52 dB <ref type="bibr" target="#b22">( Kreitewolf et al., 2017)</ref> when a threshold is estimated on the basis of manipulations of the target-to-masker ratio (TMR). For friends' voices, the benefit has been estimated as 10% to 15% <ref type="bibr" target="#b12">(Domingo et al., 2019</ref><ref type="bibr" target="#b11">(Domingo et al., , 2020;;</ref><ref type="bibr" target="#b17">Holmes et al., 2018)</ref> or 5 dB to 9 dB <ref type="bibr">(Holmes &amp; Johnsrude, 2020;</ref><ref type="bibr" target="#b19">Johnsrude et al., 2013)</ref>. A direct comparison of these studies is difficult because they tested intelligibility with different maskers (white noise in <ref type="bibr" target="#b30">Nygaard et al., 1994</ref>; speech-shaped noise in <ref type="bibr" target="#b22">Kreitewolf et al., 2017</ref>; a single competing talker in the experiments with familiar voices), and baseline performance in the unfamiliar condition differed across studies. Nevertheless, these findings imply that improved intelligibility of familiar voices is not an all-or-none phenomenon but instead may depend on the length of exposure or the setting in which voices are encountered (trained or natural). Our first aim was to assess whether brief voice training could produce speech-intelligibility benefits and, if so, how speech intelligibility relates to the length of time that participants have been trained on that voice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition of Familiar Voices</head><p>Participants are also able to explicitly recognize voices they have been trained on in the lab (e.g., <ref type="bibr" target="#b10">Doddington, 1985;</ref><ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994;</ref><ref type="bibr" target="#b41">Yonan &amp; Sommers, 2000)</ref>. Studies measuring speechintelligibility benefits from trained voices have typically measured recognition of those voices at several times during training. For example, <ref type="bibr" target="#b29">Nygaard and Pisoni (1998)</ref> found that the ability to identify 10 talkers improved steadily over 9 days of training. <ref type="bibr" target="#b41">Yonan and Sommers (2000)</ref> presented participants with 120 sentences spoken by four different talkers on 2 consecutive days; on each day of training, participants were also tested on voice identification for 80 sentences. Young adults' performance was almost perfect after only 1 day of training.</p><p>It is unclear how new voices become recognizable following shorter exposures and whether improvements in speech intelligibility parallel improvements in recognition. Our second aim in this study was to compare explicit recognition of a voice with any speech-intelligibility benefit for the same voice. The acoustic features -fundamental frequency and acoustic correlates of vocal-tract length-that are used to recognize voices and to derive the intelligibility benefit from them are at least partially overlapping <ref type="bibr" target="#b17">(Holmes et al., 2018;</ref><ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994;</ref><ref type="bibr" target="#b33">Remez et al., 1997;</ref><ref type="bibr" target="#b35">Sheffert et al., 2002)</ref>. However, <ref type="bibr" target="#b17">Holmes et al. (2018)</ref> demonstrated that a familiar voice can benefit intelligibility even if it is not explicitly recognizable. Given that speech intelligibility and voice recognition are partially dissociable, it is plausible that the intelligibility benefit from and the recognition of a previously heard voice may develop at different rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>Many people find it difficult to understand speech in noisy places. Yet voice familiarity provides a large benefit to intelligibility. We investigated the duration of voice exposure required to improve intelligibility. Speech presented with a competing talker was more intelligible when it was spoken in a voice that was previously heard for 10 to 60 min than in a novel voice. Training for 60 min provided an intelligibility benefit of 10% to 15%, commensurate with the large benefit that has been reported for naturally familiar voices, such as those of friends and spouses. These findings demonstrate that speech intelligibility can be dramatically improved with as little as 1 hr of training, highlighting the great potential of such training for improving intelligibility in everyday settings. This may particularly benefit older people and people with hearing loss, who experience particular difficulty listening in noisy settings, and people whose occupations require accurate speech perception in noisy surroundings, such as aircraft pilots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type of Training</head><p>The way that voices are trained (i.e., the type of training) has been proposed to influence how voices are learned. <ref type="bibr" target="#b7">Case et al. (2018)</ref> examined whether similarity of encoding and retrieval conditions (face-to-face interactions compared with repeating prerecorded sentences) affects implicit learning, although they found no difference. The acoustic background against which novel voices are heard might affect voice learning, but this has not yet been tested. Previously, some researchers have trained participants in quiet contexts (e.g., <ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994)</ref>, whereas others have trained participants in noisy contexts (e.g., <ref type="bibr" target="#b22">Kreitewolf et al., 2017)</ref>. However, the speech-intelligibility benefits for participants trained on voices in quiet and noisy contexts have not been compared.</p><p>Our third aim was to compare two different training conditions: voices presented alone and voices presented in the presence of multitalker babble. We might expect benefits for voices trained in noise if background noise increases cognitive load during training <ref type="bibr" target="#b25">(Mattys et al., 2012)</ref>, making participants work harder to recognize the voices <ref type="bibr" target="#b2">(Best et al., 2018)</ref> and therefore learn the voices more quickly-leading to a larger intelligibility benefit and better recognition. We also might expect benefits for participants trained on voices in noise if noise guides listeners to learn parts of a voice that are most distinct from background noise <ref type="bibr" target="#b26">(Mattys et al., 2005)</ref>, consistent with transfer-appropriate processing <ref type="bibr" target="#b28">(Morris et al., 1977)</ref> and the encoding-specificity hypothesis <ref type="bibr" target="#b38">(Tulving &amp; Thomson, 1973)</ref>-which could help participants to better understand speech or recognize a voice when it is masked by similar sounds but would likely have no effect on intelligibility or recognition in quiet.</p><p>On the other hand, we might expect benefits for voices trained in quiet if increased cognitive load because of background noise means there are fewer resources available to encode voice information <ref type="bibr" target="#b31">( Rabbitt, 1968</ref>)-leading to a larger intelligibility benefit and better recognition following training in quiet. We also might expect benefits for voices trained in quiet if increased background noise masks voice characteristics useful for recognition or intelligibility, such as fundamental frequency or formant frequencies-which might produce distinct effects on intelligibility and recognition, depending on which voice characteristics are masked <ref type="bibr" target="#b17">(Holmes et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Current Study</head><p>Here, we investigated how different amounts of experience with a voice affect recognition and intelligibility. We trained participants on three voices, each speaking for a different amount of time. We then tested participants' ability to identify those voices, and sentences spoken in the same voices, in the presence of another talker. We also compared performance with these trained voices to performance with novel voices that were not heard during training (trained and novel voices were counterbalanced across participants). Half of the participants were trained on the voices with babble noise presented simultaneously, whereas the other half heard the voices alone (in quiet) during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 53 participants. Of these, three did not complete the study. The remaining 50 participants were between the ages of 18 and 28 years (Mdn = 18.6 years, interquartile range = 1.1); 12 participants were men, 36 were women, and two preferred not to disclose gender. A sample size of 50 provides 80% power to detect within-subjects effects (i.e., among four familiarity conditions) of f ≥ .17, between-subjects effects (i.e., between two training groups) of f ≥ .32, and withinbetween-subjects interactions of f ≥ .17 <ref type="bibr" target="#b14">(Faul et al., 2007)</ref>. The familiar-voice benefit to speech intelligibility found in previous studies has a large effect size (f = 0.72 in <ref type="bibr" target="#b19">Johnsrude et al. (2013)</ref> and f = 0.88 in <ref type="bibr" target="#b17">Holmes et al. (2018)</ref>, and familiarity effects of this size should be detectable with power of about 100% in the current design.</p><p>All participants were native Canadian English speakers and had no history of hearing difficulties. They had average pure-tone hearing levels (HLs; measured at four octave frequencies between 0.5 and 4 kHz) better than 15 dB HL in each ear. The study was approved by Western University's Health Sciences Research Ethics Board, and all participants gave informed consent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>The study contained four parts: familiarization, training, an explicit-recognition test, and a speech-intelligibility test. Schematics of the four tasks are displayed in Figure <ref type="figure">1</ref>. Participants completed all four parts in a single session that lasted approximately 4 hr. Participants could take breaks between blocks within each task and were encouraged to take longer breaks between tasks.</p><p>Each participant heard three voices during familiarization and training. During familiarization, participants heard 10 sentences spoken by each of the three talkers (randomly interleaved). During training, one of the voices was heard speaking 78 sentences, another 156 sentences, and the third 468 sentences. In the explicitrecognition and intelligibility tests, they heard the same three voices and two other voices they had not previously heard. Across participants, the five talkers were counterbalanced across familiarity conditions.</p><p>Half of participants (n = 25) heard sentences alone during training (i.e., in quiet) and the other half heard the training sentences in the presence of babble noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>Acoustic stimuli were recorded using a Sennheiser e845-S microphone (Sennheiser Electronic, Wedemark, Germany) connected to a Steinberg UR22 sound card (Steinberg Media Technologies, Hamburg, Germany) in "The guards took a short break between their watches"  <ref type="figure">1</ref>. Schematics of the four phases of the study. In the familiarization phase (a), participants saw a name on the screen (e.g., "Brad") and heard a single meaningful spoken sentence (e.g., "The guards took a short break between their watches") presented without any extraneous noise (i.e., in quiet). Each sentence was spoken by one of three talkers (30 trials, 10 per condition). In the training phase (b), participants heard a single meaningful spoken sentence and had to identify which of three voices spoke each sentence. One of the voices was heard speaking 78 sentences, another 156 sentences, and the third 468 sentences (702 trials in total). Participants received feedback about whether their response was correct and were shown the correct talker name. For half of participants, all of the sentences were presented with babble noise; for the other half, all were presented in quiet. In the explicit-recognition test (c), participants judged whether closed-set sentences were spoken by familiar (three previously heard) or unfamiliar (two novel) talkers. Each sentence was presented in quiet (105 trials, 21 per talker). If the talker was familiar, participants then had to select the name that matched the voice. In the speech-intelligibility test (d), participants simultaneously heard two closed-set sentences, each of which was spoken by a different talker. They had to attend to the sentence that began with a particular name (here, "Bob") and report the other four words from the sentence by clicking one button from each of four columns. Note that only three rows of words are shown here for clarity, but the study always contained eight rows, corresponding to all of the words in the corpus (see Table <ref type="table" target="#tab_1">1</ref>). The competing sentence was always spoken by one of the two unfamiliar talkers, and the target sentence was spoken either by one of the three previously heard talkers or by the other unfamiliar talker (640 trials, 160 per condition). a single-walled, sound-attenuating booth (Model CL-13 LP MR; Eckel Industries, Morrisburg, Ontario, Canada). Stimuli were recorded in monophonic sound at a sampling rate of 44100 Hz.</p><p>During the study, participants sat in a comfortable chair in the same single-walled, sound-attenuating booth facing a 24-in. LCD monitor (either ViewSonic VG2433SMH or Dell G2410t). Acoustic stimuli were presented through the sound card and were delivered diotically through Grado Labs SR225 headphones (Grado Labs, Brooklyn, NY). Acoustic stimuli were presented at a comfortable listening level-approximately 67 dB(A) sound-pressure level. We maintained the same overall presentation level throughout the study (regardless of whether participants heard a single sentence, two sentences, or one sentence mixed with babble noise), so the level of the target sentence differed between the recognition and intelligibility tests and also between the higher and lower TMR conditions in the speech-intelligibility test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Two different sentence corpora were used in this study: one for familiarization and training and another for testing. For familiarization and training, we used meaningful sentences based on the sentence corpus used by <ref type="bibr" target="#b34">Rodd et al. (2005)</ref>, such as "The boy was able to climb the mountain." We also created new sentences based on the syntactic and syllabic structure of the existing sentences. We used 354 meaningful sentences in total (177 from <ref type="bibr" target="#b34">Rodd et al., 2005)</ref>, which are listed in the Supplemental Material available online. Of these, 351 were presented during training. During familiarization, the remaining three sentences were presented to all participants, along with a subset of 27 sentences (a different subset for different participants) that were also presented during training. We used such everyday naturalistic sentences because we wanted to approximate the natural phonetic, phonological, and semantic variety encountered when one becomes familiar with a talker in everyday life.</p><p>For testing, we wished to harness the psychometric accuracy of a closed-set procedure. We used the word set from the Boston University Gerald (BUG) corpus <ref type="bibr" target="#b20">(Kidd et al., 2008)</ref>, recorded as sentences by our participants. These sentences each contain five words, in the form "Name verb number adjective noun." An example is "Bob found three old socks."</p><p>Word report for open-set, everyday sentences is problematic. If participants are biased to report guesses, they will report more words, which could lead to higher intelligibility values when the total number of words reported is not taken into account. Guessing in openset tests is likely to lead to correct responses when sentences are semantically meaningful or when there are few lexical neighbors <ref type="bibr" target="#b36">(Sommers et al., 1997)</ref>. In contrast, the BUG matrix task requires participants to generate exactly four responses (one for each word after the name) on each trial. Intelligibility is therefore unconfounded by guessing. We used two name words ("Bob" and "Pat"). The other words each had eight possible options, which are displayed in Table <ref type="table" target="#tab_1">1</ref>. We created a subset of 384 sentences to be recorded from the BUG word set. The probabilities of each pair of words occurring together within a sentence were equated across the set.</p><p>We recorded five male talkers (20-24 years old) speaking the same 738 sentences. All talkers had a Canadian accent and had no speech impediments. To ensure that all sentences were spoken at similar rates, we played videos <ref type="bibr" target="#b16">(Holmes, 2018)</ref> indicating the desired pace for each sentence while participants completed the recordings. Participants saw the written sentence on the screen and were instructed to speak each word at the same time that a vertical bar passed the beginning of the written word. They were told to speak the sentences as naturally as possible. The recorded familiarization and training sentences each had an average duration of 3.1 s (SD = 0.7). The recorded test sentences each had an average duration of 2.4 s (SD = 0.2). The levels of recorded sentences were normalized for rootmean-square power. The babble noise was a mixture of 12 male and female talkers speaking different sentence material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Familiarization. During familiarization (Fig. <ref type="figure">1a</ref>), participants passively listened to 30 unique meaningful sentences. Of these, 10 sentences were spoken by each of three talkers. As participants listened to each sentence, a name word-"Brad," "Jeff, " or "Mike"-was displayed on the screen. Participants were asked to associate each of these names with the talker's voice. After the sentence had ended, participants clicked a mouse to hear the next sentence. Across participants, the sentences spoken by each talker were counterbalanced, as were the name words assigned to each voice. The familiarization procedure lasted approximately 10 min.</p><p>Training. In the training phase (Fig. <ref type="figure">1b</ref>), participants completed 702 trials. Each trial contained one sentence in one of the three voices. They heard one talker (most familiar) speak 468 sentences (i.e., 67% of sentences), another talker (moderately familiar) speak 156 sentences (i.e., 22% of sentences), and the remaining talker (least familiar) speak 78 sentences (i.e., 11% of sentences).</p><p>We selected the number of sentences to roughly correspond to 60, 20, and 10 min of training (respectively), which we predicted would be sufficient to observe differences among training conditions. Participants heard 351 unique sentences during training: Each sentence was heard twice (once spoken by the moderately familiar or least-familiar talker and once or twice spoken by the most-familiar talker). Across participants, the voices and sentences assigned to the three familiarity conditions were counterbalanced.</p><p>Half of participants (n = 25) heard the training sentences alone (i.e., in quiet), as during familiarization, whereas the other half heard the same sentences in the presence of simultaneous babble noise, which was presented at a signal-to-noise ratio of 0 dB. The sentencebabble mixtures were presented diotically. The babble noise began 250 ms before the sentence began and ended 250 ms after the sentence had been spoken.</p><p>After each spoken sentence had ended, a pop-up box appeared on the screen prompting participants to indicate the name associated with the talker who spoke the sentence. Feedback was provided after each response: Text presented on the screen told participants whether they had answered correctly or incorrectly and displayed the correct name; the text was colored green if participants had answered correctly and red if they had answered incorrectly. Training lasted approximately 1.5 hr and was divided into six blocks, each containing 117 trials.</p><p>Explicit-recognition test. The explicit-recognition test was presented after training. It tested whether participants recognized previously heard (trained) and novel voices as familiar and unfamiliar and whether participants could identify the names associated with the trained voices. During the explicit-recognition test (Fig. <ref type="figure">1c</ref>), participants heard BUG sentences spoken by the three trained talkers and two novel talkers. After each sentence, they indicated whether or not they had heard the talker during the training phase. If they indicated they had heard the talker during training, they were prompted to indicate the name of the talker. No feedback was provided. The recognition task contained 105 trials, each containing a unique BUG sentence: 21 sentences were spoken by each of the five talkers. Across participants, the sentences assigned to the five talkers (and conditions) were counterbalanced.</p><p>Speech-intelligibility test. Finally, participants completed a speech-intelligibility test. On each trial of this test, participants simultaneously heard two BUG sentences, each of which was spoken by a different talker. The BUG sentences were different from those presented during the explicit-recognition test. The five words contained within the two simultaneously presented sentences were always different. Participants were instructed to listen to the target sentence that began with a specified name word ("Bob" or "Pat") and report the remaining four words from that sentence by clicking words from a list of options on the screen, in any order (see Fig. <ref type="figure">1d</ref>). Participants completed 640 trials; each of the two name words-either "Bob" or "Pat"-was the target for half of the trials (in separate blocks counterbalanced across participants).</p><p>The target sentence could be spoken in any of the five voices, representing four training conditions-most familiar, moderately familiar, least familiar, and unfamiliar (two voices). The masker sentence was always in one of the two unfamiliar voices, and when the target was in an unfamiliar voice, the other unfamiliar voice spoke the masker sentence. Equal numbers of trials (160) were administered in the four conditions. Half of the masker sentences within each condition were spoken by one of the unfamiliar talkers; the other half were spoken by the other unfamiliar talker. This aspect of the design ensured that the masker talkers in all four conditions were identical. Within each condition, we presented the sentences at two different TMRs: -6 and +3 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analyses</head><p>For the explicit-recognition test, hits (correct responses) were defined as trials in which participants heard one of the trained talkers and identified the correct name. Misses were defined as trials in which participants responded that the trained talker was unfamiliar or selected an incorrect name. Hits and misses were calculated separately for the three trained talkers. Correctrejection and false-alarm rates were calculated from the 42 trials in which participants heard a novel voice.</p><p>Sensitivity <ref type="bibr" target="#b15">(Hautus, 1995)</ref> was calculated for each of the three trained talkers. Chance d′ was 0.3, and the maximum attainable d′ was 4.3.</p><p>For the speech-intelligibility test, we calculated the percentages of sentences that were reported correctly in each of the 16 conditions (4 familiarity conditions × 2 TMR conditions × 2 training groups). We calculated the familiar-voice-intelligibility benefit as the difference in the percentage of correct responses between the unfamiliar baseline condition and each of the three conditions in which a trained talker was the target (most familiar, moderately familiar, and least familiar).</p><p>To examine whether the pattern of results across manipulations differed significantly between the speech-intelligibility and explicit-recognition tasks, we converted d′ from the explicit-recognition task and the familiar-voice benefit to intelligibility (the difference in the percentage of correct responses between each of the familiar conditions and the unfamiliar baseline condition) into z scores, which were calculated separately for the two training groups.</p><p>We conducted four planned mixed analyses of variance (ANOVAs). One ANOVA was conducted for task (training, explicit recognition, and speech intelligibility) to compare training groups (between subjects) and familiarity conditions (within subjects). For the speechintelligibility test, we included an additional factor of TMR (within subjects). A fourth ANOVA directly compared performance on the explicit-recognition test and speechintelligibility test. We did not correct for multiple ANOVAs, given that all of these were planned analyses. Where Mauchly's test of sphericity was significant, we report statistics with Greenhouse-Geisser correction.</p><p>We also conducted a two-way within-subjects ANOVA to look for voice learning over the course of the study, comparing the percentage of correct responses between the beginning and end of the speech-intelligibility task. These results are also uncorrected and should be treated as exploratory because this analysis was unplanned. For this analysis, we took the first 20 trials from each familiarity-by-TMR condition (i.e., 40 trials total) and the last 20 trials from each familiarity-by-TMR condition (i.e., 40 trials total). The ANOVA compared effects of familiarity and trial position (beginning or end), collapsing across the two TMRs and training groups.</p><p>In all instances, we calculated effect sizes and confidence intervals (CIs) for effect sizes using MOTE <ref type="bibr" target="#b6">(Buchanan et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Recognition performance in the training conditions was high (Fig. <ref type="figure" target="#fig_2">2a</ref>). The most-familiar voice was recognized correctly on 98.3% (SE = 0.2) of trials, the moderately familiar voice was recognized correctly on 94.4% (SE = 1.0) of trials, and the least-familiar voice was recognized correctly on 94.3% (SE = 1.2) of trials. A two-way mixed ANOVA investigating whether performance during training differed across groups (quiet, babble; between subjects) and familiarity conditions (most familiar, moderately familiar, least familiar; within subjects) revealed no effect of group, F(1, 48) = 0.36, p = .55, ω p 2 = -.01, 95% CI = [.00, 1.00], 1 and no significant interaction between group and familiarity, F(2, 96) = 0.21, p = .81, ω p 2 = -.01, 95% CI = [.00, 1.00]. There was, however, a significant effect of familiarity, F(1.4, 67.3) = 12.64, p &lt; .001, ω p 2 = .19, 95% CI = [.01, .28], with better performance in the most-familiar condition than in the moderately familiar condition, t( <ref type="formula">49</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit recognition</head><p>The percentage of correct responses in the explicitrecognition test was good but below ceiling. The mostfamiliar voice was recognized correctly on 73.2% of trials, the moderately familiar voice was recognized correctly on 73.1% of trials, and the least-familiar voice was recognized correctly on 73.4% of trials. The unfamiliar voices were correctly recognized as unfamiliar on 84.4% of trials. Hits and false-alarm rates are displayed in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Figure <ref type="figure" target="#fig_2">2b</ref> shows the d′ values in the explicit-recognition test. We conducted a two-way mixed ANOVA with the factors training group (quiet, babble) and familiarity (most familiar, moderately familiar, least familiar). Again, there was no effect of training group, F(1, 48) &lt; 0.01, p = .95, ω p 2 = -.02, 95% CI = [.00, 1.00], and no interaction, F(2, 96) = 2.58, p = .08, ω p 2 = .02, 95% CI = [.00, .08]. There was also no effect of familiarity, F(2, 96) = 0.12, p = .89, ω p 2 = -.01, 95% CI = [.00, .08]. Collapsing across training groups, we compared recognition d′ in each familiarity condition with chance level (d′ = 0.3) using sign tests. Participants were able to identify all three voices with above-chance accuracy (S ≥ 40, p &lt; .001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech intelligibility</head><p>Intelligibility data are shown in Figure <ref type="figure" target="#fig_3">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dissociation between recognition and intelligibility</head><p>In the analyses above, there was a significant effect of familiarity for speech intelligibility but not for explicit recognition. To examine whether the pattern of results across familiarity conditions differed between the speech-intelligibility and explicit-recognition tasks, we converted d′ from the explicit-recognition task (Fig. <ref type="figure" target="#fig_2">2b</ref>  and the speech-intelligibility-benefit scores (Fig. <ref type="figure" target="#fig_2">2c</ref>) for each of the three familiar voices into z scores and entered the data into a two-way within-subjects ANOVA.</p><p>Given that effects involving training group were never significant, we pooled the groups for this analysis. The two-way interaction between task (explicit recognition, speech intelligibility) and familiarity (most familiar, moderately familiar, least familiar) was significant, F(1.8, 86.7) = 4.55, p = .017, ω p 2 = .01, 95% CI = [.00, .08]. We confirmed with follow-up one-way ANOVAs that this was driven by a significant effect of familiarity on speech-intelligibility-benefit scores, F(2, 98) = 4.27, p = .017, ω p 2 = .04, 95% CI = [.00, .13], and a nonsignificant effect of familiarity on explicit-recognition scores, F(2, 98) = 0.15, p = .87, ω p 2 = .00, 95% CI = [.00, 1.00]. These results confirm that the pattern of findings across familiarity conditions differed between the explicitrecognition and speech-intelligibility tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No evidence for voice learning during the intelligibility task</head><p>In an exploratory analysis, we investigated whether participants achieved better intelligibility at the end of the intelligibility task than at the beginning-which could potentially indicate voice learning during the intelligibility task (Fig. <ref type="figure" target="#fig_4">4</ref>). We replicated the effect of familiarity, F(3, 147) = 8.42, p &lt; .001, ω p 2 = .04, 95% CI = [.00, .10], but found no main effect of trial position, F(1, 49) = 0.01, p = .91, ω p 2 = .00, 95% CI = [.00, 1.00], and no interaction between trial position and familiarity, F(3, 147) = 0.37, p = .77, ω p 2 = .00, 95% CI = [.00, 1.00]. Thus, we found no evidence that intelligibility improved throughout the duration of the task for any of the voices; voice learning instead seems to have been restricted to the training phase of our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our results demonstrate that recognition and improved speech intelligibility for previously heard (trained) voices emerge rapidly. Even for the least-familiar talker, to which participants were exposed for approximately 10 min, we found successful voice recognition and a significant intelligibility benefit. We found a different pattern of results for explicit recognition and intelligibility, confirmed by a significant interaction between task and familiarity. Explicit recognition did not differ among the three voices that had been trained for different durations. However, speech intelligibility was best for the most-familiar voice and significantly lower for the moderately familiar and least-familiar voices. Thus, whereas recognition was relatively stable over the range of exposures we tested, intelligibility was better after longer durations of training.</p><p>We found a significant intelligibility benefit for all three trained (i.e., familiar) talkers, despite the fact that training focused on voice identification rather than reporting speech content. This demonstrates that when people learn about voices, they learn characteristics that subsequently can be exploited to enhance intelligibility, even when intelligibility is not challenged during learning. Our results demonstrate that training on voice identification provides rapid learning that improves intelligibility of the trained voices after as little as 10 min of voice training.</p><p>The benefit that we observed for the most-familiar talker (10%-15%) is similar in magnitude to that previously reported for naturally familiar voices with the same masker and task <ref type="bibr" target="#b11">(Domingo et al., 2020;</ref><ref type="bibr">Holmes &amp; Johnsrude, 2020)</ref>. This highlights the great potential of voice training for improving intelligibility in everyday settings. Voice training may be particularly beneficial for older people or people with hearing loss who experience particular difficulty listening in noisy settings (e.g., <ref type="bibr" target="#b13">Dubno et al., 1984)</ref>. There is already some evidence that older people <ref type="bibr" target="#b41">(Yonan &amp; Sommers, 2000)</ref> and older people with confirmed hearing loss <ref type="bibr" target="#b37">(Souza et al., 2013)</ref> benefit from familiar-voice information. Unlike participants in previous experiments that tested the intelligibility of trained talkers after 2 or more days of training <ref type="bibr" target="#b22">(Kreitewolf et al., 2017;</ref><ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994;</ref><ref type="bibr" target="#b41">Yonan &amp; Sommers, 2000)</ref>, our participants were trained on the voice of the mostfamiliar talker for only about 1 hr. This duration of training is comparable with everyday situations in which people become familiar with new colleagues or with radio and television presenters. The short duration of the training used here suggests that voice training could be an accessible way to improve speech intelligibility in everyday settings. Such benefits may also be relevant for people in occupations that require accurate speech perception when other sounds are presentaircraft pilots, for example. Whether familiar voices are associated with less effort as well as better intelligibility could also be investigated in future work. These results also contribute to an emerging idea that recognition of familiar voices, and the intelligibility benefit gained from them, are at least partially dissociable. The distinction between intelligibility and recognition touches on a long debate about whether indexical properties of a voice (i.e., consistent aspects of an individual's speech across utterances; see <ref type="bibr" target="#b32">Remez et al., 2007)</ref> are separate from properties that convey speech content (i.e., the words that were spoken). It was once thought that speech was stripped of indexical information in order to understand lexical content (e.g., <ref type="bibr" target="#b0">Abercrombie, 1967;</ref><ref type="bibr" target="#b3">Bricker &amp; Pruzansky, 1976;</ref><ref type="bibr" target="#b24">Liberman et al., 1967)</ref>. However, the fact that speech is more intelligible when it is spoken by familiar people (e.g., <ref type="bibr" target="#b29">Nygaard &amp; Pisoni, 1998;</ref><ref type="bibr" target="#b30">Nygaard et al., 1994;</ref><ref type="bibr" target="#b33">Remez et al., 1997)</ref> demonstrates that indexical properties of speech are used to access speech content. In addition, a functional MRI study demonstrated distinct neural activations, depending on whether participants attended to the identity of a talker or to the words they spoke <ref type="bibr" target="#b39">(von Kriegstein et al., 2005)</ref>. Fewer studies have explored whether people use familiar-voice information differently depending on whether the goal is to recognize someone's identity or to understand the words they are speaking. One previous study <ref type="bibr" target="#b17">(Holmes et al., 2018)</ref> provided evidence for a dissociation by showing that recognition and intelligibility of familiar voices are differentially sensitive to acoustic characteristics: Some participants gained an intelligibility benefit for a friend's voice after its vocal-tract length had been manipulated so it was not explicitly recognizable. Here, we provide new evidence for this dissociation by showing that recognition and intelligibility improve at different rates as voices become familiar through training.</p><p>The dissociation between intelligibility and recognition is unlikely to be fully explained by differences in difficulty. Although recognition of all three trained talkers was moderately high and did not differ across voices, these results cannot be explained by a ceiling effect: The average d′ was 2.2, whereas the maximum attainable d′ was 4.3, and the percentage of correct responses was also below ceiling at 73%. Thus, recognition could have differed among conditions but did not. Although average performance on the recognition test (73%) was better than average performance on the intelligibility test, it was not substantially better than intelligibility at the most-favorable TMR (+3 dB TMR in the most-familiar condition: 64%)-and the difference in intelligibility between the most-familiar and moderately familiar conditions was more distinct at the mostfavorable TMR. In contrast, there was no evident trend toward better explicit recognition of the most-familiar or moderately familiar talker compared with the leastfamiliar talker.</p><p>We do not find it surprising that the explicit-recognition test showed performance below ceiling, despite participants' near-perfect performance during training. Training was a three-alternative forced-choice task with a chance rate of 33%, whereas the chance rate for the explicit-recognition task was lower than this. During familiarization and training, participants may have learned characteristics that distinguished the three trained voices from each other, but those characteristics may not have enabled them to distinguish these voices from novel voices in the larger (five-talker) pool of voices. Further, the sentences presented during the recognition test were from a different corpus than those presented during training. Participants may have become very good at identifying a talker's voice in naturalistic open-set sentences, but this learning did not fully transfer to the closed-set BUG sentences. For these reasons, we do not consider the difference between training and test performance to be an interesting or important feature of our results.</p><p>Participants performed the explicit-recognition test before the intelligibility test, but they were exposed to the three trained voices equally often in both tests, so this exposure cannot account for differential effects in intelligibility among the three trained voices. Also, exposure to the novel voices in the explicit-recognition test should only reduce the magnitude of the familiarvoice benefit (not improve it). Thus, it could only work against the effect we were trying to measure, which we found to be significant for all three trained talkers. In addition, if additional voice exposure affected performance in the intelligibility test, then we would expect to find a different pattern of results between the first and last trials of the intelligibility test, but we found no evidence for this.</p><p>Recognition and intelligibility were similar regardless of whether participants were trained to recognize the talkers in quiet or in simultaneously presented babble noise-reinforcing the idea that similar training and test conditions are unnecessary for familiar-voice learning <ref type="bibr" target="#b7">(Case et al., 2018)</ref>. These results are also consistent with the conclusion mentioned above: that intelligibility is enhanced when a voice is learned, even when intelligibility is not challenged during learning, as all of our participants found it easy to report the sentences in quiet.</p><p>We selected quiet and babble for the training conditions because they both differed from a single competing talker, which is the masker we presented during the intelligibility test. A drawback of using exactly the same masker (i.e., a single competing talker) during testing and training is that it is difficult to determine whether any training benefit arises because of similarity between training and testing conditions or because of practice effects with highly similar stimuli during training and testing, which would not generalize to other conditions. Of the two training conditions we tested, babble is more similar to a single competing talker because it provides both informational and energetic masking, albeit less informational masking than a single competing talker <ref type="bibr" target="#b4">(Brungart, 2001;</ref><ref type="bibr" target="#b5">Brungart et al., 2001)</ref>. Our data provide no evidence for the encoding-specificity hypothesis <ref type="bibr" target="#b38">(Tulving &amp; Thomson, 1973)</ref> or for the idea that the additional challenge associated with listening in babble during training improves recognition or intelligibilityotherwise, we should have found better intelligibility and recognition for participants who were trained in babble than for those who were trained in quiet. It is possible that our training-in-babble noise-at 0 dB TMR-was not sufficiently challenging to promote enhanced learning, although detrimental effects of noise on recognition memory have been found at similar TMRs and at more positive TMRs in previous studies <ref type="bibr" target="#b21">(Koeritzer et al., 2018;</ref><ref type="bibr" target="#b31">Rabbitt, 1968</ref>). Nevertheless, these findings demonstrate that the benefit to intelligibility is relatively robust and is not overly sensitive to voice-training conditions.</p><p>The training task required participants to identify which of three talkers spoke on each trial. There was no difference in accuracy for participants trained in quiet and those trained in background babble. This suggests that performance during the explicit-recognition task, when participants had to say whether or not a given voice had been heard during training, would not have been affected if babble had been added. This is consistent with findings of previous studies showing that familiar voices are still recognizable in the presence of noise <ref type="bibr" target="#b2">(Best et al., 2018;</ref><ref type="bibr" target="#b9">Clarke et al., 1966;</ref><ref type="bibr" target="#b40">Wenndt &amp; Mitchell, 2012)</ref>.</p><p>In summary, we showed that a relatively small amount of training (~10 min) is sufficient for listeners to identify someone's identity from their voice and to realize a benefit to intelligibility when a competing talker is present. Nevertheless, we found that recognition and intelligibility develop over different timescales. Recognition was similar for the three voices participants were trained on for different lengths of time (~10-60 min), but intelligibility was best for the voice participants were trained on the most (~1 hr). Overall, our results demonstrate the great potential of voice training for improving intelligibility in everyday settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head><p>Action Editor: M. Natasha Rajah Editor: Patricia J. Bauer Author Contributions E. Holmes, G. To, and I. S. Johnsrude designed the study. G. To recorded the stimuli and collected the data. E. Holmes analyzed the data and drafted the manuscript. G. To and I. S. Johnsrude helped edit the manuscript. All of the authors approved the final manuscript for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) = 4.29, p &lt; .001, d z = 0.61, 95% CI = [0.30, 0.91], and in the least-familiar condition, t(49) = 3.56, p = .001, d z = 0.50, 95% CI = [0.21, 0.80]. Performance did not differ between the moderately familiar and least-familiar conditions, t(49) = 0.16, p = .88, d z = 0.2, 95% CI = [-0.26, 0.30].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. A three-way mixed ANOVA with the factors training group (quiet, babble), familiarity (most familiar, moderately familiar, least familiar, unfamiliar), and TMR (-6 dB, +3 dB; within subjects) revealed no effect of group, F(1, 48) = 0.20, p = .66, ω p 2 = -.02, 95% CI = [.00, 1.00], and no interactions involving group-Group × TMR: F(1, 48) = 1.19, p = .28, ω p 2 &lt; .01, 95% CI = [.00, .09]; Group × Familiarity: F(3, 144) = 0.17, p = .91, ω p 2 = -.01, 95% CI = [.00, 1.00]; Group × Familiarity × TMR: F(3, 144) = 0.15, p = .93, ω p 2 = .00, 95% CI = [.00, 1.00]. There was an interaction between TMR and familiarity, F(3, 144) = 7.47, p &lt; .001, ω p 2 = .03, 95% CI = [.00, .10]. The overall pattern across familiarity conditions was generally preserved at the two TMRs. All familiar voices were more intelligible than the unfamiliar voices, -6 dB TMR: t(49) ≥ 2.11, p ≤ .040, d z = 0.30; +3 dB TMR: t(49) ≥ 2.89, p ≤ .006, d z = 0.41, and the moderately familiar voice did not differ from the least-familiar voice, -6 dB TMR: t(49) = 0.26, p = .79, d z = 0.04, 95% CI = [-0.24, 0.31]; +3 dB TMR: t(49) = 1.03, p = .31, d z = 0.15, 95% CI = [-0.13, 0.42]. At both TMRs, the mostfamiliar voice was more intelligible than the moderately familiar voice, although this difference was significant only at the higher TMR, -6 dB TMR: t(49) = 1.61, p = .11, d z = 0.23, 95% CI = [-0.05, 0.51]; +3 dB TMR: t(49) = 2.56, p = .014, d z = 0.36, 95% CI = [0.07, 0.65]. Consistent with these comparisons, results showed that the main effect of familiarity was significant, F(2.6, 125.4) = 10.49, p &lt; .001, ω p 2 = .12, 95% CI = [.03, .23]. Overall, intelligibility was significantly better at +3 dB than at -6 dB TMR, F(1, 48) = 140.96, p &lt; .001, ω p 2 = .59, 95% CI = [.38, .74].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Task performance in each of the three familiarity conditions. In (a), the percentage of correct responses in the training phase is shown. The dashed line indicates chance performance (33%). In (b), d′ values in the explicit-recognition test are displayed. The dashed line indicates chance d′. In (c), the familiar-voice benefit to intelligibility is shown, collapsed across target-to-masker ratios. The familiar-voice benefit is the difference in the percentage of correct responses between each of the familiar conditions and the unfamiliar baseline condition. The shaded region in each plot shows the density of the data; data from individual participants is indicated by transparent dots. The black dots represent means, and error bars show 95% within-subjects confidence intervals (Morey, 2008). Asterisks indicate the significance of performance in each condition compared with chance (*p &lt; .05, **p &lt; .01, ***p &lt; .001).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Percentage of correct responses on the speech-intelligibility test as a function of target-to-masker ratio and familiarity condition. Error bars show ±1 SEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Percentage of correct responses at the beginning and end of the speech-intelligibility task, separately for each familiarity condition. The shaded region in each plot shows the density of the data; data from individual participants is indicated by transparent dots. The black dots represent means, and error bars show ±1 SEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Words From the Boston University Gerald (BUG) Corpus Used in the Speech-Intelligibility Test</figDesc><table><row><cell>Column 1</cell><cell>Column 2</cell><cell>Column 3</cell><cell>Column 4</cell></row><row><cell>bought</cell><cell>two</cell><cell>big</cell><cell>bags</cell></row><row><cell>found</cell><cell>three</cell><cell>blue</cell><cell>cards</cell></row><row><cell>gave</cell><cell>four</cell><cell>cold</cell><cell>gloves</cell></row><row><cell>held</cell><cell>five</cell><cell>hot</cell><cell>hats</cell></row><row><cell>lost</cell><cell>six</cell><cell>new</cell><cell>pens</cell></row><row><cell>saw</cell><cell>eight</cell><cell>old</cell><cell>shoes</cell></row><row><cell>sold</cell><cell>nine</cell><cell>red</cell><cell>socks</cell></row><row><cell>took</cell><cell>ten</cell><cell>small</cell><cell>toys</cell></row><row><cell cols="4">Note: In the speech-intelligibility test, participants simultaneously</cell></row><row><cell cols="4">heard two different five-word sentences, one beginning with "Bob"</cell></row><row><cell cols="4">and the other with "Pat," and they had to report the other four words</cell></row><row><cell cols="4">from the sentence by clicking one of eight options from each of the</cell></row><row><cell cols="2">four columns shown.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Mean Hit and False-Alarm Rates in the Explicit-Recognition Test</figDesc><table><row><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Most-familiar</cell><cell>Moderately familiar</cell><cell>Least-familiar</cell><cell></cell></row><row><cell>Training group</cell><cell>condition</cell><cell>condition</cell><cell>condition</cell><cell>False alarms</cell></row><row><cell>Quiet</cell><cell>.73 (.06)</cell><cell>.67 (.06)</cell><cell>.79 (.06)</cell><cell>.16 (.04)</cell></row><row><cell>Babble</cell><cell>.73 (.05)</cell><cell>.79 (.06)</cell><cell>.68 (.07)</cell><cell>.17 (.04)</cell></row></table><note><p>Note: Values in parentheses are ±1 SEM.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Brian Gygi</rs> for sharing the babble sound file that we used for this study.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This work was supported by the <rs type="funder">Canadian Institutes of Health Research</rs> (Operating Grant No. <rs type="grantNumber">MOP 133450</rs>) and the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (Discovery Grant No. <rs type="grantNumber">327429-2012</rs>).</p></div>
<div><head>Open Practices</head><p>Data for this study have been made publicly available via OSF and can be accessed at <ref type="url" target="https://osf.io/2gaem">https://osf.io/2gaem</ref>. Stimuli (voice recordings) cannot be shared publicly because of concerns about confidentiality. The design and analysis plans for the study were not preregistered. This article has received the badge for Open Data. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www .psychologicalscience.org/publications/badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KJfpuMs">
					<idno type="grant-number">MOP 133450</idno>
				</org>
				<org type="funding" xml:id="_Yac7s9e">
					<idno type="grant-number">327429-2012</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/0956797621991137">http:// journals.sagepub.com/doi/suppl/10.1177/0956797621991137</ref> Note 1. It is not uncommon for ω p 2 to be negative, because it corrects for bias. However, effect sizes cannot be less than 0. In this article, we report ω p 2 and associated CIs from MOTE, which outputs CIs between 0 and 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Elements of general phonetics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abercrombie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Aldine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Listen to your mother! The role of talker familiarity in infant streaming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2004.06.001</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2004.06.001" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Talker identification: Effects of masking, hearing loss, and age</title>
		<author>
			<persName><forename type="first">V</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ahlstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Roverud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Perrachione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Dubno</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.5024333</idno>
		<ptr target="https://doi.org/10.1121/1.5024333" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1085" to="1092" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Bricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pruzansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary Issues in Experimental Phonetics</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Lass</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="295" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Informational and energetic masking effects in the perception of two simultaneous talkers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1345696</idno>
		<ptr target="https://doi.org/10.1121/1.1345696" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1101" to="1109" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Informational and energetic masking effects in the perception of multiple simultaneous talkers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ericson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1408946</idno>
		<ptr target="https://doi.org/10.1121/1.1408946" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2527" to="2538" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Padfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Nuland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wikowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gillenwaters</surname></persName>
		</author>
		<ptr target="https://osf.io/tds83" />
		<title level="m">MOTE: The Shiny app to calculate effect sizes and their confidence intervals</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Short-term implicit voice-learning leads to a Familiar Talker Advantage: The role of encoding specificity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seyfarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Levi</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.5081469</idno>
		<ptr target="https://doi.org/10.1121/1.5081469" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="497" to="L502" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1907229</idno>
		<ptr target="https://doi.org/10.1121/1.1907229" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Characteristics that determine speaker recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nixon</surname></persName>
		</author>
		<idno>ESD-TR-66-636</idno>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speaker recognition-identifying people by their voices</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<idno type="DOI">10.1109/PROC.1985.13345</idno>
		<ptr target="https://doi.org/10.1109/PROC.1985.13345" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1651" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The benefit to speech intelligibility of hearing a familiar voice</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000247</idno>
		<ptr target="https://doi.org/10.1037/xap0000247" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="247" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using spatial release from masking to estimate the magnitude of the familiar-voice intelligibility benefit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Macpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.5133628</idno>
		<ptr target="https://doi.org/10.1121/1.5133628" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3487" to="3494" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effects of age and mild hearing loss on speech recognition in noise</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Dubno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Dirks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Morgan</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.391011</idno>
		<ptr target="https://doi.org/10.1121/1.391011" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-G</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03193146</idno>
		<ptr target="https://doi.org/10.3758/BF03193146" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="191" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corrections for extreme proportions and their biasing effects on estimated values of d′</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hautus</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03203619</idno>
		<ptr target="https://doi.org/10.3758/BF03203619" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="51" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech recording videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1165402</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1165402" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Version 1.0.0. Computer code</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Familiar voices are more intelligible, even if they are not recognized as familiar</title>
		<author>
			<persName><forename type="first">E</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797618779083</idno>
		<ptr target="https://doi.org/10.1177/0956797618779083" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1575" to="1583" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech spoken by familiar people is more resistant to interference by linguistically similar speech</title>
		<author>
			<persName><forename type="first">E</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000823</idno>
		<ptr target="https://doi.org/10.1037/xlm0000823" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1465" to="1476" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swinging at a cocktail party: Voice familiarity aids speech perception in the presence of a competing voice</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hakyemez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Trang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797613482467</idno>
		<ptr target="https://doi.org/10.1177/0956797613482467" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1995" to="2004" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Listening to every other word: Examining the strength of linkage variables in forming streams of speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Mason</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2998980</idno>
		<ptr target="https://doi.org/10.1121/1.2998980" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3793" to="3802" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The impact of age, background noise, semantic ambiguity, and hearing loss on recognition memory for spoken sentences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Koeritzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Van Engen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Peelle</surname></persName>
		</author>
		<idno type="DOI">10.1044/2017_JSLHR-H-17-0077</idno>
		<ptr target="https://doi.org/10.1044/2017_JSLHR-H-17-0077" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="751" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Implicit talker training improves comprehension of auditory speech in noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kreitewolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Von Kriegstein</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2017.01584</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2017.01584" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1584</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A crosslanguage familiar talker advantage</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2933847</idno>
		<ptr target="https://doi.org/10.1121/1.2933847" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3331</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perception of the speech code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Shankweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Studdert-Kennedy</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0020279</idno>
		<ptr target="https://doi.org/10.1037/h0020279" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="431" to="461" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech recognition in adverse conditions: A review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mattys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1080/01690965.2012.705006</idno>
		<ptr target="https://doi.org/10.1080/01690965.2012.705006" />
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="953" to="978" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integration of multiple speech segmentation cues: A hierarchical framework</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mattys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Melhorn</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.134.4.477</idno>
		<ptr target="https://doi.org/10.1037/0096-3445.134.4.477" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="500" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Confidence intervals from normalized data: A correction to Cousineau</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tutorial in Quantitative Methods for Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="64" />
			<date type="published" when="2005">2008. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Levels of processing versus transfer appropriate processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bransford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Franks</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-5371(77)80016-9</idno>
		<ptr target="https://doi.org/10.1016/S0022-5371(77)80016-9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="519" to="533" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Talker-specific learning in speech perception</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Nygaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03206860</idno>
		<ptr target="https://doi.org/10.3758/BF03206860" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="376" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Speech perception as a talker-contingent process</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Nygaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.1994.tb00612.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.1994.tb00612.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="46" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Channel-capacity, intelligibility and immediate memory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M A</forename><surname>Rabbitt</surname></persName>
		</author>
		<idno type="DOI">10.1080/14640746808400158</idno>
		<ptr target="https://doi.org/10.1080/14640746808400158" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="248" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the perception of similarity among talkers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellowes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Nagel</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2799903</idno>
		<ptr target="https://doi.org/10.1121/1.2799903" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3688" to="3696" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Talker identification based on phonetic information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellowes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.23.3.651</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.23.3.651" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rodd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhi009</idno>
		<ptr target="https://doi.org/10.1093/cercor/bhi009" />
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1261" to="1269" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to recognize talkers from natural, sinewave, and reversed speech samples</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Sheffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellowes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Remez</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.28.6.1447</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.28.6.1447" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1447" to="1469" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Some considerations in evaluating spoken word recognition by normal-hearing, noise-masked normal-hearing, and cochlear implant listeners. I: The effects of response format</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.1097/00003446-199704000-00001</idno>
		<ptr target="https://doi.org/10.1097/00003446-199704000-00001" />
	</analytic>
	<monogr>
		<title level="j">Ear and Hearing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The advantage of knowing the talker</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gehani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mccloy</surname></persName>
		</author>
		<idno type="DOI">10.3766/jaaa.24.8.6</idno>
		<ptr target="https://doi.org/10.3766/jaaa.24.8.6" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Academy of Audiology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Encoding specificity and retrieval processes in episodic memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tulving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Thomson</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0020071</idno>
		<ptr target="https://doi.org/10.1037/h0020071" />
	</analytic>
	<monogr>
		<title level="j">Psycho logical Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="352" to="373" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interaction of face and voice areas during speaker recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Von Kriegstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sterzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Giraud</surname></persName>
		</author>
		<idno type="DOI">10.1162/0898929053279577</idno>
		<ptr target="https://doi.org/10.1162/0898929053279577" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="376" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Familiar speaker recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wenndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference session]. 37th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03-25">2012. March 25-30</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The effects of talker familiarity on spoken word identification in younger and older listeners</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Yonan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1037/0882-7974.15.1.88</idno>
		<ptr target="https://doi.org/10.1037/0882-7974.15.1.88" />
	</analytic>
	<monogr>
		<title level="j">Psychology and Aging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="99" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multivoxel patterns reveal functionally differentiated networks underlying auditory feedback processing of speech</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vicente-Grabovetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cusack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.6319-11.2013</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.6319-11.2013" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4339" to="4348" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
