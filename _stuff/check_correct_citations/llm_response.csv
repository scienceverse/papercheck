xref_id,id,text,section,div,p,s,expanded,answer,time,tokens,error,error_msg
b37,paper_1016,"We planned this sample size given the possibilities of participants' recruitment at the time of the preregistration; it was not guided by an a priori power analysis (see Lakens, 2022).",method,8,1,3,"In the study reported here, we made some deviations from the preregistered procedure. First, we tested more participants than initially declared (n = 104 vs. n = 60). We planned this sample size given the possibilities of participants' recruitment at the time of the preregistration; it was not guided by an a priori power analysis (see Lakens, 2022). While conducting the study, we had better recruitment opportunities than expected. We have also considered more analyses than initially preregistered. In the following parts, we clearly indicated which analyses were preregistered, and which were not.",GOOD,0.037439136,311,NA,NA
b44,paper_1020,"Power analysis is not the only way to justify your sample size (see Lakens, 2022), but despite increased attention to statistical power, it is still rare to find articles that justified their sample size through power analysis (Chen & Liu, 2019;Guo et al., 2014;Larson & Carbine, 2017).",intro,1,2,4,"For decades researchers have highlighted that empirical research has chronically low statistical power (Button et al., 2013;Cohen, 1962;Sedlmeier & Gigerenzer, 1989). This means that the study did not include enough participants to reliably detect a realistic effect size (see Table 1 for a definition of key terms). One method to avoid low statistical power is to calculate how many participants you need for a given effect size in a process called ""power analysis"". Power analysis is not the only way to justify your sample size (see Lakens, 2022), but despite increased attention to statistical power, it is still rare to find articles that justified their sample size through power analysis (Chen & Liu, 2019;Guo et al., 2014;Larson & Carbine, 2017). Even for those that do report a power analysis, there are often other problems such as poor justification for the effect size, misunderstanding statistical power, or not making the power analysis reproducible (Bakker et al., 2020;Beribisky et al., 2019;Collins & Watt, 2021). Therefore, we present a beginner's tutorial which outlines the key decisions behind power analysis and walk through how it applies to t-tests for two independent samples and two dependent samples. We expect no background knowledge as we will explain the key concepts and how to interact with the software we use.",GOOD,0.056943695,481,NA,NA
b44,paper_1020,"There is a direct relationship between observed power and the p-value of your statistical test, where a p-value of .05 means your observed power is 50% (Lakens, 2022).",intro,5,2,3,"There is also post-hoc power analysis if we want statistical power as the outcome given an observed effect size, sample size, and alpha. Post-hoc power analysis is an attractive idea, but it should not be reported as it essentially expresses the p-value in a different way. There is a direct relationship between observed power and the p-value of your statistical test, where a p-value of .05 means your observed power is 50% (Lakens, 2022). Remember, probability in frequentist statistics does not apply to individual events, so using the observed effect size in a single study ignores the role of the smallest effect size of interest in the long-run. As post-hoc power is uninformative, we only focus on a priori and sensitivity power analysis in this tutorial.",GOOD,0.032496747,344,NA,NA
b44,paper_1020,"Setting your inputs is the most difficult part of power analysis as you must understand your area of research and be able to justify your choices (Lakens, 2022).",intro,13,1,3,"Now that you are familiar with the concepts, we turn our focus to decision making in power analysis. In part one, we defined the main inputs used in power analysis, but now you must decide on a value for each one. Setting your inputs is the most difficult part of power analysis as you must understand your area of research and be able to justify your choices (Lakens, 2022).",GOOD,0.026581077,264,NA,NA
b44,paper_1020,"Some of these decisions are difficult to make and all the strategies are not always available, but there are different sources of information you can consult for choosing and justifying your smallest effect size of interest (Lakens, 2022).",intro,16,2,2,"Many studies approach power analysis with a single effect size and value for power in mind, but as we will demonstrate in part three, power exists along a curve. Some of these decisions are difficult to make and all the strategies are not always available, but there are different sources of information you can consult for choosing and justifying your smallest effect size of interest (Lakens, 2022).",GOOD,0.027618065,262,NA,NA
b44,paper_1020,"We also know meta-analyses can report inflated effect sizes due to publication bias (Lakens, 2022), meaning you can look for a more conservative estimate such as the lower bound of the confidence interval around the average effect or if the authors report a bias-corrected average effect.",intro,16,3,4,"First, you could identify a meta-analysis relevant to your area of research which summarises the average effect across several studies. If you want to use the estimate to inform your power analysis, you must think about whether the results in the meta-analysis are similar to your planned study. Sometimes, meta-analyses will report a broad meta-analysis amalgamating all the results, then report moderator analyses for the type of studies they include, so you could check whether there is an estimate restricted to methods similar to your study. We also know meta-analyses can report inflated effect sizes due to publication bias (Lakens, 2022), meaning you can look for a more conservative estimate such as the lower bound of the confidence interval around the average effect or if the authors report a bias-corrected average effect.",GOOD,0.03266433,346,NA,NA
b44,paper_1020,"Designing an informative study is a balance between your inferential goals and the resources available to you (Lakens, 2022).",intro,20,3,2,"Power analysis is a reflective process, and it is important to keep these three lessons in mind when designing your study. Designing an informative study is a balance between your inferential goals and the resources available to you (Lakens, 2022). That is why we framed changes in the inputs around how many hours of data collection your study would take assuming it lasted 30 minutes. You will rarely have unlimited resources as a researcher, either from the funding body supporting your research, or from the number of participants in your population of interest. Planning your study with statistical power in mind provides you with the most flexibility as you can make decisions, like considering a one-tailed test or using a within-subjects design, before you can preregister and conduct your study.",BAD,0.04581349,339,NA,NA
b44,paper_1020,"Updated 04/04/2022 90% or 95%, it will require more participants, so you must perform a cost-benefit analysis based on how easy it will be to recruit your sample(Lakens, 2022).",tab,2,1,6,"Updated 04/04/2022 90% or 95%, it will require more participants, so you must perform a cost-benefit analysis based on how easy it will be to recruit your sample(Lakens, 2022). One-and two-tailed testsIn tests comparing two values, such as the difference between two groups or the relationship between two variables, you can choose a one-or two-tailed test. Lakens (2016a) argued one-tailed tests are underused and offer a more efficient procedure. As the rejection region is one 5% area (instead of two 2.5% areas), the critical value is smaller, so holding everything else constant, you need fewer participants for a statistically significant result. One-tailed tests also offer a more severe test of a hypothesis since the observed result must reach the rejection region in the hypothesised direction. The p-value in your test may be smaller than alpha, but if the result is in the opposite direction to what you predicted, you still cannot reject the null hypothesis. This means one-tailed tests can be an effective option when you have a strong directional prediction.",GOOD,0.037712234,416,NA,NA
b37,paper_322,"This sample size aligns with guidelines suggesting that empirical research should involve more than 30 but fewer than 500 participants (Lakens, 2022).",method,15,1,5,"This study was conducted as an experimental study in two government high schools in the Kashmir district. It involved two sections of a girls' high school and two sections of a boys' high school, aiming to enhance students' career choices. A total of 284 students from both high schools participated in this therapy. Purposive sampling was used to collect student data (Campbell et al., 2020). This sample size aligns with guidelines suggesting that empirical research should involve more than 30 but fewer than 500 participants (Lakens, 2022). The demographic pro le of the respondents provides a detailed view of the study's target population (see Table 1). Workshops focused on goal setting, planning, and organizing were conducted. Decision-making skills were taught through career-related simulations.",BAD,0.046411153,343,NA,NA
b27,paper_329,"Future studies could further expand the sample size, use more scientific methods to calculate the sample size (Lakens, 2022;Schoemann et al., 2017), and adopt stratified sampling for participant recruitment.",discussion,11,4,10,"In this study, we tested the hypothesis regarding the chain medication effect of paternal parenting style and experiential avoidance, in relation to mother's parenting style on negative emotions, expanding on past studies on parenting style. However, there are some limitations to the current study. First, the measurement tools need to be further improved. This study adopts the recall method to measure the parenting style of college students, and the absence of instructions to participants about which period of parenting they should recall is a limitation. This may be due to the lack of explicit inquiry in the original scale regarding the specific period of growth experience recalled by participants. The limitations of recalling parenting styles have been discussed in previous studies (Rothrauff et al., 2009). One study noted that retrospective measurements ""may constitute valid indicators of the individual's current perception of those features, and as such, may be useful in understanding psychological development or adjustment"" (Henry et al., 1994). While this may be a challenge that cannot be resolved at the present stage, future studies will strive to employ diversified measures by integrating subjective reports from both parents and children when assessing parenting style. Secondly, the representativeness of the sample needs to be further improved. Future studies could further expand the sample size, use more scientific methods to calculate the sample size (Lakens, 2022;Schoemann et al., 2017), and adopt stratified sampling for participant recruitment. Given that participants in this study were from the same university, the applicability of the results of this survey needs to be further verified. Readers need to be cautious if they plan to generalize the results of the survey outside the survey institution. In addition, there are limitations in conducting cross-sectional studies with college students as participants. Future studies can extend the range of participants, conduct longitudinal follow-up surveys, and examine the interactive effects of an individual's psychological traits on parenting styles.",GOOD,0.05826005,566,NA,NA
b24,paper_362,"While larger sample sizes can be beneficial, Lakens (2022) suggests that for studies like ours, combining in-depth testing with qualitative insights and expert heuristics is substantial in providing broader applicability of the insights gained.",method,6,3,3,"Our study utilised a sample size of 102, extending beyond a basic survey by conducting in-depth testing, training participants, and observing their interactions with the tool. This comprehensive approach and regional diversity to capture user variations provided richer data than a simple survey. While larger sample sizes can be beneficial, Lakens (2022) suggests that for studies like ours, combining in-depth testing with qualitative insights and expert heuristics is substantial in providing broader applicability of the insights gained.",GOOD,0.027794242,280,NA,NA
b43,paper_370,"These hypotheses will be tested at an alpha of .05, following Läkens (2022).",intro,7,1,6,"Our research focused on two main questions: 1) whether social exclusion alters one's perception of a situation as more negative, and 2) whether personality traits along with situational perception influence feelings of neglect and exclusion. To address our research questions, we formulated hypotheses based on Horstmann & Ziegler (2019) and Brown et al. (2015) regarding situational perception's impact on affect. We propose: H1.1) Social exclusion alters situational perception to be more adverse and negative, and less dutiful and positive. H1.2) These effects are not influenced by personality traits. H1.3) Expected effect sizes for H1.2 are between 0.30 and 0.40, derived from Horstmann & Ziegler (2019;ß ~.38) and Sherman et al. (2015;ß ~ .35). These hypotheses will be tested at an alpha of .05, following Läkens (2022).",GOOD,0.035559314,388,NA,NA
b43,paper_370,"Guided by Läkens (2022), we utilized Monte-Carlo simulations to identify the sample size required for 80% power at an alpha of .05, referencing McDonald & Donnellan (2012) and Sherman et al. (2015) for effect sizes in personality and situational variables, respectively.",method,9,1,2,"Power analysis. Guided by Läkens (2022), we utilized Monte-Carlo simulations to identify the sample size required for 80% power at an alpha of .05, referencing McDonald & Donnellan (2012) and Sherman et al. (2015) for effect sizes in personality and situational variables, respectively. Initial simulations suggested 350 participants provided at least 80% power for personality variables. We adjusted our sample size to over 450 to ensure at least 90% power, as recommended by Schöndbrot & Perugini (2013) and detailed in our supplementary materials (<https://osf.io/yw64e/? view_only=bc5ed88572fd4cd29ea1860cca71c468>).",GOOD,0.032218937,341,NA,NA
b43,paper_370,"We anticipated a moderate correlation between situational perception and both mood states and feelings of neglect and exclusion, expecting effect sizes between 0.30 and 0.40, and used a .05 alpha level for testing, following Läkens's (2022) guidelines.",discussion,18,1,4,"Study 2 aimed to explore changes in situational perception within individuals affected by social inclusion or exclusion, replicating and extending findings from our initial study on the connection between situational perception and affect. We posited that H2.1) being in a social exclusion condition would cause participants to view the Cyberball situation as more adverse, negative, less positive, and deceitful compared to social inclusion condition. Further, we conjectured that H2.2) the impact of situational perception on emotional responses would be independent from personality traits. We anticipated a moderate correlation between situational perception and both mood states and feelings of neglect and exclusion, expecting effect sizes between 0.30 and 0.40, and used a .05 alpha level for testing, following Läkens's (2022) guidelines.",GOOD,0.032633585,348,NA,NA
b43,paper_370,"A Monte-Carlo simulation power analysis (Läkens, 2022), was conducted to estimate the sample size required to achieve at least 80% power with an alpha of .05 for the expected effect sizes.",method,20,1,2,"Power analysis and determination of sample size. A Monte-Carlo simulation power analysis (Läkens, 2022), was conducted to estimate the sample size required to achieve at least 80% power with an alpha of .05 for the expected effect sizes. Drawing on prior effect sizes from Sherman et al. (2015) (82.62%) being female, and ages ranging from 18 to 41 (M = 20, SD = 3). Three were excluded from linear mixed-models analysis for not following instructions. In this withinsubjects study, participants engaged in the Cyberball game, completed the DIAMONDS brief, and answered mood state questions twice, experiencing both inclusion and exclusion conditions in a randomized order. To mitigate order effects, conditions were counterbalanced across the final sample (N=256), with half (128) starting in the ""included"" condition and the other half in the ""excluded"" condition.",GOOD,0.034629557,378,NA,NA
b50,paper_386,"However, the adequacy of the sample may also be judged by other considerations (e.g., precision in estimating population values, representativeness and generalizability to the target population; see [51]).",discussion,19,13,2,"Finally, the size of the sample in this study was deemed adequate on the basis of considerations pertaining to statistical significance (i.e., statistical power to enable the testing of a specific hypothesis). However, the adequacy of the sample may also be judged by other considerations (e.g., precision in estimating population values, representativeness and generalizability to the target population; see [51]). For example, larger samples are likely to reflect more heterogeneity or encompass more of the diversity found in populations. Pursuing a larger sample size in future studies would help address these additional considerations and would, therefore, be desirable.",GOOD,0.030057393,307,NA,NA
b33,paper_387,"First, the sensitivity power analysis suggested that the analyses may be underpowered if one considers a detectable effect size of R 2 = 0.10 to be larger than a practically or theoretically relevant effect size [34].",discussion,24,1,3,"Using self-report questionnaire methods announced over social media, this study gathered data from a volunteer sample of 93 US military veterans about their use of dark humor, their sense of connectedness to others, and their current level of life satisfaction. With these methodological procedures come some reasons to be cautious while interpreting the findings of these studies. First, the sensitivity power analysis suggested that the analyses may be underpowered if one considers a detectable effect size of R 2 = 0.10 to be larger than a practically or theoretically relevant effect size [34]. Larger sample sizes are desirable because they increase the likelihood that a finding could be replicated [35,36]. Given that little to no known research exists on the use of dark humor and the well-being of military service members, however, the current findings may still offer descriptive and heuristic theoretical value even while also requiring further testing for replication.",GOOD,0.039340685,361,NA,NA
b45,paper_388,This analysis was performed with the software GPower 3.1 [47].,method,11,6,2,"A common criterion is the completion of an a priori power analysis (β) for a predefined significance level (α), used to reject the null hypothesis that the proposed regression or the coefficient of interest is not significant [46]. This analysis was performed with the software GPower 3.1 [47]. Thus, we confirmed that this sample size ensured that the statistical power for testing both the overall significance of the model and the significance of the individual coefficients exceeded 99% at the 5% significance level. 2.",GOOD,0.029942602,289,NA,NA
b31,paper_405,"For this, sample size calculations were based on a smallest effect size of interest (Lakens, 2022;Lakens et al., 2018).",method,3,1,3,"In the sample size planning, we focused on the analysis that required the largest sample. This was the correlational analysis of the association between ACT interpretation bias scores and the mental health-related self-report measures of personality and well-being, used to evaluate association with mental-health related measures. For this, sample size calculations were based on a smallest effect size of interest (Lakens, 2022;Lakens et al., 2018). For the specific research question at hand, there is little previous research that would suggest effect sizes to be expected, and the few previous findings that have been reported (see Introduction) are probably not suitable for deriving reliable estimates of effect sizes due to the small samples in which they were obtained. From other studies in other domains of psychology, we know that correlations between implicit behavioral measures (as acquired in the ACT) and explicit self-report measures (as assessed with questionnaires) are typically rather small, rarely exceed effect sizes of r = .25, and are often considerably lower (Mischel, 1968;Bernoster et al., 2019;Bosco et al., 2015). We therefore wanted to ensure that we could detect correlations well below .25 with decent statistical power. On the other hand, very small correlations close to zero did not appear to be of theoretical interest to us. Taking these two aspects into account, we defined .15 as the smallest effect size of interest for this study. For this smallest effect size of interest, sample size calculations suggested a sample size of N = 345 to be necessary to detect correlations of r ≥ .15 with 80% power at a p level of .05 in two-sided tests (R package pwr; Champely, 2020).",GOOD,0.045970028,535,NA,NA
b31,paper_415,"While the sample size is smaller than initially calculated, it may still provide valuable initial insights, especially among urban youth, a critical component of any conservation strategy given the increasing propensity towards urbanization [30,31].",method,4,1,5,"We initially calculated the survey's sample size using Calculator.net (<https://www.calculator.net/sample-size-calculator.html>, accessed on 10 February 2023), aiming for a 95% confidence level and a 5% margin of error based on an estimated 50% level of awareness regarding insect pollinator conservation among the Moroccan public. The target population included individuals across Morocco who are active on social media platforms. The calculated sample size necessary for a statistically representative outcome was 385 participants. However, our cross-sectional survey garnered 301 respondents. While the sample size is smaller than initially calculated, it may still provide valuable initial insights, especially among urban youth, a critical component of any conservation strategy given the increasing propensity towards urbanization [30,31].",BAD,0.039950418,336,NA,NA
b47,paper_418,"In addition, the 384-strong sample size is acceptable for this study, according to [51].",method,10,3,2,"A sample size of 384 academically advanced adults with high social media engagement was determined by employing a sample size calculator available online [50]. In addition, the 384-strong sample size is acceptable for this study, according to [51].",BAD,0.03436373,233,NA,NA
b55,paper_419,The sample size was determined based on resource limitations and heuristics [56].,method,4,1,3,"Native Swedish-speaking individuals with normal or corrected vision and between 18 and 35 years old were recruited for this study. All participants met the following inclusion criteria: absence of disabilities including sensory and physical disabilities but excluding corrected visual deficits, absence of neuropsychiatric and developmental disabilities, and absence of psychological disorders. The sample size was determined based on resource limitations and heuristics [56]. More specifically, previous studies with similar manipulations and designs showed statistically significant effects with samples in the range of 20 to 50 participants [22,24,28,29,57] and such an approach was also deemed feasible in our study given the available funding and the planned balancing of conditions. Thirty-six participants were recruited, and our expected sample size was reached. Participants were recruited by advertisement at the University campus, and by approaching groups of students to inform them about the study. The age of the participants spanned between 20 and 28 (M = 23.7, SD = 1.99), and most were female (72%). The Matrices subtest from WAIS-IV [58] indicated better-than-normal non-verbal ability (M = 13.1, SD = 3.08), and the Letter-number sequencing subtest from WAIS-IV revealed typical, or slightly higher, levels of verbal working memory (M = 11.4, SD = 2.25). The study adheres to the ethical principles of the Declaration of Helsinki [59], was approved by the Regional Ethical Review Board in Linko ¨ping (dnr 2017/141-31), and the participants gave their written informed consent for participation. The individual displayed in Fig 1 in this manuscript has given written informed consent (as outlined in PLOS consent form) to publish these case details. Data was collected in the spring of 2019 and pseudonymized before analysis.",BAD,0.055414047,568,NA,NA
b21,paper_430,"Based on the available participants and limited time to alter training schedules within a highly trained cohort, a final sample size of 11 highly trained swimmers (six males, five females; age: 17 ± 3 years; height: 1.71 ± 0.05 m; body mass: 60.6 ± 8.3 kg; 200 m freestyle World Aquatic points: 650 ± 99) was justified for the purpose of this study [22,23].",method,4,1,3,"An a priori power calculation [repeated measures analysis of variance (ANOVA) for within-between interactions; two groups; six measures; α = 0.05; ß = 0.80; correspondence = 0.78 (based on repeatability data in Section 2.4)] suggested that 14 swimmers were required to identify a small effect size (0.20) in swimming performance (G*Power, v.3.1.9.4, Universität Düsseldorf, Düsseldorf, Germany). This study recruited 17 highly trained middle-distance swimmers (aged ≥ 16 years, nationally competitive at 200-400 m distances [21]); however, six swimmers withdrew from data collection before all data were collected (attrition × 4, injury × 1, withdrew consent × 1). Based on the available participants and limited time to alter training schedules within a highly trained cohort, a final sample size of 11 highly trained swimmers (six males, five females; age: 17 ± 3 years; height: 1.71 ± 0.05 m; body mass: 60.6 ± 8.3 kg; 200 m freestyle World Aquatic points: 650 ± 99) was justified for the purpose of this study [22,23]. Throughout the investigation, all swimmers were engaged in an endurance training phase consisting of 6-8 pool (mean swimming volume: 52.1 ± 7.7 km•week -1 ) and 2-3 land-based (~60 min) training sessions•week -1 . Written informed consent was provided prior to participation in this study by all swimmers and their primary caregivers if aged under 18 years. Institutional ethical approval was granted in accordance with the Declaration of Helsinki.",BAD,0.061552081,555,NA,NA
b26,paper_442,"While a priori power analysis is a widely employed technique for determining sample size, alternative methods may be considered for making informed decisions regarding sample size in the presence of resource limitations (Lakens, 2022).",method,3,1,4,"Initially, 1892 participants were recruited for our online study via a local survey data collection agency (Istanbul Economics Research; <https://researchistanbul.com/home/>). A target sample size of 1,500 was determined before data collection. Initially, given the constraints of our limited resources for this study, we made the decision to maximize participant recruitment, resulting in a total of 1500 participants. While a priori power analysis is a widely employed technique for determining sample size, alternative methods may be considered for making informed decisions regarding sample size in the presence of resource limitations (Lakens, 2022). Considering potential attrition and without any data analysis, we oversampled, ending with 1892 participants. As preregistered, those who failed to provide complete answers to MFQ-2 (n = 675) and those who were identified as multivariate outliers on the 36 items of the MFQ-2 scale as exhibiting in Mahalanobis distance (n = 118, χ2 (36) = 67.99, p < .001) were excluded from the analyses. The final sample size was 1099 (M age = 33.22, SD = 16.17; 578 female). Post hoc, we ran both sensitivity and achieved power analysis. The standard sensitivity analysis for a Pearson correlation test with α = 0.05, 1 -β = 0.90, and N = 1099 revealed that our design could reliably detect magnitudes greater than r = .10. The achieved power analysis (Moshagen & Erdfelder, 2016) with RMSEA = 0.05 as the level of misspecification on MFQ-2 items revealed that 1099 observations achieved power higher than > 99.9%.",BAD,0.060797383,553,NA,NA
b8,paper_443,Masing-masing kelurahan diambil responden sebanyak 30 orang sesuai dengan kebutuahn dalam penelitian ini [9].,method,6,1,2,"This study used questionnaires to respondents (120 respondents) who live on the banks of the Siak River in this research area. Masing-masing kelurahan diambil responden sebanyak 30 orang sesuai dengan kebutuahn dalam penelitian ini [9]. The questionnaire is a quantitative research instrument consisting of closed-ended questions that contain predetermined answer choices, scales, and categories. In this study, the questionnaire was used to obtain information about the characteristics of respondents who were settlers on the banks of the Siak River. Then, information about respondents' perceptions of environmental awareness/friendliness and their pro-environmentals behavior.",BAD,0.037582306,312,NA,NA
b29,paper_452,The parameters utilized adhere to previous publications in G Power software studies [29][30][31].,method,16,6,4,"The sample size was determined using G Power 3.1.9.7 software. Based on prior research [12,13,21], we employed a theoretical medium effect size (0.50 for the t-test; 0.3 for the Chi-square analysis; 0.3 for the correlation analysis; and 0.25 for the ANOVA test). The significance level (α) was set at 0.05, with a desired power of 0.8. The parameters utilized adhere to previous publications in G Power software studies [29][30][31].",GOOD,0.029178639,303,NA,NA
b35,paper_456,"This sample size was determined by the number of credits that we had for this experiment (i.e., resource constraints; Lakens, 2022).",method,6,1,2,"We planned to recruit 160 participants. This sample size was determined by the number of credits that we had for this experiment (i.e., resource constraints; Lakens, 2022).",BAD,0.031420229,222,NA,NA
b31,paper_459,We recruited a convenience sample [32] of all first-year bachelor's degree students available at the Faculty of Physical Education and Sport at Charles University from Physical Education and Sport and Coaching study programmes.,method,3,1,1,"We recruited a convenience sample [32] of all first-year bachelor's degree students available at the Faculty of Physical Education and Sport at Charles University from Physical Education and Sport and Coaching study programmes. The reached sample consisted of 111 students (35 women and 76 men) aged 19-23 years (mean = 20.21, SD = 1.02 years) (further sample descriptives are available in the Supplemental digital content). All participants were active athletes, who passed a semester-long ""Basics of gymnastics"" course focused on the fundamentals of gymnastics, including learning handstand with all necessary drills and preparatory exercises used in learning static handstand. The Basic gymnastics course curriculum does not differ between the two study programmes. In addition, before the study onset, all participants underwent two 45minute lessons directly focused on static handstands performed. On this basis, we assumed that students are able to master handstand to a sufficient degree. Only participants with no history of shoulder surgeries or acute upper limb injury were allowed to participate.",GOOD,0.036057088,398,NA,NA
b32,paper_464,"Fourth, although the inclusion of a range of RGNC provided an adequate sample size for formal quantitative evaluation (Lakens, 2022), RGNC are a heterogenous group of conditions with varying presentations.",discussion,21,1,10,"There are several limitations to consider. First, all collateral measures were based on parent report and the perspectives of the children were not able to be captured due to the developmental and communicative level of all participants. Parents may have perceived and/or reported improvements in other areas as a reflection of improved sleep; a Halo effect (Hunter et al., 2020;McLay et al., 2022b). In addition, parents were aware of the rationale for administering the collateral effect measures and in turn may have responded in a socially desirable manner. To prevent potential biases influencing results, future research should obtain data from additional sources for triangulation. Second, while the psychometric properties of the selected measures were good, it is possible some were not sensitive enough to detect small changes (e.g., the RQI has only 5 broad items). In addition, although most have been used extensively with children with neurodevelopmental conditions (e.g., Glasson et al., 2020;McLay et al., 2022b), some (e.g., CBCL 5-18 years) have not been validated with this population, and thus may have compromised reliability and validity. Observational data would ensure small and specific changes were captured and validated (e.g., Cohen et al., 2018). Third, although this study administered a range of collateral effect measures, sleep difficulties can negatively impact a wide range of areas of functioning (e.g., childrens' cognitive functioning, school attendance and communication, and parents' child-related stress and perceived control) that were not measured. Fourth, although the inclusion of a range of RGNC provided an adequate sample size for formal quantitative evaluation (Lakens, 2022), RGNC are a heterogenous group of conditions with varying presentations. We know the etiology of sleep difficulties and the child and parent variables assessed are multifaceted, and the direction of the relationship varies. Therefore, the degree to which children and families studied here are representative of the range of children with RGNC is unknown, as is the generality of intervention effects. Finally, although these results are promising, it is not possible to determine whether changes in sleep and/or the strategies taught in behavioral sleep intervention directly resulted in all the collateral benefits reported. The intervention duration for some participants was prolonged (maximum = 210 nights), and thus so was the time between completion of pre-and post-intervention measures. It is possible change could have occurred without intervention, at least in some cases and particularly for young children in sensitive developmental periods. In future research, comparison to a waitlist control group would clarify if the magnitude of change was greater for those who received a behavioral sleep intervention. On the other hand, for those who had shorter intervention periods (minimum = 29 nights), collateral effects might not yet have had time to emerge. The duration of any collateral benefits is also important, and future research should consider administering collateral measures during follow-up periods, to determine the maintenance of collateral effects or whether collateral effects emerge later (McLay et al., 2022b).",GOOD,0.072741075,804,NA,NA
b44,paper_466,"Given the lack of prior research into the treatment of sleep disturbance with this population, constraints of time and financial support, the disruptions caused by COVID-19, and that by definition RGNC are rare, this N (26) is considered sufficient for research to be informative (Lakens, 2022).",method,4,1,5,"This study is a retrospective analysis of case data collected within a series of single case experimental studies (e.g., Woodford et al., 2022;2024a), similar to that conducted by McLay et al. (2020). Case data was gathered in the context of non-concurrent, multiple-baseline across participants experimental designs wherein all participants were randomly allocated pre-determined baseline lengths (namely, 7, 14, or 21 days). Such random allocation ensured that extraneous factors that may have affected response to intervention (e.g., researcher biases) were controlled for, while the assigned lengths were minimally sufficient to permit stability to be observed (Barlow et al., 2009;Christ, 2007;Cooper, et al., 2020;Kratochwill et al., 2010). Given the lack of prior research into the treatment of sleep disturbance with this population, constraints of time and financial support, the disruptions caused by COVID-19, and that by definition RGNC are rare, this N (26) is considered sufficient for research to be informative (Lakens, 2022).",BAD,0.044992828,416,NA,NA
b15,paper_467,"In such instances, the use of quantitative measures is sometimes seen as unlikely to provide reliable results (Lakens, 2022;McCarthy, Whittaker, Boyle, & Eyal, 2017) and may also elicit data that feels impersonal and fails to give insights into the lives of individuals or groups being investigated.",discussion,10,3,3,"Several writers Berger and Lorenz (2016); Leko (2014) and Pugach (2001) have acknowledged the value of qualitative research for those working in the field of disability and inclusive education. Practitioners, such as teachers or therapists, who may wish to conduct investigations to analyse and improve their practice, are likely to be working with small samples. In such instances, the use of quantitative measures is sometimes seen as unlikely to provide reliable results (Lakens, 2022;McCarthy, Whittaker, Boyle, & Eyal, 2017) and may also elicit data that feels impersonal and fails to give insights into the lives of individuals or groups being investigated. Some researchers, Houts, Edwards, Wirth, and Deal (2016) rightly propose that the development of some quantitative techniques, such as the use of Rasch Analysis, can have benefits even when applied to samples of li mited size, but acknowledge that many quantitative measures deployed with such samples are limited. Cox (2012) recognised that many novice researchers come from a professional background and have the experience and opportunity to conduct investigations that could influence the development of practice. However, she suggested that the dominance of quantitative methods often acts as a constraint on those practitioners who might otherwise engage with research. She further argues that while policymakers appear to depend upon the numerical data more commonly associated with quantitative data when making a case for change or the implementation of new policies, the importance of understanding the specific needs of groups, such as those with disabilities, emphasises the strength of qualitative approaches. While both she and the authors of this paper readily acknowledge the value of quantitative studies and recognise that in some instances a mixed-methods approach may have greater currency, it is suggested that qualitative research is more likely to reveal the lived experiences of those for whom policymakers see the need to bring about change. This was certainly a view endorsed by those who attended the training reported in this article. Cook and Cook (2016) stress the importance of recognising qualitative research as an umbrella term that incorporates many approaches to investigation. One of the greatest advantages of qualitative research that they perceive is the ability to provide in-depth descriptions of the phenomena being investigated. These authors emphasise that qualitative research is not an alternative to other methodologies but needs to be applied in situations that benefit from the descriptive detail needed to aid understanding of a phenomenon. It is important to recognise that qualitative studies might, as in the case of the work reported in this paper, identify the effectiveness of instruction but that they cannot eliminate the fact that other methods of delivery might be equally effective. Only experimental research can determine these factors. However, the ethical challenges of conducting experimental research in these circumstances may present problems with respect to the intentions of tutors to deliver training in a manner that is equitable and does not put a group at a disadvantage.",GOOD,0.069962154,758,NA,NA
b20,paper_477,"The study chose 40 respondents due to the small population and accessibility issues caused by the absence of some HoDs (Lakens, 2022).",method,13,1,5,"The study population included academic heads of department at the University of Cape Coast. The Cross-Sectional Study design was used to examine capacity building prior to appointment in the position of department heads at a single point in time from different departments (Kesmodel, 2018). The sampling technique used in this study was simple random sampling to ensure that every respondent had an equal chance of being selected. The sample consisted of Forty (40) HoDs at the University of Cape Coast. The study chose 40 respondents due to the small population and accessibility issues caused by the absence of some HoDs (Lakens, 2022). The chosen area for the study was representative of the population, relevant to the research question, and had available and accessible data. The survey questionnaire was used to collect data for the study. Section A of the research instrument sought responses on the demographic characteristics of respondents. Section B contained issues about capacity building prior to appointment as heads of departments. The study used a concise questionnaire with five questions to meet its objectives. More complex questions were avoided to respect respondents' time, and binary answers were used for simplicity and clarity. The questionnaires were administered using the face-to-face method. One hundred per cent (100%) return rate was achieved. Data for the study were analyzed through descriptive statistics (frequency and simple percentages) using the Statistical Package for Social Sciences version 23.",BAD,0.063273356,462,NA,NA
b127,paper_488,"To determine the sensitivity power analysis (Lakens, 2022) to detect our focal effects, we conducted simulation-based sensitivity analyses following data collection, that allow for simulation of mixed-effects (logistic) regression models that capture multiple sources of random variations (Kumle et al., 2021; see Analytic Strategy and Results for further details).",method,4,2,2,"We indicated a minimum sample of 600 participants but given our interest in the between person moderators of disgust sensitivity and legal expertise, and to account for possible attrition, we aimed to recruit as large a sample as possible. To determine the sensitivity power analysis (Lakens, 2022) to detect our focal effects, we conducted simulation-based sensitivity analyses following data collection, that allow for simulation of mixed-effects (logistic) regression models that capture multiple sources of random variations (Kumle et al., 2021; see Analytic Strategy and Results for further details).",GOOD,0.029045937,299,NA,NA
b47,paper_491,"Thus, based on these prior studies, we aimed to obtain approximately 50 participants for each condition [48].",method,16,3,4,"Our analyses were based on the methods of Parsons et al [32] and Hedge et al [41]. Both studies used the same data sets, which had data from 47 (study 1) and 56 (study 2) participants for the Stroop task. In these studies, this sample size was sufficient to observe effects with medium effect sizes. Thus, based on these prior studies, we aimed to obtain approximately 50 participants for each condition [48].",GOOD,0.038383298,279,NA,NA
b69,paper_500,"To estimate sample size, G-power software (version 3.1.9.4) was used with the following statistical assumptions: a type 1 error of 5% and a minimum statistical power of 80% [70].",method,5,1,2,"High-functioning, working adults were recruited from a wide array of social and mental health services. To estimate sample size, G-power software (version 3.1.9.4) was used with the following statistical assumptions: a type 1 error of 5% and a minimum statistical power of 80% [70]. Based on previous studies, a moderate effect was expected between study variables [17,63], so the minimum sample size was estimated to be 209 respondents.",GOOD,0.028398962,282,NA,NA
b50,paper_503,"With a medium effect size of .50 for our study, a level of significance set at α = .05, and a study power set at 80%, 34 participants were required to conduct the intervention study (50).",method,11,1,2,"Statistical power analyses were used to calculate the sample size. With a medium effect size of .50 for our study, a level of significance set at α = .05, and a study power set at 80%, 34 participants were required to conduct the intervention study (50). Because this represented the minimum sample size needed, 53 WMs were asked via mail to participate in this second study. Participants were recruited for the online intervention if they met the following criteria: (1) had returned to work after maternity leave; (2) had at least one child (newborn-6 years old). These criteria are in line with the main aim of the study (i.e., to examine women's experiences of returning to work after maternity leave). It is well established in many countries that the burden of care shortly after birth and in general, is especially high for mothers compared to fathers. Furthermore, there are few resources and accessibility to high-quality childcare in Italy, making it challenging for parents to find infant care after the birth of an infant. Thirty-four of those 53 responded to the email and agreed to participate. All participants were Italian WMs living in an urban area of northern Italy. Most participants were between 31 and 40 years old (67.6%), 23.5% were over 41, and some were between 18 and 30 (8.8%). Almost all were employed in paid work (97.1%), while one was a freelancer (2.9%). The majority had an open-ended contract (91.2%), whereas the rest had a fixed-term contract (8.8%). Seventy-six point five percent of participants worked full-time, 23.5% part-time. Specifically, almost a third of the WMs (32.2%) worked as healthcare professionals (i.e., 25% nurse; 3.6% The majority of participants (92.9%) were married or cohabiting, while 7.1% were divorced or separated. Half of the WMs participating in the study had two children (50%), 35.3% had one, and 14.7% had three or more. Participants were asked who generally takes care of their child in times of need. More than a third of the participants ask their parents for help (39.3%); 32.1% ask their partner for help; 14.3% prefer to hire a babysitter; 10.8% ask for help from other family members (e.g., aunt, grandparents, in-laws); 3.6% ask for help from local services (e.g., nursery schools). All participants were duly informed that participation was anonymous and voluntary. Although 34 WMs were initially recruited, the final group consisted of 28 participants, which was due mostly to mother's own health concerns, the health of their children, or their workload (Figure 2). Because the final sample size was 28, a sensitivity power analysis was conducted to detect the effect size.",GOOD,0.073250817,798,NA,NA
b50,paper_503,"With a sample size of 28, a significance level of .05, and a desired power of .80, the effect size our study could detect is .55 (50).",method,11,2,1,"With a sample size of 28, a significance level of .05, and a desired power of .80, the effect size our study could detect is .55 (50). After providing their informed consent, participants were administered the self-report measures described below. Later, the online intervention began. The self-report questionnaires were used before (T1), and immediately after the intervention (T2). State anxiety was measured before and after each session of the intervention. Each participant was identified by an ID number to aggregate their data and preserve anonymity. All data were collected online through a Google Forms link sent by email. There were no missing data.",GOOD,0.030532693,313,NA,NA
b57,paper_514,"Also, effect size matters because a larger sample size could be needed to achieve statistical significance in situations where the effect size is small [58,59].",discussion,11,3,5,"The most frequent statistical outcome considered in research is the p-value, which is a measure that helps researchers assess the evidence against a null hypothesis. The relationship between sample size and p-values is crucial to statistical analysis [56,57]. As the sample size increases, statistical tests become more sensitive to detecting differences or effects. This increased sensitivity can be attributed to the reduction in sampling variability, leading to more precise estimates of population parameters. Also, effect size matters because a larger sample size could be needed to achieve statistical significance in situations where the effect size is small [58,59]. Another consideration regarding sample size involves the issue that a larger sample size might reduce the risk of Type II errors (false negatives), as the increased power improves the ability to detect true effects, but researchers should be cautious about the increased risk of Type I errors (false positives) when interpreting small p-values [60]. In this situation, the clinical or practical significance of the findings should be considered. Non-parametric or noncentral t-distributions could indeed be employed in case of violation of normality when dealing with few samples; however, the significance computed through a t-test is still commonly used because the Welch t-test or rank transformation before running a t-test might pose other challenges when dealing with small samples (reduced statistical power or increased false positives) [27]. The analysis sequence proposed in the current study could support researchers in identifying proteins that are not completely detected by statistics in situations where only a few data are available. A restricted number of samples not only affects p-values but also the correct identification of the confidence intervals or the reproducibility of the experimental results, which has already been observed in the literature [20,27,61,62]. However, increasing the sample size might only sometimes be possible. Working with human cells, especially primary ones, can be resource-intensive in terms of time and resources; using a smaller number of samples can help streamline experiments, making them more manageable within budgetary and time constraints [63]. With few samples, it may be easier to maintain consistent conditions across experiments and design experiments that directly test biological hypotheses without unnecessary complexity. Considering that using a limited number of samples, including three, is a common practice, new tools for analyzing and interpreting the results might help researchers support their findings. The proposed analysis pipeline evaluates donors' cell profiles without assuming any distribution. Another aspect to consider when working on data from few donors relates to the variability in the findings: biological variability between different cultures can be substantial. One could assume that with few samples, the inherent experimental variability might be reduced, making it easier to detect experimental effects. However, this might only be true sometimes, and high variability in a few sample experiments might pose challenges and affect the convertibility of the outcomes to the human population. The current study provides further evidence on a dataset under investigation because descriptive ML-based analysis on actual data might support statistical inference or help researchers focus their attention on specific results, being more careful in interpreting the outcomes.",BAD,0.073567363,782,NA,NA
b13,paper_515,"Sampling is an important step when designing an empirical study to justify the size of the sample to be collected (Lakens, 2022).",method,2,1,3,"This research was conducted in a sub-district that has good rice-fish potential with farmers who are actively cultivating rice-fish. Sampling was carried out by census, namely all rice-fish farmers in Candibinangun Village, Pakem District, as many as 35 rice-fish farmers. Sampling is an important step when designing an empirical study to justify the size of the sample to be collected (Lakens, 2022).",BAD,0.035811221,273,NA,NA
b21,paper_516,"Alternatively, researchers might have known that relying on benchmarks is problematic because determining whether an AUC is fair, good, excellent, or practically relevant depends on the context and its associated costs and benefits (Baker et al., 2014;Lakens, 2022;Panzarella et al., 2021).",discussion,14,4,3,"The fact that most research relied on statistical significance to interpret their findings might explain why many did not use AUC benchmarks. Researchers might have been unaware about such benchmarks. Alternatively, researchers might have known that relying on benchmarks is problematic because determining whether an AUC is fair, good, excellent, or practically relevant depends on the context and its associated costs and benefits (Baker et al., 2014;Lakens, 2022;Panzarella et al., 2021). For instance, if a lineup procedure has no costs (e.g., provide a minor instruction to focus on eyes before the lineup) but leads to an improved AUC (compared to the lineup without such instruction), even if relatively small, might already be good enough. 5 On the other hand, if the procedure is very costly or difficult (e.g., construct a 16 person versus 6 person fair lineup), greater differences in the ROC curves and AUC values might be warranted. However, only 15 AUC values from two studies were interpreted in light of the context of the study 6 which makes it unlikely that most researchers were aware of the importance of contextualizing their findings to understand the magnitude or meaningfulness of the results.",GOOD,0.038386316,432,NA,NA
b21,paper_516,"These findings highlight the need for researchers to conduct power analyses and make them reproducible just as their data and analyses (Lakens, 2022;Hardwicke et al., 2022).",discussion,15,7,13,"For both equivalence and minimum-effects testing but also to examine the uncertainty of an estimate, 95% CIs are crucial (Jané et al., 2024). That is, using the 95% CI, it can be established whether an observed finding is too small to care about (i.e., 95% CI is smaller than the SESOI), practically meaningful (95% CI is greater than the SESOI; Westlake, 1972), or to understand the uncertainty around the estimates (relatively large 95% CIs). In our study, less than half of the studies (= 44.1%) provided 95% CIs alongside the reported AUC values. However, when 95% CIs were reported, it was for the AUC value while 95% CIs along the ROC curves were never reported. However, it is possible to plot such 95% CIs for the entire ROC curve via ""pROC"" in R and we recommend researchers to do so (for an example see Figure 8 and R code on <https://osf.io/dgxsz/>). The visualization of the ROC curve and its 95% CIs is crucial to see whether ROC curves overlap. Specifically, if two ROC curves overlap (see Figure 3), AUC values become less informative and alternative or local (at individual thresholds) tests need to be performed (for alternative analyses see for example Moise et al., 1988). Establishing a SESOI will also improve the planning of new studies such as when conducting power analyses. That is, via power analyses researchers will be able to accurately estimate the number of participants necessary. This can inform researchers whether they have adequate funding to conduct a meaningful study or save funding by not collecting more data than necessary. Interestingly, similar to other fields (e.g., motor behavior research; McKay et al., 2022;Mesquida et al., 2023), the majority of eyewitness identification studies did not conduct power analyses. Moreover, when power analyses were conducted, they were oftentimes not reproducible. These findings highlight the need for researchers to conduct power analyses and make them reproducible just as their data and analyses (Lakens, 2022;Hardwicke et al., 2022). Although power analyses for ROC curve analyses and AUC values can be conducted via the R package ""pROC"" (Robin et al., 2011), further work is necessary to allow more contextualized power analyses using for example the SESOI (but see Mah, 2022).",GOOD,0.058479372,704,NA,NA
b11,paper_520,"Conducting studies requires a significant number of participants, making them time-consuming and reliant on expert knowledge [12].",intro,1,3,2,"User studies, while essential, suffer drawbacks for systematic testing. Conducting studies requires a significant number of participants, making them time-consuming and reliant on expert knowledge [12]. Test-retesting reliability criteria are challenging to meet [13]. Technical measurements within user studies, like measuring contact forces, present challenges with special equipment requirements [14] and potential side effects. Specifically, installing the required sensor at the contact location can cause inaccurate results or change the original behavior of the test subject.",GOOD,0.030520714,279,NA,NA
b23,paper_522,"The approach of basing the sample size on previous studies was practical when other information sources were lacking or when new manipulations were used (Althubaiti, 2023;Lakens, 2022).",method,3,3,4,"We did not estimate the sample size in prior. The sample size was determined via the results of a previous study (Yamamoto and Nakao, 2022) that compared the degree of FBI through top-down manipulations (top-down association with a virtual body through instruction). The study included 31 participants, with 27 used for analysis. The approach of basing the sample size on previous studies was practical when other information sources were lacking or when new manipulations were used (Althubaiti, 2023;Lakens, 2022). Our study's main objective involved participants' interpretation of a virtual body as their own in a negative state, which was a novel aspect. Owing to this unique aspect, we could not directly refer to effect sizes from previous studies. Therefore, we determined our sample size based on studies that conducted and compared similar manipulations.",BAD,0.056918305,362,NA,NA
b21,paper_523,"The sample size for each study was determined based on the falsifiability criteria outlined by Lakens (2022), ensuring that our methodology could reliably detect the smallest possible effect size.",method,6,1,2,"In our research exploring the guilt-mimicry dynamic, we conducted a series of six stud ies, each meticulously planned to provide insights into how different types of mimicry influence guilt perception. The sample size for each study was determined based on the falsifiability criteria outlined by Lakens (2022), ensuring that our methodology could reliably detect the smallest possible effect size. We recruited as many participants as possible within the constraints of the research panel and adhered to practical and ethical standards (Lakens & Evers, 2014). Recruitment ceased when new enrollments stopped.",BAD,0.047166272,300,NA,NA
b42,paper_525,"While comparisons between groups can already be conducted with smaller samples, an N of at least 25-30 is recommended [42,43].",discussion,25,2,1,"While comparisons between groups can already be conducted with smaller samples, an N of at least 25-30 is recommended [42,43]. Depending on the planned analyses and parameters set, the requirements can also be much higher [44]. Therefore, the data collection is continuing. In the long run, cooperation with other hospitals regarding data collection might be possible as well.",GOOD,0.026136693,257,NA,NA
b31,paper_526,"Given our sample size of 47 and the 6 predictors in our final linear regression model, our designed study showed a 90% power to detect effects (f 2 ) of at least 0.433 [32].",method,10,1,2,"We performed a post-hoc sensitivity power analysis using GPower (Version 3.1.9.2, Universität Kiel, Kiel, Germany) for linear multiple regression. Given our sample size of 47 and the 6 predictors in our final linear regression model, our designed study showed a 90% power to detect effects (f 2 ) of at least 0.433 [32].",GOOD,0.031125256,268,NA,NA
b36,paper_528,"Despite its importance, there is still no well-established routine to adequately justify sample sizes (Lakens, 2022), leading to samples that may not be sufficiently powered to detect the desired effects -in this case, an interaction between awareness and learning.",intro,1,6,4,"On another level, which is more extensively discussed in the literature, is the concept of sampling noise. When inferring a population effect through an empirical sample, the computed effect is only an estimation, which attempts to approximate the true population effect but is influenced by various uncertainties inherent in the sample. This is why participant sample size is one of the main considerations in designing experiments. Despite its importance, there is still no well-established routine to adequately justify sample sizes (Lakens, 2022), leading to samples that may not be sufficiently powered to detect the desired effects -in this case, an interaction between awareness and learning. A common argument to justify testing a specific number of participants is that the sample size is similar to previous ones in the same literature, but this does not guarantee that those sample sizes were sufficiently informative. For instance, in the domain of location probability learning, which is the main focus of the present study, median sample sizes are typically between 16 and 24 . With these samples, the minimum detectable correlation 1 with 90% power is .705 for contextual and probabilistic cuing tasks and .601 for the additional singleton task . Given that some amount of measurement noise is a very reasonable assumption in 2 the measures taken in these tasks, there are good reasons to expect that the empirical correlations involving them will often be substantially smaller than these values.",GOOD,0.040667865,459,NA,NA
b36,paper_528,"This phenomenon is commonly studied using the additional singleton task (e.g., Di Caro et al., 2019;Gao & Theeuwes, 2020, 2022;Lin et al., 2021;van Moorselaar & Theeuwes, 2021;Wang & Theeuwes, 2018a, 2018b).",intro,2,1,3,"Numerous studies have argued that our ability to guide attention is strongly influenced by unconscious learning processes that enable us to detect and exploit statistical regularities in our environment (Chun & Jiang, 1998;Chun & Turk-Browne, 2008;Ferrante et al., 2018;Geng & Berhmann, 2005;Jiang, 2018;Gaspelin & Luck, 2018;Krishnan et al., 2022;Stilwell et al., 2019;Turk-Browne et al., 2005). While much of this research is concerned with how we learn to attend to locations where a target is likely to appear, many studies have made the complementary claim that with sufficient practice, we can unconsciously learn to ignore locations in a scene where salient but irrelevant distractors typically appear. This phenomenon is commonly studied using the additional singleton task (e.g., Di Caro et al., 2019;Gao & Theeuwes, 2020, 2022;Lin et al., 2021;van Moorselaar & Theeuwes, 2021;Wang & Theeuwes, 2018a, 2018b). This increasingly popular experimental task serves as a perfect example for the present purposes.",GOOD,0.04027217,459,NA,NA
b50,paper_535,"Although no formal power analysis was undertaken, a target sample size of 50 was selected for this study as it was deemed achievable given the stroke population in the study sites and the time and resource constraints of the study (Lakens 2022).",method,13,1,1,"Although no formal power analysis was undertaken, a target sample size of 50 was selected for this study as it was deemed achievable given the stroke population in the study sites and the time and resource constraints of the study (Lakens 2022). We calculated estimates of effect size (eta-squared and adjusted R 2 ) and present 95% confidence intervals.",BAD,0.041155305,257,NA,NA
b79,paper_538,"Simultaneously, to ensure the robustness of the results and control Type II error and lack of precision for the estimates due to small sampling rate 81 , we compute the confidence intervals using bootstrapping 82 and specific implementations for non-parametric tests 83 .",annex,32,5,2,"In some cases, after performing a detailed assessment of the distribution of response variables in information treatment groups within scenarios (i.e., interactions Informationtreatment × Scenario ), and for the within- scenario pairwise contrast tests, a one-sided Mann-Whitney-Wilcoxon test can be used, with a significance level p < 0.1, for its higher power due to the particular characteristics of the compared distributions which can allow making a strong hypothesis about the values 79,80 . Simultaneously, to ensure the robustness of the results and control Type II error and lack of precision for the estimates due to small sampling rate 81, we compute the confidence intervals using bootstrapping 82 and specific implementations for non-parametric tests 83 . Hence, only differences with significant p values ( p ART-C adj BY < 0.05 or p MMW adj BY < 0.1 ) and significant confidence intervals are highlighted in Figs. 2, 3 and4.",GOOD,0.036068977,385,NA,NA
b46,paper_539,"The sample size for this study follows the heuristic principle, as it adopted the general rule provided in the literature (Lakens, 2022).",method,8,2,5,"More recently, Seitkazin (2020) concluded that the answer to whether leadership styles are distinctive in the private and public sectors might be ""yes"" or ""no"". Several aspects of EL are transferable between the private and public sectors (Heres & Lasthuizen, 2012). In addition, specific studies that observed leadership differences between public and private organisations never established the significance of the difference. For this study, a sample of public and private sector employees has implications for the broader application of the findings (Asim et al., 2021). The sample size for this study follows the heuristic principle, as it adopted the general rule provided in the literature (Lakens, 2022). For example, the sample size met the recommendation of a minimum of five participants for each item measured in a study or a minimum of 20 participants for every focal variable of a study (Rahman, 2023;Zaman et al., 2021). Hence, this study sample size was deemed sufficient, considering the investigation of three focal variables totalling 27 items. Additionally, the chosen sample size adhered to the principle that when the anticipated effect size is uncertain, a sample size suitable for a medium effect size is satisfactory; as such, a sample size of 163 provides more than 80% power to detect a relationship at a significance level of 0.05 if it exists (Althubaiti, 2023).",GOOD,0.049644625,478,NA,NA
b34,paper_546,"Our sample size was constrained by limited time and financial resources as this was part of a thesis project (Lakens, 2022).",intro,7,2,1,"Our sample size was constrained by limited time and financial resources as this was part of a thesis project (Lakens, 2022). These constraints led to a substantially smaller sample size than the preregistered N = 400. We therefore retained the full sample to increase power and thus did not implement the preregistered exclusion criteria. The main findings remain the same regardless of the exclusions. The final sample consisted of 177 participants (85 males, 82 females, two other/prefer not to answer; Mage = 26.59, SDage = 7.59). Participants reported an average of close to seven years of work experience (M = 6.83, SD = 8.51).",BAD,0.039227673,331,NA,NA
b34,paper_546,"Our sample size was constrained by the limited resources (Lakens, 2022).",discussion,18,1,5,"We preregistered our experiment on the Open Science Framework (link: <https://osf.io/c6ft4>). Participants were mainly students at a business school in Norway. To qualify, participants had to be above 18 years old and fluent in English. Participants were informed that they had the chance to win a gift card worth approximately $100. Our sample size was constrained by the limited resources (Lakens, 2022). We preregistered a target sample size of 200 participants. A total of 150 people participated in the laboratory experiment. Six participants did not complete the decision-making task, leaving us with a final sample size of 144 (73 males, 71 females, Mage = 26.17, SDage = 7.98). Participants had, on average, eight years of work experience (SD = 11.70).",BAD,0.041157306,361,NA,NA
b21,paper_547,"Since our sample size is already determined by the number of available PRP-QUANT preregistrations, we conducted sensitivity analyses for our hypothesis tests (Lakens, 2022).",method,6,5,3,"All included PRP-QUANT preregistrations will be compared to the N = 52 OSF preregistrations sampled by Bakker et al. (2020) to test hypothesis 1 (accessible at Veldkamp et al., 2020). Our sample size of N = 74 PRP-QUANT preregistrations already surpasses that of Bakker et al. (2020), which they determined through a power analysis for a Wilcoxon-Mann-Whitney test with ɑ = .05 and a power of .8 to detect a medium effect size of Cohen's d = 0.5 (which corresponds to Cliff's D of approximately 0.33, Romano et al., 2006), a difference they defined as practically meaningful between two samples of preregistrations. Since our sample size is already determined by the number of available PRP-QUANT preregistrations, we conducted sensitivity analyses for our hypothesis tests (Lakens, 2022). Figure 1A shows a sensitivity curve depicting the relationship between effect size and power for testing hypothesis 1 given our current sample sizes, which was created in R (R Core Team, 2023) based on a power simulation with 1000 repetitions that incorporated the variability in the data from Bakker et al. (2020; see R script in the supplemental material, <https://doi.org/10.23668/psycharchives.14107>). This curve suggests that we would have a power of .97 to detect small effects of d = 0.2 for the overall difference in restrictiveness between templates, employing a nested Wilcoxon-Mann-Whitney test and ɑ = .05. Meanwhile, an effect size of d = 0.5 would be detectable with a power above .99. Since the effect size found in Bakker et al. (2020) was even higher (D = 0.49, which resembles d of about 0.8, Romano et al., 2006), an effect of similar size could therefore also be detected with a high power. However, the difference between two structured templates is likely smaller than that between a structured and an unstructured template.",GOOD,0.079134893,647,NA,NA
b61,paper_548,"The sample size of current study was limited by the resources that are available (Lakens, 2022) and was comparable to the previous study on the similar topic (Morasse et al., 2022).",intro,1,8,11,"Building on previous studies, the current study aimed to examine the impact of loss-gain frame on children's knowledge and behaviors of fairness, to uncover the underlying cognitive mechanisms, and to unravel development directions by contrasting findings between children (about 10 years of age) and adults (about 20 years of age). Participants acted as dictators in a modified DG, in which they received an initial endowment of tokens and unilaterally made decisions, either for themselves or for others, to allocate some portion to an anonymous recipient. In each round, the relative gains and losses of dictators and recipients were manipulated by applying separate self and other multiplier ratios to convert tokens into payoffs for the dictators and the recipients, respectively. Leveraging the quantitative nature of the experimental design, we employed computational models to disentangle specific cognitive processes through which the framing effect influences children's fairness behaviors/knowledge. Computational modeling aims to uncover cognitive processes driving observed behavioral patterns by employing mathematically precise equations (Hu et al., 2017;Gao et al., 2018;Husain and Roiser, 2018;Yu et al., 2018). Computational models reflect the instantiations of cognitive hypotheses, with the merit of allowing for precise predictions and quantitative test of competing hypotheses (Ahn et al., 2017;Wilson and Collins, 2019). In the current study, we investigated several hypotheses on the underlying cognitive processes of explicit fairness behaviors/knowledge as well as how loss-gain frame interacts with those processes to modulate fairness decision-making. For fairness behaviors, it was hypothesized that both children and adults were averse to disadvantageous inequity, while potentially divergent preferences for advantageous inequity: children might be indifferent to advantageous inequity and even exhibit advantage-seeking preferences; in contrast, adults would be averse to advantageous inequity (Hypothesis 1). Regarding knowledge of fairness, we expected no significant differences between children and adults (Hypothesis 2). Lastly, it was hypothesized that loss (vs. gain) frame would reduce inequity aversion and increase advantage-seeking motivation among children while increasing inequity aversion among adults (Hypothesis 3). . ) participated in the current study. The sample size of current study was limited by the resources that are available (Lakens, 2022) and was comparable to the previous study on the similar topic (Morasse et al., 2022). The power sensitivity analysis was conducted with MorePower software (Campbell and Thompson, 2012). Using a 2 × 2 × 2 repeated-measures analysis of variance (ANOVA), effect size as small as η p 2 = 0.114 (RM = 2 × 2, IM = 2, α = 0.05, 1 -β = 0.80) can be reliably detected given the current sample size (Bakeman, 2005). The child participants were recruited from a primary school in Chengdu, China, and the adult participants were recruited from South China Normal University, Guangzhou, China. The Ethics Committee of South China Normal University approved the experimental design and participant recruitment. All participants were screened to ensure that they did not have any current or past neurological or psychiatric disorders, and provided informed consent before experiment.",BAD,0.08382303,863,NA,NA
b23,paper_550,"On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24].",method,7,1,7,"The study adopted a total sampling strategy, a set of purposive sampling methods that target the entire population with a particular set of characteristics. For example, the population of regular students in the School of Nursing and Midwifery is estimated at ve hundred (500); only students who consented to participate were included. However, the authors administered 300 copies of the instruments, and only 227 copies were usable for the nal analyses, representing a response rate of 75.67 per cent. The reason for purposively sampling the opinions of this particular population was further justi ed by one study which revealed that getting to a population is almost always done purposively since the researcher knows the population he is interested in and goes directly to them for such information [10]. Some other studies further justi ed the choice of adopting this sampling strategy when they emphasised that purposive sampling helps the researcher to concentrate on people with particular characteristics (which in this case is smartphone addiction and challenging academic performance) who will better be able to assist in providing relevant data and information on the subject matter [17,48]. In terms of the sample size, a renowned scholars reviewed the contemporary approach to calculating sample size with nite and non-nite populations and provided two empirical options: the n-hat method introduced in 2013 and the Multistage Non-nite Population (MNP) or n-omega method -which in either case, the minimum sample size ranges from 30 to 40 participants/counts [31]. On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24]. In addition, after an extensive review, recent studies suggested that a sample between 160 and 300 valid observations is well suited for multivariate analysis [34]. Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",BAD,0.071449615,691,NA,NA
b23,paper_550,"Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",method,7,1,9,"The study adopted a total sampling strategy, a set of purposive sampling methods that target the entire population with a particular set of characteristics. For example, the population of regular students in the School of Nursing and Midwifery is estimated at ve hundred (500); only students who consented to participate were included. However, the authors administered 300 copies of the instruments, and only 227 copies were usable for the nal analyses, representing a response rate of 75.67 per cent. The reason for purposively sampling the opinions of this particular population was further justi ed by one study which revealed that getting to a population is almost always done purposively since the researcher knows the population he is interested in and goes directly to them for such information [10]. Some other studies further justi ed the choice of adopting this sampling strategy when they emphasised that purposive sampling helps the researcher to concentrate on people with particular characteristics (which in this case is smartphone addiction and challenging academic performance) who will better be able to assist in providing relevant data and information on the subject matter [17,48]. In terms of the sample size, a renowned scholars reviewed the contemporary approach to calculating sample size with nite and non-nite populations and provided two empirical options: the n-hat method introduced in 2013 and the Multistage Non-nite Population (MNP) or n-omega method -which in either case, the minimum sample size ranges from 30 to 40 participants/counts [31]. On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24]. In addition, after an extensive review, recent studies suggested that a sample between 160 and 300 valid observations is well suited for multivariate analysis [34]. Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",BAD,0,0,NA,NA
b23,paper_550,"On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24].",method,7,1,7,"The study adopted a total sampling strategy, a set of purposive sampling methods that target the entire population with a particular set of characteristics. For example, the population of regular students in the School of Nursing and Midwifery is estimated at ve hundred (500); only students who consented to participate were included. However, the authors administered 300 copies of the instruments, and only 227 copies were usable for the nal analyses, representing a response rate of 75.67 per cent. The reason for purposively sampling the opinions of this particular population was further justi ed by one study which revealed that getting to a population is almost always done purposively since the researcher knows the population he is interested in and goes directly to them for such information [10]. Some other studies further justi ed the choice of adopting this sampling strategy when they emphasised that purposive sampling helps the researcher to concentrate on people with particular characteristics (which in this case is smartphone addiction and challenging academic performance) who will better be able to assist in providing relevant data and information on the subject matter [17,48]. In terms of the sample size, a renowned scholars reviewed the contemporary approach to calculating sample size with nite and non-nite populations and provided two empirical options: the n-hat method introduced in 2013 and the Multistage Non-nite Population (MNP) or n-omega method -which in either case, the minimum sample size ranges from 30 to 40 participants/counts [31]. On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24]. In addition, after an extensive review, recent studies suggested that a sample between 160 and 300 valid observations is well suited for multivariate analysis [34]. Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",BAD,0,0,NA,NA
b23,paper_550,"Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",method,7,1,9,"The study adopted a total sampling strategy, a set of purposive sampling methods that target the entire population with a particular set of characteristics. For example, the population of regular students in the School of Nursing and Midwifery is estimated at ve hundred (500); only students who consented to participate were included. However, the authors administered 300 copies of the instruments, and only 227 copies were usable for the nal analyses, representing a response rate of 75.67 per cent. The reason for purposively sampling the opinions of this particular population was further justi ed by one study which revealed that getting to a population is almost always done purposively since the researcher knows the population he is interested in and goes directly to them for such information [10]. Some other studies further justi ed the choice of adopting this sampling strategy when they emphasised that purposive sampling helps the researcher to concentrate on people with particular characteristics (which in this case is smartphone addiction and challenging academic performance) who will better be able to assist in providing relevant data and information on the subject matter [17,48]. In terms of the sample size, a renowned scholars reviewed the contemporary approach to calculating sample size with nite and non-nite populations and provided two empirical options: the n-hat method introduced in 2013 and the Multistage Non-nite Population (MNP) or n-omega method -which in either case, the minimum sample size ranges from 30 to 40 participants/counts [31]. On the other hand, another scholar propounded that sample size can be justi ed through any of these six approaches: i) collecting data from the entire population in question, (ii) obtaining a sample size based on resource challenges, (iii) carrying out a-priori power analysis, (iv) planning for a desired precision, (v) using heuristics, or (vi) out rightly acknowledging the absence of a justi cation [24]. In addition, after an extensive review, recent studies suggested that a sample between 160 and 300 valid observations is well suited for multivariate analysis [34]. Given this, the sample size justi cation in this study is pegged on the triangular standpoint of Memon et al. studies, who suggested between 160 and 300 samples is su cient, coupled with Louangrath study's ndings as well as Laken's' second approach in justifying sample size in a scienti c study [24,31,34].",BAD,0,0,NA,NA
b51,paper_554,"The calculated sample size in this study was based on the 'Sample Size Justification' [52] article and was calculated using G*Power 3.1.9.4 software, i.e., the sample size was calculated (post hoc) using the mean and SD (standard deviation) parameters (comparison between BC and RC was 1.57 for the proportion correct, 1.52 for the reaction time, and 1.89 for the N1 amplitude).",method,14,2,1,"The calculated sample size in this study was based on the 'Sample Size Justification' [52] article and was calculated using G*Power 3.1.9.4 software, i.e., the sample size was calculated (post hoc) using the mean and SD (standard deviation) parameters (comparison between BC and RC was 1.57 for the proportion correct, 1.52 for the reaction time, and 1.89 for the N1 amplitude). All values showed that the required sample size was up to 10 participants. In addition, this sample size is consistent with previous studies from our lab [Sterkin et al., 2012 [35] (n = 8), Siman-Tov et al., 2021 [31] (n = 10)].",BAD,0.045950172,346,NA,NA
b16,paper_556,"Power analysis is a crucial and often necessary step when planning a confirmatory study (Lakens, 2022) or preparing a pre-registration or registered report (Chambers & Tzavella, 2022).",intro,16,1,2,"In this section, we introduce how to estimate the statistical power. Power analysis is a crucial and often necessary step when planning a confirmatory study (Lakens, 2022) or preparing a pre-registration or registered report (Chambers & Tzavella, 2022). Furthermore, statistical power for complex models can only be estimated using Monte Carlo simulations.",GOOD,0.027411412,258,NA,NA
b40,paper_558,"Due to our available resources, we aimed to collect responses from 500 participants (Lakens, 2022).",method,13,1,1,"Due to our available resources, we aimed to collect responses from 500 participants (Lakens, 2022). However, we were able to collect 547 responses. Statistical power was calculated using R (version 4.1.2) (R Core Team, 2021) and the simsem package (Version 0.5-13) (Pornprasertmanit et al., 2021). The power analysis aimed to find the smallest effect size that could be found with a recommended minimum power of 90%, given our current sample size as well as the statistical method. The smallest effect size of interest (SESOI; Lakens et al., 2018) is complementary to and comparable to the alpha-level, i.e., a threshold for rejecting results. To determine our SESOI, we calculated different simulation analyses with the structure of the proposed structural equation model (SEM). While including different effect sizes in our simulation and keeping the α level (α = .05), power (90%), and sample size (N = 547) constant, we ran 1,000 replications for our model. Consequently, we set our SESOI at β = |.14| and did not interpret results with effect sizes below that threshold.",GOOD,0.039229807,438,NA,NA
b40,paper_563,"The pre-registration employed the most comprehensive OSF template developed by Bowman et al. 39 and describes the study design, data collection procedures, variables, and sample size, which was rationalised through simulation-based a-priori power analyses 40,41 : [URL REDACTED].",method,4,1,3,"We sought to increase the reproducibility and transparency of our study in response to recent calls for a ""credibility revolution"" within and beyond the social and behavioural sciences 37 . Hence, we followed best Open Science practices and pre-registered at the OSF all methodological procedures underlying the TISP project on 15 th November 2022, i.e. prior to collecting data 38 . The pre-registration employed the most comprehensive OSF template developed by Bowman et al. 39 and describes the study design, data collection procedures, variables, and sample size, which was rationalised through simulation-based a-priori power analyses 40,41 : [URL REDACTED]. This pre-registration refers to the main TISP publication 31 while we submitted three further pre-registrations for subsequent publications. The methodological procedures underlying the collection of the TISP dataset can be found in the sections Design Plan, Sampling Plan and Variables.",GOOD,1.038916561,373,NA,NA
b42,paper_564,"To answer this question, we performed a sensitivity power analysis (Lakens, 2022) estimating which effect sizes the Bayesian t-tests used can be expected to have sufficient power to detect given our sample size.",results,12,6,2,"An important question to consider is what the minimal effect sizes the tests in the current study are able to detect. To answer this question, we performed a sensitivity power analysis (Lakens, 2022) estimating which effect sizes the Bayesian t-tests used can be expected to have sufficient power to detect given our sample size. Traditionally, 'power' refers to the probability of obtaining a statistically significant result at a given significance level (often α = 0.05), given certain a nonzero true effect-size. Since we performed Bayesian t-tests rather than frequentist t-tests, 'power' instead refers to the probability of obtaining a Bayes factor larger than 3 (which is commonly regarded as ""substantial"" evidence for the alternative hypothesis). Fig. 6 shows estimates of this probability for a range of effect sizes from 0 to 2 for the two comparisons investigated using Bayesian t-tests. As can be seen, effect sizes of about 1 and larger can be expected to be detected with a probability close to unity based on this criterion, while the probability of detecting an effect size of 0.5 would be less than 0.5. Each point shown in Fig. 6 was obtained by drawing 3000 samples of random normally distributed data for the effect size in question, performing a Bayesian t-test for each random sample, and computing the proportion of cases where the Bayes factor was larger than 3. The number of participants for the each of the two groups in each comparison in the simulation was the same as in our study (see Tab. 1). The curve for the comparison between magicians (n = 53) and (male) controls is somewhat higher than the curve for the comparison between magicians and (male) actors, because there were more controls (n = 39) than actors (n = 28) in our sample. The Bayesian t-test in the simulation were performed using the Bayes Factor package (Morey et al., 2022), which is also used by JASP. The prior was the same as in the analyses we performed using JASP (Cauchy with scale parameter 0.707).",GOOD,0.05299371,621,NA,NA
b24,paper_566,"In the area of significance testing, this ritualism may lead researchers to (a) preregister analyses and demote exploratory analyses as ""tentative,"" even when significance tests retain their validity in non-preregistered, exploratory situations (Devezer et al., 2021;Rubin, 2017Rubin, , 2020a)); (b) use a conventional alpha level when an alternative unconventional alpha level is more appropriate (Lakens et al., 2018); (c) use a two-sided test when a one-sided test is more consistent with one's statistical inference (Georgiev, 2018;Rubin, 2022); (d) conduct an a priori power analysis when there is no clear basis for an effect size estimate and a sensitivity power analysis is more appropriate (Lakens, 2022;Perugini et al., 2018); and (e) follow a Neyman-Pearson interpretation when a Fisherian interpretation is more appropriate (Hurlbert & Lombardi, 2009;Rubin, 2020b).",intro,13,1,3,"In my view, statisticism refers to an overgeneralization of abstract statistical principles at the expense of context-specific nuance and caveats (e.g., Boring, 1919;Brower, 1949). Statisticism may help to explain the unthinking statistical ritualism that has been noted by some commentators (Davidson, 2018;Gigerenzer, 2004Gigerenzer,, 2018;;Proulx & Morey, 2021). In the area of significance testing, this ritualism may lead researchers to (a) preregister analyses and demote exploratory analyses as ""tentative,"" even when significance tests retain their validity in non-preregistered, exploratory situations (Devezer et al., 2021;Rubin, 2017Rubin,, 2020a)); (b) use a conventional alpha level when an alternative unconventional alpha level is more appropriate (Lakens et al., 2018); (c) use a two-sided test when a one-sided test is more consistent with one's statistical inference (Georgiev, 2018;Rubin, 2022); (d) conduct an a priori power analysis when there is no clear basis for an effect size estimate and a sensitivity power analysis is more appropriate (Lakens, 2022;Perugini et al., 2018); and (e) follow a Neyman-Pearson interpretation when a Fisherian interpretation is more appropriate (Hurlbert & Lombardi, 2009;Rubin, 2020b).",GOOD,0.044174661,511,NA,NA
b33,paper_568,"Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34].",method,3,1,8,"Thirteen French semi-elite adolescent swimmers, classified based on the criteria by Swann et al. (2015) (i.e., with achievements ranging from medalists to participants in the French Championship in their category) [33], were randomly divided into an experimental group (n = 7; females = 4, males = 3, Age = 13.9 ± 0.38, Height = 164.9 ± 3.97, Weight = 51.71 ± 5.67, YearsExperience = 5.7 ± 0.48) and a no-treatment control group (n = 6; females = 4, males = 2, Age = 13.5 ± 0.55, Height = 164.9 ± 3.9, Weight = 51.17 ± 3.76, YearsExperience = 5.5 ± 0.54). All the swimmers were in the same training group. Thus, the same academic and training program applied to all swimmers. They trained for 16 h per week (specific training = 13 h; physical preparation = 3 h). Finally, to obtain an overview of the athletes' mental health, the researchers asked whether the athletes were seeing or had seen a psychologist. None of the athletes were concerned. These data were collected orally and individually. Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34]. Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34]. Then, a priori power analysis was performed using Power IN Two-Level Designs software, which is designed to estimate standard errors of regression coefficients in hierarchical linear models for power calculations [35]. If α is chosen at 0.05, a medium effect size of 0.50 is what we expect, and a power of 0.80 is desired [36], then a sample of 13 participants along seven measurement points is required.",GOOD,0.059857285,623,NA,NA
b33,paper_568,"Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34].",method,3,1,9,"Thirteen French semi-elite adolescent swimmers, classified based on the criteria by Swann et al. (2015) (i.e., with achievements ranging from medalists to participants in the French Championship in their category) [33], were randomly divided into an experimental group (n = 7; females = 4, males = 3, Age = 13.9 ± 0.38, Height = 164.9 ± 3.97, Weight = 51.71 ± 5.67, YearsExperience = 5.7 ± 0.48) and a no-treatment control group (n = 6; females = 4, males = 2, Age = 13.5 ± 0.55, Height = 164.9 ± 3.9, Weight = 51.17 ± 3.76, YearsExperience = 5.5 ± 0.54). All the swimmers were in the same training group. Thus, the same academic and training program applied to all swimmers. They trained for 16 h per week (specific training = 13 h; physical preparation = 3 h). Finally, to obtain an overview of the athletes' mental health, the researchers asked whether the athletes were seeing or had seen a psychologist. None of the athletes were concerned. These data were collected orally and individually. Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34]. Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34]. Then, a priori power analysis was performed using Power IN Two-Level Designs software, which is designed to estimate standard errors of regression coefficients in hierarchical linear models for power calculations [35]. If α is chosen at 0.05, a medium effect size of 0.50 is what we expect, and a power of 0.80 is desired [36], then a sample of 13 participants along seven measurement points is required.",GOOD,0,0,NA,NA
b33,paper_568,"Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34].",method,3,1,8,"Thirteen French semi-elite adolescent swimmers, classified based on the criteria by Swann et al. (2015) (i.e., with achievements ranging from medalists to participants in the French Championship in their category) [33], were randomly divided into an experimental group (n = 7; females = 4, males = 3, Age = 13.9 ± 0.38, Height = 164.9 ± 3.97, Weight = 51.71 ± 5.67, YearsExperience = 5.7 ± 0.48) and a no-treatment control group (n = 6; females = 4, males = 2, Age = 13.5 ± 0.55, Height = 164.9 ± 3.9, Weight = 51.17 ± 3.76, YearsExperience = 5.5 ± 0.54). All the swimmers were in the same training group. Thus, the same academic and training program applied to all swimmers. They trained for 16 h per week (specific training = 13 h; physical preparation = 3 h). Finally, to obtain an overview of the athletes' mental health, the researchers asked whether the athletes were seeing or had seen a psychologist. None of the athletes were concerned. These data were collected orally and individually. Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34]. Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34]. Then, a priori power analysis was performed using Power IN Two-Level Designs software, which is designed to estimate standard errors of regression coefficients in hierarchical linear models for power calculations [35]. If α is chosen at 0.05, a medium effect size of 0.50 is what we expect, and a power of 0.80 is desired [36], then a sample of 13 participants along seven measurement points is required.",GOOD,0,0,NA,NA
b33,paper_568,"Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34].",method,3,1,9,"Thirteen French semi-elite adolescent swimmers, classified based on the criteria by Swann et al. (2015) (i.e., with achievements ranging from medalists to participants in the French Championship in their category) [33], were randomly divided into an experimental group (n = 7; females = 4, males = 3, Age = 13.9 ± 0.38, Height = 164.9 ± 3.97, Weight = 51.71 ± 5.67, YearsExperience = 5.7 ± 0.48) and a no-treatment control group (n = 6; females = 4, males = 2, Age = 13.5 ± 0.55, Height = 164.9 ± 3.9, Weight = 51.17 ± 3.76, YearsExperience = 5.5 ± 0.54). All the swimmers were in the same training group. Thus, the same academic and training program applied to all swimmers. They trained for 16 h per week (specific training = 13 h; physical preparation = 3 h). Finally, to obtain an overview of the athletes' mental health, the researchers asked whether the athletes were seeing or had seen a psychologist. None of the athletes were concerned. These data were collected orally and individually. Given the ecological nature of our sample, a few additional recommendations are necessary (e.g., availability of population, a priori power analysis) [34]. Since the sample encompasses the entirety of available semi-elite athletes for our study, it is inherently justified to utilize all the available data in line with the principle articulated by Lakens [34]. Then, a priori power analysis was performed using Power IN Two-Level Designs software, which is designed to estimate standard errors of regression coefficients in hierarchical linear models for power calculations [35]. If α is chosen at 0.05, a medium effect size of 0.50 is what we expect, and a power of 0.80 is desired [36], then a sample of 13 participants along seven measurement points is required.",GOOD,0,0,NA,NA
b24,paper_569,"In the area of significance testing, this ritualism may lead researchers to (a) preregister analyses and demote exploratory analyses as ""tentative,"" even when significance tests retain their validity in non-preregistered, exploratory situations (Devezer et al., 2021;Rubin, 2017Rubin, , 2020a)); (b) use a conventional alpha level when an alternative unconventional alpha level is more appropriate (Lakens et al., 2018); (c) use a two-sided test when a one-sided test is more consistent with one's statistical inference (Georgiev, 2018;Rubin, 2022); (d) conduct an a priori power analysis when there is no clear basis for an effect size estimate and a sensitivity power analysis is more appropriate (Lakens, 2022;Perugini et al., 2018); and (e) follow a Neyman-Pearson interpretation when a Fisherian interpretation is more appropriate (Hurlbert & Lombardi, 2009;Rubin, 2020b).",intro,13,1,3,"In my view, statisticism refers to an overgeneralization of abstract statistical principles at the expense of context-specific nuance and caveats (e.g., Boring, 1919;Brower, 1949). Statisticism may help to explain the unthinking statistical ritualism that has been noted by some commentators (Davidson, 2018;Gigerenzer, 2004Gigerenzer,, 2018;;Proulx & Morey, 2021). In the area of significance testing, this ritualism may lead researchers to (a) preregister analyses and demote exploratory analyses as ""tentative,"" even when significance tests retain their validity in non-preregistered, exploratory situations (Devezer et al., 2021;Rubin, 2017Rubin,, 2020a)); (b) use a conventional alpha level when an alternative unconventional alpha level is more appropriate (Lakens et al., 2018); (c) use a two-sided test when a one-sided test is more consistent with one's statistical inference (Georgiev, 2018;Rubin, 2022); (d) conduct an a priori power analysis when there is no clear basis for an effect size estimate and a sensitivity power analysis is more appropriate (Lakens, 2022;Perugini et al., 2018); and (e) follow a Neyman-Pearson interpretation when a Fisherian interpretation is more appropriate (Hurlbert & Lombardi, 2009;Rubin, 2020b).",GOOD,0,0,NA,NA
b51,paper_572,"Other recommendations focus on the coverage and diversity of respondents, which should at least be a researcher's consideration to ensure research results are more relevant, reduce potential bias, help improve accuracy in understanding population variability, and broaden the scope of respondents who need further empowerment (Johnson et al., 2020;Lakens, 2022).",discussion,12,6,13,"Another research finding is related to the results of the Andrich Threshold index in Table 5, indicating an indication of respondents' inability to distinguish the options provided in the cognitive load scale. Non-ideal spacing between ratings occurs in options 2 (Disagree) and 4 (Agree). The ideal rating criteria are met when the rating scale analysis > 1.4 logit. If the logit value of the rating < 1.4, it is recommended to merge the rating scale. If the ratings are not merged, it signifies that the 5-Likert scale on the cognitive load instrument lacks adequate discrimination and has overlapping thresholds in various items, so the response format is considered for modification (Boone & Staver, 2020;Casale et al., 2023). Figure 6, part (a), shows the graph before merging the ratings, and part (b) displays the graph after merging the ratings. After merging rating 2 with rating 1 and rating 4 with rating 5, the logit values of the ratings become ideal according to the rating scale analysis criteria. Based on the main findings outlined, the following recommendations are considered strategic steps to provide reinforcement and positive contributions to the development of the cognitive load scale within the context of the formal definition of limits. The unique characteristics of the mathematical context and learning environment introduce contextual variations in measurement outcomes, rendering research results non-generalizable (Fennell & Rowan, 2001;Wang et al., 2020). Referring to this, conducting external validation according to Ho et al. (2020) and Quintão et al. (2020) offers the possibility of generalizing results, validating contextual applicability, and identifying variability. Furthermore, expanding contextual variables is also a consideration in constructing the cognitive load scale. Rationalizing research findings can provide a more realistic depiction of material complexity, yield a more holistic analysis, and provide a comprehensive insight into factors contributing to cognitive load, determining which factors have the most significant impact (Breves & Stein, 2023;Mangaroska et al., 2022). Other recommendations focus on the coverage and diversity of respondents, which should at least be a researcher's consideration to ensure research results are more relevant, reduce potential bias, help improve accuracy in understanding population variability, and broaden the scope of respondents who need further empowerment (Johnson et al., 2020;Lakens, 2022).",GOOD,0.064673807,681,NA,NA
b25,paper_575,"Our smallest effect size of interest was a correlation of .1 (Lakens, 2022).",intro,3,1,2,"To address these limitations, here we re-examine the relation between morality and politics, using the Morality as Cooperation Questionnaire (Curry, Jones Chesters, et al., 2019)-a measure of moral values that includes a broader range of moral domains, with better psychometric properties, and with items designed to avoid confounding morality with politics-and measures of political ideology that capture both social and economic conservatism. Our smallest effect size of interest was a correlation of .1 (Lakens, 2022). We chose this value for two reasons: First, as correlations between variables can be non-zero but negligible, the proposal that moral values and political orientation are associated becomes meaningful when the associations are not negligible. Correlations of 0.1 are typically labelled small, with correlations closer to zero being considered neg ligible. Second, in a recent meta-analysis of the associations between moral values and political orientation (Kivikangas et al., 2021), the smallest average association between a moral domain and political orientation was estimated at -0.15 with a 95% CI from -0.17 to -0.12. This suggests that associations between moral values and political orientation typically have an effect size of at least r = 0.1. Consequently, we formulated and tested the following seven hypotheses: something all children need to learn"" and is included in the Authority domain. The MFQ contains no items pertaining to reciprocity.",GOOD,0.041984554,477,NA,NA
b34,paper_577,"when pre-study power calculations have not been performed (Lakens, 2022).",results,13,1,1,"when pre-study power calculations have not been performed (Lakens, 2022). The sensitivity power plot below (Figure 5) shows that a withinsubjects ANOVA with two measurements and 17 participants has reasonable power for the detection of differences with large effect sizes (e.g., 0.8 for f = 0.5).",GOOD,0.026522756,258,NA,NA
b14,paper_578,"A smallest effect size of interest (SESOI; Lakens, 2022) was used to a) estimate an appropriate sample size and to b) have a second inference criterion besides the p-value.",method,11,1,1,"A smallest effect size of interest (SESOI; Lakens, 2022) was used to a) estimate an appropriate sample size and to b) have a second inference criterion besides the p-value.",GOOD,0.024304631,226,NA,NA
b39,paper_590,"This was the minimum benchmark that we aimed for as the larger the sample size, the lower the probability of a Type II error and the higher our confidence in the research findings (see Lakens, 2022;Lakens & Evers, 2014;VanVoorhis & Morgan, 2007).",intro,7,2,3,"A power analysis was conducted using G*Power software. For a MANOVA (special effects and interaction), and to obtain an effect size of .125, 99% statistical power, and an alpha level of .05, at least 121 participants should be recruited. This was the minimum benchmark that we aimed for as the larger the sample size, the lower the probability of a Type II error and the higher our confidence in the research findings (see Lakens, 2022;Lakens & Evers, 2014;VanVoorhis & Morgan, 2007).",GOOD,0.030284287,303,NA,NA
b31,paper_593,"Mathematically, the window size should be chosen to meet the statistical requirement of PCA to prevent too small sample size [32], and the sliding interval can be selected quite flexibly.",results,8,3,1,"Mathematically, the window size should be chosen to meet the statistical requirement of PCA to prevent too small sample size [32], and the sliding interval can be selected quite flexibly. However, the selection of window size and sliding interval should be in accordance with the actual need for damage identification (e.g., how small the debonding propagation can be detected). In particular, when dealing with discontinuous damage in the structure, an excessively large window may erroneously identify the intact area as damaged area.",GOOD,0.029696714,285,NA,NA
b132,paper_597,"These analyses estimate the smallest effect size that can be statistically significant under a given family wise alpha level (5%) and sample size (Lakens, 2022).",discussion,27,12,2,"In this case, as suggested by Lakens et al. (2018), we carried out minimal statistically detectable effect analyses for the multiple regression models. These analyses estimate the smallest effect size that can be statistically significant under a given family wise alpha level (5%) and sample size (Lakens, 2022). The analyses were done in GPower software (version 3.1.9.7) (Faul et al., 2009). The description and outputs of the minimal statistically detectable effect analyses can be found in Appendix 2D. Based on the criterion for Cohen's f 2 (Cohen, 1988), the given sample size and level of power (95%), we are able to detect medium and large effects.",GOOD,0.031679368,337,NA,NA
b132,paper_597,"It is characterised by high levels of input use, animal density and productivity (Kwakman, 2021;Vellinga et al., 2011).",intro,51,1,2,"The Dutch dairy sector is among the largest dairy exporters in the world. It is characterised by high levels of input use, animal density and productivity (Kwakman, 2021;Vellinga et al., 2011). At the same time, undesirable nitrogen (N) surplus and greenhouse gas emissions generated from the production of dairy products pose intertwined environmental and related societal challenges (Jongeneel & Gonzalez-Martinez, 2021), such as soil acidification, biodiversity loss, ecosystem damages, air and water pollution, public health damages as well as climate change. In 73% of 162 Dutch nature reserves, nitrogen deposits have already exceeded ecological risk thresholds by 50% on average (Stokstad, 2019). Dutch farmers have to comply with many national directives related to nitrogen use, for instance the Nitrate Directive, the Water Framework Directive, the Birds and Habitats Directives, as well as the international treaty 'the Paris Agreement' (Wageningen University & Research, n.d.). In terms of reducing the GHG emissions, Dutch dairy farmers are currently taking voluntary mitigation measures to contribute to the national climate agreement (Rijksoverheid, 2022).",GOOD,0.038671686,429,NA,NA
b132,paper_597,"Effects can only be statistically significant if R 2 does not lie within the critical interval (Lakens, 2022).",annex,83,1,2,"We have selected the exact test family, with linear multiple regression statistical test and sensitivity power analysis to compute the minimum detectable effect size. Effects can only be statistically significant if R 2 does not lie within the critical interval (Lakens, 2022). All the R 2 s from Table 2.6, 2.7, 2.8 &2.9 are larger than the matching calculated upper critical R 2 s from Figure D1-D4. Hence, we are able to detect statistically significant effects with our sample size. ρ 2 under H1 is the squared multiple correlation coefficient. The effect size f 2 is calculated from ρ 2 as follows: f 2 = ρ 2 /(1-ρ 2 ) (Cohen, 1988). Since the ρ 2 s under H1 are 0.22, 0.19, and 0.20 and 0.17 in Figure D1-D4, the minimum detectable effect sizes for the multiple regression models in Table 2.6-2.9 can be calculated as 0.28, 0.23, 0.25, and 0.20 respectively.",GOOD,0.039208258,434,NA,NA
b24,paper_6,"Future work may extend these approaches using simulation-based or Bayesian power analysis methods, which offer flexible alternatives under complex study designs and prior-informed modeling frameworks [25,26].",discussion,9,7,8,"These findings have important implications for nutrition research, where underpowered studies remain common. The lack of formal power calculations in many published studies [6,8] reduces the credibility and generalizability of research findings. Incorporating power analysis into study planning is essential, especially as the field increasingly prioritizes reproducibility and transparent reporting. Researchers should clearly document the rationale for their sample sizes and the assumptions that support their calculations. Although G*Power was not used directly in this review, the sample size estimates were designed to reflect the outputs produced by G*Power using the standard settings. Alternative tools, such as R packages (e.g., pwr, simr) or software like PASS and SAS, can be used for more advanced or customized sample size calculations. Simulation-based methods also offer flexible solutions, particularly for complex models or non-standard assumptions, and can be implemented using platforms such as R and Python. Future work may extend these approaches using simulation-based or Bayesian power analysis methods, which offer flexible alternatives under complex study designs and prior-informed modeling frameworks [25,26]. By broadening the range of accessible tools and encouraging thoughtful planning, these strategies will ultimately improve the interpretability, reproducibility, and impact of nutritional research.",GOOD,0.038721235,431,NA,NA
b25,paper_612,"Given the target population, recruitment was resource-constrained, and we made our best efforts to recruit as many participants as possible within practical limitations [26].",method,3,2,2,"The inclusion criteria were as follows: (1) competitive male bodybuilders aged 25-60 years of any subdivision (i.e., men's physique/classic); (2) ability to provide a detailed report of dietary intake, dehydration, AASs use, nonsteroidal agents use, and training program variables during the five days pre-contest, contest day, and one day postcontest. Given the target population, recruitment was resource-constrained, and we made our best efforts to recruit as many participants as possible within practical limitations [26].",BAD,0.036182915,294,NA,NA
b25,paper_612,"Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26].",method,4,1,3,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0.044418057,508,NA,NA
b25,paper_612,"Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27].",method,4,1,4,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26].",method,4,1,5,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26].",method,4,1,3,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27].",method,4,1,4,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26].",method,4,1,5,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26].",method,4,1,3,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27].",method,4,1,4,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b25,paper_612,"Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26].",method,4,1,5,"In addition, as this is the first study initiative of this nature at the national (Brazil) level, there were no prior data to support the calculation of the sample size. Therefore, this pilot sample method was used, which consists of generating information about the population on the basis of the first results observed, aiming to obtain a reasonable estimator for the population variance. Also, a sensitivity power analysis, employed when the sample size is already known, was applied with the use of t-tests and means-the difference between two dependent means (matched pairs) [26]. Bonferroni correction was used for sensitivity power analysis with a corrected alpha level of 0.0125 (0.05/four moments), a total sample size of 10, and a power of 0.80 [26,27]. Thus, the minimal statistically detectable effect size to reflect whether a hypothesis test would yield an informative answer based on dietary recommendations for bodybuilding contest preparation of weekly weight loss (% body weight) of 1% was 1.28 Cohen's d (large effect size) [8,26]. However, considering a reduction of -2.40% (-2.12 kg) body weight between contest day and two days before the contest observed in the present study, the effect size achieved was -1.12 for Cohen's d (large effect size), with a mean of difference -2.70, an SD of difference of 2.39, a total sample size of 10 participants, an alpha level of 0.05, and a power of 0.88 [27,28].",GOOD,0,0,NA,NA
b11,paper_618,"There is a growing concern about the impact of the sample size on the ability to detect a specific effect size and control for type I and type II errors (Adam, 2020;Lakens, 2022).",discussion,24,9,1,"There is a growing concern about the impact of the sample size on the ability to detect a specific effect size and control for type I and type II errors (Adam, 2020;Lakens, 2022). This information, combined with explicitly stated inclusion and exclusion criteria and the percentage/number of people excluded, is lacking, especially in studies published in national journals.",GOOD,0.027493165,259,NA,NA
b15,paper_622,"This sample size has been determined to provide sufficient data for robust statistical analysis while ensuring the feasibility of data collection and analysis (Lakens, 2022).",method,6,2,2,"A total of 100 participants were included in this study. This sample size has been determined to provide sufficient data for robust statistical analysis while ensuring the feasibility of data collection and analysis (Lakens, 2022). In order to qualify for participation in this study, respondents are required to fulfill specific criteria ensuring the relevance and depth of their contributions. First and foremost, individuals must possess a minimum of three years of experience in their respective Local Universities and Colleges (LUCs), demonstrating a comprehensive understanding of their institutions' operations and dynamics. This criterion aims to include respondents with a substantial background that can enrich the study's insights. Additionally, participants should have encountered or been exposed to political tensions within the context of their roles, as this exposure is deemed essential for providing valuable perspectives on the study's focus on political affiliation and its impact. Lastly, respondents must hold plantilla positions within their LUCs, indicating that they occupy formal and significant roles in institutional leadership. This criterion ensures that the participants' positions are integral to the organizational structure, contributing to a more comprehensive and nuanced exploration of the study's themes.",BAD,0.045360509,405,NA,NA
b62,paper_623,"Due to the exploratory nature of the study and for pragmatic reasons (63), 40 patients will be prospectively enrolled in the study.",intro,11,1,1,"Due to the exploratory nature of the study and for pragmatic reasons (63), 40 patients will be prospectively enrolled in the study. From each included patient, three biopsies will be taken and tissue from primary diagnosis or recurrence will be analyzed. Specimen staining is envisioned using approx. 20 antibodies. This results in 80 histological sections per patient to be processed and evaluated, and a total number of up to 3200 specimens to be examined. For proteome profiling our benchmarking study has highlighted that proteome alterations (e.g. between ""responders"" and ""nonresponders"") are detectable with this sample size even in the presence of inter-individual proteome heterogeneity (39). interpretation. The authors also acknowledge support from the Open Access Publication Fund of the University of Freiburg.",BAD,0.040375033,350,NA,NA
b16,paper_627,"When sample sizes are substantially smaller, inconclusive results might have become more likely, and researchers might need to re-evaluate which research question they can adequately answer (Lakens, 2022).",intro,6,2,7,"Unforeseen events are largely outside the control of a researcher. The less control researchers have over the final sample size, the less flexibility there is in the data analysis, and therefore there is often little risk of biasing selection effects, as long as data-dependent stopping rules are avoided. Researchers typically perform the preregistered analysis on the data, even if the sample size turns out to be larger or smaller than planned. If researchers can convincingly argue that they did not engage in optional stopping or selective reporting (Simmons et al., 2011) a deviation in the preregistered sample size does not inflate the Type 1 error rate. A smaller number of observations than planned can reduce the severity of the test, but primarily because the lower statistical power for the effect of interest makes it less likely that the test will support the hypothesis if the hypothesis is true. A larger sample size than preregistered, all else equal, increases the test's severity, as the test is more likely to support the prediction when it is true. When sample sizes are substantially smaller, inconclusive results might have become more likely, and researchers might need to re-evaluate which research question they can adequately answer (Lakens, 2022). It might even make little sense to perform a hypothesis test if the statistical power is very low.",GOOD,0.057852086,453,NA,NA
b26,paper_630,"Existen al menos seis maneras de poder justificar el tamaño de la muestra (Lakens, 2022); sin embargo, la de mayor uso es la de representatividad.",intro,3,1,3,"Desde el reconocimiento de la crisis de confiabilidad (Hartgerink et al., 2017;Ioannidis, 2005), se han hecho esfuerzos para mejorar los métodos usuales que determinan el tamaño de la muestra (Simmons et al., 2011) logramos dos cosas. Primero, mostramos que a pesar del respaldo nominal de los psicólogos empíricos a una baja tasa de hallazgos falsos positivos (≤ .05. Existen al menos seis maneras de poder justificar el tamaño de la muestra (Lakens, 2022); sin embargo, la de mayor uso es la de representatividad. Desde esta perspectiva, se considera que la forma clásica para muestreos aleatorios simples, que cabe aclarar que existen diferentes aproximaciones en dependencia del tipo de muestreo, es mediante la siguiente ecuación: La anterior forma tiene un uso central en la epidemiología (Milton, 2001;Polit, 2002) debido a que busca una sensibilidad suficiente para poder efectuar una prueba Z diseñada para muestras mayores de 130 sujetos. No obstante, esta aproximación es poco sensible en estudios observacionales o diseños experimentales en los que la población a estudiar suele ser pequeña o se tiene acceso a pocos sujetos en los que se objetiva el fenómeno. Por ejemplo, con esta aproximación, mientras menor sea la población, mayor es el número de la muestra; inversamente, si se tiene mayor población, menor será la muestra para representarla, es decir, si se tienen 100 sujetos de población, la muestra debe ser de 80, pero si se tiene una población de 1,000, la muestra tendrá que ser de 278, o si son 10,000, será de 370. Conforme la población vaya aumentando, la muestra se estabilizará, de tal modo que si ahora se tiene una población de 100,000, se necesitarán tan solo 383 en la muestra, o si son 1'000,000, será de 385 fijo por más que se incremente la población. Por ende, es poco realista que algún investigador que estudie fenómenos poco típicos, enfermedades raras o que cuente con recursos limitados pueda utilizar esta fórmula para justificar su n. Por ello, sugerimos utilizar el AP, que no es una propuesta novedosa, pero ha sido poco utilizada. Existen tres formas comunes de usarlo: el primero es denominado a priori; el segundo, calculando la sensibilidad; y el tercero es hacer un post-hoc. Con base en el objetivo de nuestro trabajo, solo profundizaremos en el primero, ya que es el que permite calcular la n en función de un δ de interés en particular para un fenómeno específico. A continuación, mostramos una explicación gráfica a manera de guía utilizando el programa Jamovi 2.3.2 (2022) de libre acceso. Primero, tras abrir el programa, se selecciona el botón de módulos ubicado en la esquina superior derecha y se da clic en ""Biblioteca Jamovi"". Luego, se instala el módulo ""jpower"".",GOOD,0.081396249,915,NA,NA
b26,paper_632,"For guidance on how to hypothesize effect sizes, readers are referred to Durlak (2009) and Lakens (2022), for example.",intro,7,1,3,"This section offers guidelines for selecting the proportion of true null hypotheses and the conditional error rates. Once the classical error rates have been specified using the conditional justification, the sample size still needs to be determined for the hypothesized effect size. For guidance on how to hypothesize effect sizes, readers are referred to Durlak (2009) and Lakens (2022), for example. The summary of the guidelines and the procedure of conditional justification are provided in Table 3.",GOOD,0.027670718,279,NA,NA
b63,paper_635,"Given the budget for this study, we were able to gather responses from 450 participants [64].",method,15,1,1,"Given the budget for this study, we were able to gather responses from 450 participants [64].",BAD,0.239433517,204,NA,NA
b22,paper_638,"As Lakens (2022) recently pointed out, there are many power analysis tools available, but learning to use them effectively takes time.",intro,1,2,4,"B Felix Zimmer felix.zimmer@uzh.ch 1 Psychological Methods, Evaluation and Statistics, Department of Psychology, University of Zurich, Binzmuehlestrasse 14, Box 27,8050 Zurich,Switzerland To address the challenges in finding a cost-efficient sample size while maintaining high statistical power, researchers can utilize power analysis tools to optimize their study designs. The mlpwr package provides a means to perform simulation-based power analysis for a broad class of applications (Zimmer and Debelak, in press). It fills two previously existing gaps in the literature by allowing for user-defined scenarios with multiple design parameters and explicitly accounting for the cost of study designs during the search algorithm. As Lakens (2022) recently pointed out, there are many power analysis tools available, but learning to use them effectively takes time. In response to this issue, we provide an introduction to the background and the application of the mlpwr package.",GOOD,0.042161162,375,NA,NA
b22,paper_638,"Although the sample size is still often only stated but not justified (Lakens, 2022), stating and justifying a sample size before conducting a study is arguably becoming common practice.",intro,3,1,10,"The recent replication crisis has put low statistical power and replicability of scientific research into focus (Open Science Collaboration, 2015; Button et al., 2013). Starting from the observation that most published research results might be wrong (Ioannidis, 2005;Simmons et al., 2011), there have been several developments to improve the replicability of scientific studies (Shrout and Rodgers, 2018). One of these are registered reports, in which research projects are reviewed and conditionally accepted based on sound methodology rather than on the statistical significance of the result. In registered reports, justification of sample size based on power analysis is usually mandatory. For example, in the journal Nature Human Behaviour, the sample size should be large enough to achieve at least 95% statistical power (Nature Human Behaviour, 2022). Looking at recent developments, registered reports are indeed accompanied by more frequent justification of sample size (Soderberg et al., 2021). Another means to ensure replicability is through pre-registrations, where key properties of the planned research are fixed before data collection and statistical analyses. A study by Bakker et al. (2020) found that pre-registered studies had larger sample sizes than earlier psychological studies. However, the study did not find that explicit recommendations for performing power analysis led to larger sample sizes. Although the sample size is still often only stated but not justified (Lakens, 2022), stating and justifying a sample size before conducting a study is arguably becoming common practice.",GOOD,0.05008965,495,NA,NA
b22,paper_638,"It is generally fast but sometimes unavailable, in particular for more complex or uncommon statistical hypothesis tests (Lakens, 2022).",intro,6,1,4,"Before we present our implementation to find optimal designs in these scenarios, we want to give an overview over power analysis methodology and implementations. Methodologically, two approaches can be distinguished for determining the power of a study. One is the analytical or formula-based approach. It is generally fast but sometimes unavailable, in particular for more complex or uncommon statistical hypothesis tests (Lakens, 2022). An alternative approach with higher availability but higher computational effort is the simulation-based approach. Both analytical and simulationbased approaches can be used in the presence of multiple design parameters.",GOOD,0.073821143,294,NA,NA
b22,paper_638,"For this reason, a common challenge is that analytical solutions are unavailable (Lakens, 2022).",intro,7,3,5,"The speed of analytical approaches makes them the first choice for simpler models. However, a slight change in the study design and the hypothesis test in question may require a different analytical treatment. This is because it can be difficult or even impossible to derive analytical formulas for more complex models (such as determining the power to test a random effect in a multilevel model, Cools et al. 2008). For this reason, a common challenge is that analytical solutions are unavailable (Lakens, 2022).",GOOD,0.034905745,286,NA,NA
b22,paper_638,"An important prerequisite for generating data and planning a study design in general is the determination of the expected size of the effect to be studied (Lakens, 2022).",method,16,3,3,"In most cases, the test of the hypothesis is more straightforward for applied researchers, as it is a standard use case of R packages and taught in many statistics courses. Generation of artificial data is however less often practiced and may be unfamiliar to applied researchers. An important prerequisite for generating data and planning a study design in general is the determination of the expected size of the effect to be studied (Lakens, 2022). There are many definitions of effect size that depend heavily on the statistical model used (e.g., Brysbaert and Stevens 2018;Lorah 2018;Chalmers 2022).",GOOD,0.029677543,311,NA,NA
b26,paper_643,"However, the more diverse the countries studied are, the more credible the results will be due to the heterogeneity and robustness of the samples [25][26][27].",discussion,41,2,4,"Could this result be linked to the size of these countries, which do not have enough land for UA? Or could it be because they do not practice UA at all? Another possibility is that these countries may be applying modern urbanism extensively, thinking that UA is like bringing the village back to the city. However, the more diverse the countries studied are, the more credible the results will be due to the heterogeneity and robustness of the samples [25][26][27].",GOOD,0.028985185,280,NA,NA
b86,paper_651,"For instance, it may be used for power analyses when no better alternative to estimate the effect size is present (Lakens, 2022), and for evaluating the relative importance of the effect size of single predictors.",results,17,2,5,"On average, the knot is located at R 2 = .051, suggesting that ""useful"" predictors tend to (univariately) explain at least 5.1% of the variance in SVO, corresponding to a Pearson's correlation of r = .23. 8 Thus, it seems that for a predictor to be important in the overall picture it must as a rule-of-thumb attain a Pearson's correlation of at least r = .20. Curiously, this result coincides with the mean effect size in 20 th century social and personality psychology (r = .21; Richard et al., 2003; see also Gignac & Szodorai, 2016). We suggest that this value also serves as a benchmark for researchers interested in the relation between personality and SVO. For instance, it may be used for power analyses when no better alternative to estimate the effect size is present (Lakens, 2022), and for evaluating the relative importance of the effect size of single predictors.",GOOD,0.039998142,392,NA,NA
b86,paper_651,"Fourth, in the service of identifying relevant predictors, our contribution can inform power analysis decisions (Lakens, 2022): Because important traits tend to attain correlations of at least r = .20, sample size calculations should be set to detect effects of at least r = .20.",discussion,23,2,10,"Our results can be of service to the field in multiple ways-methodologically, theoretically, and in application. First, the 13.9% benchmark can assist in model evaluation, that is, it will help researchers to answer the elusive question ""how good is my model?"". The benchmark also has implications for how much we emphasize personality traits in theoretical models and ascribe relevance to them in applied settings: It tells us that in a multivariate perspective, personality is substantially more predictive of behavior than is implied by single correlations (cf. Thielmann et al., 2020). In fact, the best single-trait model explained 9.3% of the variance in SVO-almost five percentage points less than the best multivariate model. Second, our findings regarding the adequacy of simple models (i.e., linear models, Pearson's correlation) support the mainstream statistical practice to consider linear effects of personality traits rather than nonlinearities and trait-trait interactions. Third, the finding that narrower traits out-predict basic personality should motivate researchers to map out more nuanced aspects of personality to complement basic personality dimensions in the prediction of prosocial behavior. In fact, our results suggest that the way toward better predictive models lies in improving our predictors (e.g., considering personality nuances; Seeboth & Mõttus, 2018) more so than in increasing the complexity of our statistical modeling practices. Importantly, however, in taking such a way forward, researchers should prevent introducing ever more construct inflation (Thielmann, Hilbig, & Zettler, 2022). Fourth, in the service of identifying relevant predictors, our contribution can inform power analysis decisions (Lakens, 2022): Because important traits tend to attain correlations of at least r = .20, sample size calculations should be set to detect effects of at least r = .20. That said, there are notable exceptions to this rule (e.g., right-wing authoritarianism;",GOOD,0.056931971,579,NA,NA
b20,paper_655,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022).",intro,3,1,1,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022). Statistical power, the probability of correctly rejecting the null hypothesis if there is a true effect, is a function of study design, hypothesized effect size, sample size, and alpha level. Sta`s`cal power increases as either the sample size or the hypothesized effect size increases. When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022). Inaccurate justification of the effect size parameter in a priori power analysis can lead to studies that cannot reliably detect a wide range of plausible effect sizes (Brydges, 2019). For example, if a researcher expects to find a 'medium-sized' effect, they might plan for a one-tailed paired-samples t-test using Cohen's suggestion for a medium effect size (d = 0.5) with 80% power, which would require a sample size of N = 27. However, if the 'true' medium effect size (i.e., the 50 th percentile effect size) for their specific field is d = 0.4, the study would only have 64.8% power, meaning that there would be a considerable chance of not detecting an effect even though it exists.",GOOD,0.046230451,525,NA,NA
b20,paper_655,"When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022).",intro,3,1,4,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022). Statistical power, the probability of correctly rejecting the null hypothesis if there is a true effect, is a function of study design, hypothesized effect size, sample size, and alpha level. Sta`s`cal power increases as either the sample size or the hypothesized effect size increases. When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022). Inaccurate justification of the effect size parameter in a priori power analysis can lead to studies that cannot reliably detect a wide range of plausible effect sizes (Brydges, 2019). For example, if a researcher expects to find a 'medium-sized' effect, they might plan for a one-tailed paired-samples t-test using Cohen's suggestion for a medium effect size (d = 0.5) with 80% power, which would require a sample size of N = 27. However, if the 'true' medium effect size (i.e., the 50 th percentile effect size) for their specific field is d = 0.4, the study would only have 64.8% power, meaning that there would be a considerable chance of not detecting an effect even though it exists.",GOOD,0,0,NA,NA
b20,paper_655,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022).",intro,3,1,1,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022). Statistical power, the probability of correctly rejecting the null hypothesis if there is a true effect, is a function of study design, hypothesized effect size, sample size, and alpha level. Sta`s`cal power increases as either the sample size or the hypothesized effect size increases. When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022). Inaccurate justification of the effect size parameter in a priori power analysis can lead to studies that cannot reliably detect a wide range of plausible effect sizes (Brydges, 2019). For example, if a researcher expects to find a 'medium-sized' effect, they might plan for a one-tailed paired-samples t-test using Cohen's suggestion for a medium effect size (d = 0.5) with 80% power, which would require a sample size of N = 27. However, if the 'true' medium effect size (i.e., the 50 th percentile effect size) for their specific field is d = 0.4, the study would only have 64.8% power, meaning that there would be a considerable chance of not detecting an effect even though it exists.",GOOD,0,0,NA,NA
b20,paper_655,"When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022).",intro,3,1,4,"Given this variability of effect size benchmarks across different fields, the use of Cohen's general effect size heuristics can lead to potential misinterpretations of observed effect sizes, and, in a priori power analysis, this can lead to study test and design combinations that are statistically underpowered to reliably answer specific research questions (Lakens, 2022). Statistical power, the probability of correctly rejecting the null hypothesis if there is a true effect, is a function of study design, hypothesized effect size, sample size, and alpha level. Sta`s`cal power increases as either the sample size or the hypothesized effect size increases. When designing a study, researchers should decide on a sample size that ensures sufficient statistical power, which is typically 80%, although some researchers suggest 90%-95% power instead (e.g., Lakens, 2022). Inaccurate justification of the effect size parameter in a priori power analysis can lead to studies that cannot reliably detect a wide range of plausible effect sizes (Brydges, 2019). For example, if a researcher expects to find a 'medium-sized' effect, they might plan for a one-tailed paired-samples t-test using Cohen's suggestion for a medium effect size (d = 0.5) with 80% power, which would require a sample size of N = 27. However, if the 'true' medium effect size (i.e., the 50 th percentile effect size) for their specific field is d = 0.4, the study would only have 64.8% power, meaning that there would be a considerable chance of not detecting an effect even though it exists.",GOOD,0,0,NA,NA
b20,paper_655,"A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022).",intro,3,2,6,"There are several ways in which effect sizes can be selected more accurately when performing an a priori power analysis. Cohen (1988) originally proposed that the effect size parameter in a priori power analysis should be treated as an expected population effect size. However, since the population effect size is almost always unknown, authors have more recently suggested that researchers should treat this parameter as a smallest effect size of interest (SESOI) instead (Lakens & Evers, 2014). With this approach, the researcher decides on the smallest effect size that is deemed 'worthwhile' and designs a study that is adequately powered to detect this effect size. Consequently, the power level is sufficiently high to detect this smallest effect size of interest, and power only increases for larger effect sizes. A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022). This approach is useful when data is already collected, or there is a known limit to the number of participants that can be tested, for example due to resource constraints. In this case, the goal is to gain insight into what range of effect sizes can be detected for a range of acceptable power levels. Consequently, the researcher aims to strike a balance between which effect sizes are meaningful to detect or reject and with what level of accuracy the researcher wishes to detect or reject these effects. However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",GOOD,0.060276878,522,NA,NA
b20,paper_655,"However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",intro,3,2,10,"There are several ways in which effect sizes can be selected more accurately when performing an a priori power analysis. Cohen (1988) originally proposed that the effect size parameter in a priori power analysis should be treated as an expected population effect size. However, since the population effect size is almost always unknown, authors have more recently suggested that researchers should treat this parameter as a smallest effect size of interest (SESOI) instead (Lakens & Evers, 2014). With this approach, the researcher decides on the smallest effect size that is deemed 'worthwhile' and designs a study that is adequately powered to detect this effect size. Consequently, the power level is sufficiently high to detect this smallest effect size of interest, and power only increases for larger effect sizes. A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022). This approach is useful when data is already collected, or there is a known limit to the number of participants that can be tested, for example due to resource constraints. In this case, the goal is to gain insight into what range of effect sizes can be detected for a range of acceptable power levels. Consequently, the researcher aims to strike a balance between which effect sizes are meaningful to detect or reject and with what level of accuracy the researcher wishes to detect or reject these effects. However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",GOOD,0,0,NA,NA
b20,paper_655,"A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022).",intro,3,2,6,"There are several ways in which effect sizes can be selected more accurately when performing an a priori power analysis. Cohen (1988) originally proposed that the effect size parameter in a priori power analysis should be treated as an expected population effect size. However, since the population effect size is almost always unknown, authors have more recently suggested that researchers should treat this parameter as a smallest effect size of interest (SESOI) instead (Lakens & Evers, 2014). With this approach, the researcher decides on the smallest effect size that is deemed 'worthwhile' and designs a study that is adequately powered to detect this effect size. Consequently, the power level is sufficiently high to detect this smallest effect size of interest, and power only increases for larger effect sizes. A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022). This approach is useful when data is already collected, or there is a known limit to the number of participants that can be tested, for example due to resource constraints. In this case, the goal is to gain insight into what range of effect sizes can be detected for a range of acceptable power levels. Consequently, the researcher aims to strike a balance between which effect sizes are meaningful to detect or reject and with what level of accuracy the researcher wishes to detect or reject these effects. However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",GOOD,0,0,NA,NA
b20,paper_655,"However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",intro,3,2,10,"There are several ways in which effect sizes can be selected more accurately when performing an a priori power analysis. Cohen (1988) originally proposed that the effect size parameter in a priori power analysis should be treated as an expected population effect size. However, since the population effect size is almost always unknown, authors have more recently suggested that researchers should treat this parameter as a smallest effect size of interest (SESOI) instead (Lakens & Evers, 2014). With this approach, the researcher decides on the smallest effect size that is deemed 'worthwhile' and designs a study that is adequately powered to detect this effect size. Consequently, the power level is sufficiently high to detect this smallest effect size of interest, and power only increases for larger effect sizes. A different, yet related, approach is sensitivity power analysis, where the researcher uses a given sample size as a starting point instead of an effect size of interest (Lakens, 2022). This approach is useful when data is already collected, or there is a known limit to the number of participants that can be tested, for example due to resource constraints. In this case, the goal is to gain insight into what range of effect sizes can be detected for a range of acceptable power levels. Consequently, the researcher aims to strike a balance between which effect sizes are meaningful to detect or reject and with what level of accuracy the researcher wishes to detect or reject these effects. However, even with these more sensible approaches to power, researchers need to have a good understanding of what effect sizes are interesting for their research question in order to justify the effect size parameter in their power analysis (Lakens, 2022).",GOOD,0,0,NA,NA
b20,paper_655,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022).",intro,5,1,1,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022). Publica`on bias refers to the phenomenon where studies with sta`s`cally significant results are more likely to be published than studies without a significant result, which, in turn, leads to an overall infla`on of effect sizes (Kvarven et al., 2020;Schäfer & Schwarz, 2019). This is an issue that mainly affects small studies (Dwan et al., 2008). Non-significant effects in small studies are oqen apributed to low power and are therefore deemed inconclusive or uninforma`ve (Evangelou et al., 2012). The publica`on of small sample studies with non-significant results is decided against (either by journal editorial teams or the authors not submirng the study for publica`on in the first place) more frequently than large studies with non-significant results, since non-significant results in studies with a large sample size are more difficult to explain on the basis of low power (Bupon et al., 2013). Meanwhile, small studies that do find effects are more likely to be published, despite the higher likelihood that they greatly overes`mate the true effect size (Rochefort-Maranda, 2021). Aside from publica`on bias, small studies are more sensi`ve to other biases, such as vibra`on effects (i.e., the variability in a study's effect size es`mate based on the selec`on of analysis method), selec`ve data analysis and selec`ve repor`ng, and reduced quality of other aspects of the study design (Bupon et al., 2013). Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022). Therefore, it is essen`al to address these biases when interpre`ng individual study results or planning a new study using an ESD.",GOOD,0.062598714,648,NA,NA
b20,paper_655,"Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022).",intro,5,1,8,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022). Publica`on bias refers to the phenomenon where studies with sta`s`cally significant results are more likely to be published than studies without a significant result, which, in turn, leads to an overall infla`on of effect sizes (Kvarven et al., 2020;Schäfer & Schwarz, 2019). This is an issue that mainly affects small studies (Dwan et al., 2008). Non-significant effects in small studies are oqen apributed to low power and are therefore deemed inconclusive or uninforma`ve (Evangelou et al., 2012). The publica`on of small sample studies with non-significant results is decided against (either by journal editorial teams or the authors not submirng the study for publica`on in the first place) more frequently than large studies with non-significant results, since non-significant results in studies with a large sample size are more difficult to explain on the basis of low power (Bupon et al., 2013). Meanwhile, small studies that do find effects are more likely to be published, despite the higher likelihood that they greatly overes`mate the true effect size (Rochefort-Maranda, 2021). Aside from publica`on bias, small studies are more sensi`ve to other biases, such as vibra`on effects (i.e., the variability in a study's effect size es`mate based on the selec`on of analysis method), selec`ve data analysis and selec`ve repor`ng, and reduced quality of other aspects of the study design (Bupon et al., 2013). Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022). Therefore, it is essen`al to address these biases when interpre`ng individual study results or planning a new study using an ESD.",GOOD,0,0,NA,NA
b20,paper_655,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022).",intro,5,1,1,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022). Publica`on bias refers to the phenomenon where studies with sta`s`cally significant results are more likely to be published than studies without a significant result, which, in turn, leads to an overall infla`on of effect sizes (Kvarven et al., 2020;Schäfer & Schwarz, 2019). This is an issue that mainly affects small studies (Dwan et al., 2008). Non-significant effects in small studies are oqen apributed to low power and are therefore deemed inconclusive or uninforma`ve (Evangelou et al., 2012). The publica`on of small sample studies with non-significant results is decided against (either by journal editorial teams or the authors not submirng the study for publica`on in the first place) more frequently than large studies with non-significant results, since non-significant results in studies with a large sample size are more difficult to explain on the basis of low power (Bupon et al., 2013). Meanwhile, small studies that do find effects are more likely to be published, despite the higher likelihood that they greatly overes`mate the true effect size (Rochefort-Maranda, 2021). Aside from publica`on bias, small studies are more sensi`ve to other biases, such as vibra`on effects (i.e., the variability in a study's effect size es`mate based on the selec`on of analysis method), selec`ve data analysis and selec`ve repor`ng, and reduced quality of other aspects of the study design (Bupon et al., 2013). Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022). Therefore, it is essen`al to address these biases when interpre`ng individual study results or planning a new study using an ESD.",GOOD,0,0,NA,NA
b20,paper_655,"Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022).",intro,5,1,8,"Because field-specific ESDs largely consist of effect sizes reported in published studies, it is likely that they are influenced by publica`on bias (Bupon et al., 2013;Lakens, 2022). Publica`on bias refers to the phenomenon where studies with sta`s`cally significant results are more likely to be published than studies without a significant result, which, in turn, leads to an overall infla`on of effect sizes (Kvarven et al., 2020;Schäfer & Schwarz, 2019). This is an issue that mainly affects small studies (Dwan et al., 2008). Non-significant effects in small studies are oqen apributed to low power and are therefore deemed inconclusive or uninforma`ve (Evangelou et al., 2012). The publica`on of small sample studies with non-significant results is decided against (either by journal editorial teams or the authors not submirng the study for publica`on in the first place) more frequently than large studies with non-significant results, since non-significant results in studies with a large sample size are more difficult to explain on the basis of low power (Bupon et al., 2013). Meanwhile, small studies that do find effects are more likely to be published, despite the higher likelihood that they greatly overes`mate the true effect size (Rochefort-Maranda, 2021). Aside from publica`on bias, small studies are more sensi`ve to other biases, such as vibra`on effects (i.e., the variability in a study's effect size es`mate based on the selec`on of analysis method), selec`ve data analysis and selec`ve repor`ng, and reduced quality of other aspects of the study design (Bupon et al., 2013). Given these various biases, any es`mate of the popula`on effect size that is based on previous literature is likely to be an overes`ma`on in most cases (Lakens, 2022). Therefore, it is essen`al to address these biases when interpre`ng individual study results or planning a new study using an ESD.",GOOD,0,0,NA,NA
b20,paper_655,"Publication bias is a pressing issue in the psychological sciences, in which some studies are more likely to be published than others (Bupon et al., 2013;Lakens, 2022).",intro,17,4,1,"Publication bias is a pressing issue in the psychological sciences, in which some studies are more likely to be published than others (Bupon et al., 2013;Lakens, 2022). Consequently, effect size estimates drawn from empirical studies may be inflated. The ESDist package allows users to account for publication bias by incorporating limit meta-analysis (Rücker et al., 2011;Schwarzer et al., 2015).",GOOD,0.027991143,275,NA,NA
b81,paper_663,"The respondents were selected using a nonprobabilistic sampling technique called Stratified Purposive Sampling [79][80] to focus on cases that are rich in desired information and can accommodate the heterogeneity of the research population (certification stakeholders), which is divided into different groups [81].",method,13,1,3,"A total of 137 participants were involved, including 65 issuers, 67 certificates, and 5 regulators. Cochran's formula for the unknown population and its adjustment formula for the finite population were used to determine the sample size for issuers and recruiters, whereas, for regulators, a complete enumeration was used [77][78]. The respondents were selected using a nonprobabilistic sampling technique called Stratified Purposive Sampling [79][80] to focus on cases that are rich in desired information and can accommodate the heterogeneity of the research population (certification stakeholders), which is divided into different groups [81].",GOOD,0.030011328,310,NA,NA
b46,paper_676,"The convergence of the estimate across the two sources of evidence in such a broad range of conditions can aid developmental scientists in sample size calculations for future studies of IDS preference (Lakens, 2022).",discussion,22,1,5,"Our main finding is that the IDS preference effect generalizes across relevant study dimensions in both the MA and MLR. The moderated models showed convergent results for IDS preference, with infants showing a general preference to attend to IDS over ADS stimuli during early development across a wide variety of ages, task contexts and linguistic backgrounds. This analysis thus conformed to previous studies showing that the unique properties of IDS robustly captivate infants' attention from an early point in development (Cooper & Aslin, 1990;Pegg et al., 1992;Werker & McLeod, 1989;Fernald & Kuhl, 1987). The size of the IDS preference was also remarkably similar between the MA and the MLR: both data sources converged on an average effect size estimate of d ≈ 0.35. The convergence of the estimate across the two sources of evidence in such a broad range of conditions can aid developmental scientists in sample size calculations for future studies of IDS preference (Lakens, 2022). For example, the effect size estimate of d = 0.35 implies that a sample at least as large as N = 66 infants is needed to have 80% or greater power to detect an IDS preference at an alpha level of .05 in a within-participant design using a paired-samples t-test. Our full dataset is also openly available, allowing researchers to account for potential sources of variability and tune their power estimates to specific methodological and modeling choices. Why does IDS exert such an early, widespread effect on infants' preferential attention? One promising explanation posits that the engaging features of IDS reside in the mutual feedback loops between infant and caregiver, where infants' active participation and caregiver responsiveness both contribute to the developmental process (Warlaumont et al., 2014;Ko et al., 2016). Given that adults use IDS as a consistent signal in addressing children during development, infants may start to associate the acoustic features of IDS with relevance and to recognize themselves as recipients of these salient utterances (Nencheva et al., 2021). This elevated attention to the speech stream, in turn, may drive the commonly observed language benefits of IDS during development (Golinkoff et al., 2015;Hartman et al., 2017;Peter et al., 2016).",GOOD,0.056148723,662,NA,NA
b34,paper_678,"Since the sample size was restricted mainly by our financial resources and time (Lakens, 2022), we sent an email to the pool of 66 participants who completed the screening study, informing them that the first 31 participants who confirmed their interest in participating in the fMRI study would be invited to the neuroimaging laboratory.",method,7,2,3,"Participants were asked to rate 20 items (e.g., ""It is difficult to hurt my feelings"") using a 4-point scale (from 1-""disagree completely"" to 4-""fully agree""). There were no outliers in terms of the individualdifferences scores. Since the sample size was restricted mainly by our financial resources and time (Lakens, 2022), we sent an email to the pool of 66 participants who completed the screening study, informing them that the first 31 participants who confirmed their interest in participating in the fMRI study would be invited to the neuroimaging laboratory. There were no other exclusion criteria. A total of 31 right-handed volunteers (20 females; M age = 26.5, SD age = 6.2) from the community sample were selected for the fMRI study. All participants reported no neurological or psychiatric disorders and gave informed consent before the study. They were informed about the general design of the task and that they could withdraw at any time without any consequences. Each participant received financial compensation of 100 Polish zlotys; PLN (approximately $25). Two participants were excluded from further analyses because of scanner failure and one participant decided to withdraw from the study. The procedure was approved by the ethical committee at SWPS University.",BAD,1.073956482,446,NA,NA
b45,paper_686,"We also conducted a posteriori sensitivity power analysis (Lakens, 2022) using G*Power 3.1 (Faul et al., 2007) according to which given N = 34, α = 0.05 and a power = 80% a minimum partial equal to η 2 = 0.1968 was found, consistent with the literature on this topic.",method,3,2,4,"Three participants were excluded from the analyses: the first due to a recording error of the experimenter, the second started to perform the task before the recording was initiated, and the last needed to be excluded since the experiment was interrupted by an external issue. Therefore, the statistical analyses were performed on 34 female subjects (age range:19-51 years; mean age = 24.65; SD = 7.21). The sample size was established based on heuristic evaluations of the literature on affective priming, which reports numerous studies with samples of 22-33 subjects (Yamada and Decety, 2009;Wu et al., 2021). We also conducted a posteriori sensitivity power analysis (Lakens, 2022) using G*Power 3.1 (Faul et al., 2007) according to which given N = 34, α = 0.05 and a power = 80% a minimum partial equal to η 2 = 0.1968 was found, consistent with the literature on this topic.",GOOD,0.037500162,404,NA,NA
b31,paper_697,"Given the possibility of incomplete responses, the uncertainty around the expected effect size and the possibility that the effect may be smaller than the estimate, the study aimed to collect as large a sample as time and resources permitted (Lakens, 2022) and specified a 5-week data collection period.",method,5,5,2,"The power analysis indicated that a minimum sample of 234 participants was required. Given the possibility of incomplete responses, the uncertainty around the expected effect size and the possibility that the effect may be smaller than the estimate, the study aimed to collect as large a sample as time and resources permitted (Lakens, 2022) and specified a 5-week data collection period.",BAD,0.043350415,258,NA,NA
b46,paper_698,"× design e f f ects where m = sample size [45,46], n = correction for the sample size (m) as a finite population, p (L) = indicator percentage (proportion of HFs conducting RI out of all HFs in the respective states), z = Z-value (1.96), e = relative error margin (10% = 0.01), and the design effect = 1.15.",method,14,3,1,"× design e f f ects where m = sample size [45,46], n = correction for the sample size (m) as a finite population, p (L) = indicator percentage (proportion of HFs conducting RI out of all HFs in the respective states), z = Z-value (1.96), e = relative error margin (10% = 0.01), and the design effect = 1.15.",GOOD,0.027680446,274,NA,NA
b38,paper_711,"In this preliminary study, our heuristic approach guided us to aim for a minimum of 40 participants, establishing a foundational dataset for our analysis (Lakens, 2022).",method,3,2,1,"In this preliminary study, our heuristic approach guided us to aim for a minimum of 40 participants, establishing a foundational dataset for our analysis (Lakens, 2022). However, our study faced the unforeseen challenge of participant dropout due to the COVID-19 pandemic mandated that we proceed with our research using a diminished final sample size for our analysis.",BAD,0.034474519,255,NA,NA
b13,paper_733,"It is noteworthy that the median sample size for survey-based platforms was close to the minimum sample size typically recommended for survey-based research (Barlett et al., 2001), suggesting that the ability to more participants online has not necessarily translated to adequate and justifiable sample sizes for surveybased research (see Lakens, 2022 for more information on justifying sample sizes).",discussion,13,3,3,"We found that both study length and sample size appear to vary with the type of recruitment method used: shorter studies are more likely to be crowdsourced, while longer studies are more likely to use professional recruitment platforms such as Prolific. On the whole, studies using survey-based platforms (such as Qualtrics) also had a considerably higher number of participants and a longer estimated time than studies using experiment-focused platforms (such as Gorilla). It is noteworthy that the median sample size for survey-based platforms was close to the minimum sample size typically recommended for survey-based research (Barlett et al., 2001), suggesting that the ability to more participants online has not necessarily translated to adequate and justifiable sample sizes for surveybased research (see Lakens, 2022 for more information on justifying sample sizes). We caveat this interpretation however, by highlighting that ∼5.5% of studies on Prolific and ∼24% of participants on Gorilla were studies with a sample size over 500, which clearly shows that despite the median being on the low side, researchers can and are recruiting large online samples. As low power due to inadequately small sample sizes contributes to research waste (Button et al., 2013), the potential for researchers to collect high quality data from large sample sizes quickly with online research platforms is particularly encouraging.",GOOD,0.040088953,456,NA,NA
b55,paper_735,The targeted number of participants was 150 which was reasonable given the resource constraints (56).,method,5,1,1,"The targeted number of participants was 150 which was reasonable given the resource constraints (56). Overall, N = 202 people opened the questionnaire. It was delivered exclusively online through the social networks of the research team following a convenience sampling approach. As part of the data cleaning process, data from 51 participants who just opened the questionnaire were deleted immediately and data from another four who completed <75% of the questionnaire were excluded from the analysis. Eventually, data from N = 146 participants was used for data analysis which almost met the targeted sample size.",BAD,0.062794629,294,NA,NA
b88,paper_736,"Second, we chose the sample size based on resource constraints regarding the limited number of available female participants for our laboratory experiments during the COVID-19 pandemic [89].",discussion,45,1,5,"There are several important limitations of the study. First, this study employed a very small number of participants who were female only. Therefore, the presented results should be considered preliminary only and subject to confirmation in future research. We want to note that we have recently completed a follow-up study with a larger cohort of male participants. Second, we chose the sample size based on resource constraints regarding the limited number of available female participants for our laboratory experiments during the COVID-19 pandemic [89]. Third, although we minimized the potential learning effects in each participant by administering the different comfort levels in a random sequence unknown to the participants, the effects of potential learning effects have yet to be investigated. It should also be noted here that people may perform the same physical tasks repeatedly at work and in everyday life. Brain plasticity can affect neural signatures of body responses to such tasks [90,91]. Lastly, our laboratory study could have produced somewhat different results if a control condition with no force applied first was used. However, a substitute for the control provision was the application of the maximum voluntary contraction (MVC), which can be considered equivalent to a control condition. This is because, at first, each subject was asked to exert the maximum force to the best of her ability without stipulating any physical comfort level or other influence. Future studies should ruminate on the above considerations when investigating the neural signatures of physical exertions and the associated comfort of such exertions.",BAD,0.055647739,472,NA,NA
b34,paper_740,"Quantitative research design is being used in the current study for determining the degree or the extent to which different factors identified (Lakens, 2022;Rashid & Rasheed, 2022).",method,8,1,7,"The approach used in the current research is explanatory. This is because the research under consideration is about developing hypotheses and then testing them for determining the presence of association between the independent and dependent variables. Therefore, the use of explanatory approach is used in the current study for providing explanation regarding the association between different aspects related to production and elimination of waste in the construction industry. According to Knight et al. (2022), explanatory design is used in quantitative researches for formulating hypotheses and designing methodology for the purpose of data collection with the objective of testing the overall theory (Khan, et al., 2022a, b). The other aspect related to the adoption of research approach is the choice of deductive approach. The justification of using the deductive approach is the involvement of hypotheses and then testing them for analyzing the relationship between independent and dependent variables. Quantitative research design is being used in the current study for determining the degree or the extent to which different factors identified (Lakens, 2022;Rashid & Rasheed, 2022). Out of different types of research designs that can be applied, the one that is applicable in the current research is the causal design (Hashmi et al., 2020a, b). This is because it is a kind of research that is based on analyzing cause and effect relationship between the variables which is a key aspect of any particular quantitative study (Sürücü & Maslakçi, 2020).",GOOD,0.042732075,478,NA,NA
b21,paper_742,"However, we also recognise and acknowledge that post hoc power analyses have substantial statistical issues [22][23][24] and thus cannot necessarily be relied on to confirm adequate sample size for all analyses and comparisons.",discussion,26,1,14,"Despite the large sample size and robust analysis and subanalyses of the SMHAT-1 in our current study, several limitations should be acknowledged. First, these data were collected after the start of and during the SARS-CoV-2 global pandemic, at which time the Tokyo 2020 Games were delayed, resulting in many athletes having to adjust to alternative training and competition schedules. The Beijing 2022 Games also introduced additional stressors for athletes that may not be present during a non-Games year, both related and unrelated to pandemic adjustments. These concerns likely influenced, to some degree, the prevalence of positive SMHAT-1 screens observed here. Moreover, these data, much like the original APSQ-validation studies, consist of athletes from a single country. It is acknowledged that this dataset is more diverse regarding sex, sport and seasons. However, additional data from other countries, especially countries with varying mental health educational systems, mental healthcare infrastructure and global sociocultural differences, should be analysed to confirm our results. Relatedly, only a small number of athletes were positively screened on subsequent questionnaires, especially on the PHQ-9 Q9. This small number of positive cases may produce biased estimates and unreliable (high or low) FNRs (see PHQ-9 Q9 in Winter athletes, table 2). We also acknowledge that more specific analyses of Paralympic athletes are required, accounting for category and degree of impairments. Regrettably, we do not have the requisite sample size to conduct this analysis, but we strongly encourage this in future SMHAT-1 studies. Although we included all available athletes in these analyses, we also acknowledge that the sample size may be underpowered, particularly for some of our subgroup analyses. To investigate this further, we calculated post hoc power analyses and reported those values in online supplemental material 1. However, we also recognise and acknowledge that post hoc power analyses have substantial statistical issues [22][23][24] and thus cannot necessarily be relied on to confirm adequate sample size for all analyses and comparisons. For example, the statistically significant differences in ASSQ score by season had a post hoc power (1-β) of <0.80, and thus these differences should therefore be interpreted with caution, although the true effect size in the population may not be represented by our observed effect size, and thus these power analyses may be misleading. We therefore further encourage other National Olympic and Paralympic Committees to continue collaborating and aggregating data to better understand how the SMHAT-1 can be improved for the sake of athletes' mental health and well-being.",GOOD,0.059809303,714,NA,NA
b20,paper_744,"If neither a precise prediction nor previously reported effect sizes are available to guide the specification, a reasonable approach is to use the smallest effect size of interest (Lakens, 2022).",intro,1,3,3,"Even though this procedure is widely accepted as a nominal standard in psychology, a priori power analyses are not without challenges. First, researchers must specify an expected effect size, which may be difficult. If neither a precise prediction nor previously reported effect sizes are available to guide the specification, a reasonable approach is to use the smallest effect size of interest (Lakens, 2022). This strategy, however, is related to a second challenge: The required sample size for a statistical test is a function of the test procedure (e.g., parametric vs. non-parametric tests), the desired 𝛼 (which is usually set to .05 or lower), the statistical power 1 -𝛽 (which is usually set to .80 or higher), and the hypotheses, that is, the expected effect size. When the effect size is small, the sample size required to ensure sufficiently low error probabilities is large. Unfortunately, psychological effects are typically small or medium sized (Funder & Ozer, 2019;Richard et al., 2003;Roberts et al., 2007). Thus, power analyses based on realistic effect-size expectations or smallest effect sizes of interest typically yield required sample sizes that exceed available resources. As a result, psychological studies are often based on too small sample sizes and thus underpowered (Button et al., 2013;Rossi, 1990;Szucs & Ioannidis, 2017).",BAD,0.129799362,477,NA,NA
b17,paper_745,"The large sample size presented in the experiment, about four to ve times larger than Simpson (1990) had the power to detect small, medium or large effects, given the sample size and after consulting power curvers (Lakens, 2022) such that if an even small gender effect was there, the current research would have detected it.",discussion,8,1,2,"Lack of Gender Differences Overall, men and women within the sample displayed similar levels of emotional distress after a romantic break-up. The large sample size presented in the experiment, about four to ve times larger than Simpson (1990) had the power to detect small, medium or large effects, given the sample size and after consulting power curvers (Lakens, 2022) such that if an even small gender effect was there, the current research would have detected it. Of course, there is still room to consider that there might have been a very small gender effect that escaped detection. Although highly avoidant men were the least distressed group out of the 4 gender/attachment style groups, the difference was not at a signi cant level in comparison to highly avoidant women (2% difference). A possible reason for a departure from Simpson's 1990 result could've been a difference in experimental design of the emotional distress measure. In the 1990 measure, the open-endedness of the questions, although measured on a likert scale, did not account for a participant's detailed account of emotional distress. This could have made it di cult to elicit detailed responses from any participant that is going through a breakup and may rather not be talking about their feelings-especially highly avoidant people who already have the tendency to avoid or suppress their emotions, with added societal pressure due to their gender. Furthermore, the experimental setting of a phone call which was used by Simpson 1990 is more personal than an anonymous online survey. Research has shown that people are more comfortable sharing personal details in an anonymous context (Joinson, 1999) and as a result Simpson's experimental design could have lowered the social-comfortability of participants, lowering the candidness of responses. Another experimental factor that could have contributed to Simpson's found gender difference could have been due to the mean age of the participants (18-19 years old) since research has shown that adolescent females favor higher levels of emotional intelligence, expression, and management than males of the same age (Ciarrochi et al., 2001).",GOOD,0.088170053,607,NA,NA
b63,paper_752,Provided that sample size was not estimated a priori in this study (sample size was constricted by limited resources and time constraints) a sensitivity analysis [63] was performed in G Power [64] to determine whether the effect size of this analysis was sensitive enough to detect a moderation.,method,9,1,9,"All analyses were run with IBM SPSS software, version 21. Given that the distribution of the studied variables did not significantly deviate from normality according to the Kolmogorov-Smirnoff test (all p > 0.05), parametric statistical analyses were performed. Pearson's correlation analyses were carried out to explore the relationship between the main variable (performance of children in the Snack Delay task, measuring IC) and both temperamental and environmental factors. A 95% confidence interval level (CI) for the estimated correlation parameters was also computed. To test our hypothesis of a three-way interaction between SES, EC, and parenting style that explained individual differences in the Snack Delay task, we conducted a moderation analysis. Analysis was performed with the macro PROCESS for SPSS [62]. We estimated the coefficients at a confidence interval level of 95% using bias-corrected bootstrapping approach with 5000 samples. Since age showed no significant correlation with the performance of toddlers in the Snack Delay task (r = 0.16, p = 0.22), we did not include age as a covariate in our analyses for parsimony. Provided that sample size was not estimated a priori in this study (sample size was constricted by limited resources and time constraints) a sensitivity analysis [63] was performed in G Power [64] to determine whether the effect size of this analysis was sensitive enough to detect a moderation. The critical F value for the R 2 increase in linear multiple regression (fixed model) with 80% power and α = 0.05 was calculated for our sample size.",GOOD,0.05892076,516,NA,NA
b12,paper_753,"It is noteworthy that the median sample size for survey-based platforms was close to the minimum sample size typically recommended for survey-based research (Barlett et al., 2001), suggesting that the ability to recruit more participants online has not necessarily translated to adequate and justifiable sample sizes for survey-based research (see Lakens, 2022 for more information on justifying sample sizes).",discussion,16,2,3,"We found that both study length and sample size appear to vary with the type of recruitment method used: shorter studies are more likely to be crowdsourced, while longer studies are more likely to use professional recruitment platforms such as Prolific. On the whole, studies using survey-based platforms (such as Qualtrics) also had a considerably higher number of participants and a longer estimated time than studies using experiment-focused platforms (such as Gorilla). It is noteworthy that the median sample size for survey-based platforms was close to the minimum sample size typically recommended for survey-based research (Barlett et al., 2001), suggesting that the ability to recruit more participants online has not necessarily translated to adequate and justifiable sample sizes for survey-based research (see Lakens, 2022 for more information on justifying sample sizes). We caveat this interpretation however by highlighting that ~5.5% of studies on Prolific and ~24% of participants on Gorilla were studies with a sample size over 500, which clearly shows that despite the median being on the low side, researchers can and are recruiting large online samples. As low power due to inadequately small sample sizes contributes to research waste (Button et al., 2013), the potential for researchers to collect high quality data from large sample sizes quickly with online research platforms is particularly encouraging.",GOOD,0.040123572,454,NA,NA
b30,paper_755,"For the present study, sample size is justified based on feasibility [31], and thus, no formal power analysis was performed.",method,8,1,1,"For the present study, sample size is justified based on feasibility [31], and thus, no formal power analysis was performed. As many participants as possible were recruited given the constraints on the investigators' time and resources. Thus, efforts have been undertaken to ensure that data are as easy as possible to meta-analytically aggregate in the future. All statistical analyses were performed using 'R' software (v 4.0.2; R Core Team, <https://www.r-project.4org/>, accessed on 1 November 2021).",BAD,0.036611921,296,NA,NA
b25,paper_756,"Many commonly used measures of statistical significance, such as the p-value, are strongly dependent on the size of the surveyed sample of people [26].",method,7,1,1,"Many commonly used measures of statistical significance, such as the p-value, are strongly dependent on the size of the surveyed sample of people [26]. When discussing the results of this study, effect size measures were used because they were independent of the size of the tested sample of the respondents [27][28][29]. In connection with the above, a sensitivity power analysis was carried out. The presented sample of 67 people using chemsex allowed for reliable conclusions about effects greater than ρ = 0.32 (in the case of correlations) and ƒ 2 = 0.12 (in the case of regression analyses), both at the level of α = 0.05 and the power of 1-β = 0.80. This means that based on the results of the presented group, one should not conclude about low effects that are smaller than the indicated value of the presented coefficients. In the case of the group comparison between people using chemsex (n = 67) and the control group (n = 108), the analysis showed that the results of Cohen's d coefficient exceeded the value of 0.44 (low effect), which should be considered reliable.",GOOD,0.037926667,424,NA,NA
b25,paper_763,"The main issues to consider when planning data collection and analysis, are: the population of interest; the research design, e.g., a randomized controlled trial or a survey (see, for example, Burkholder et al., 2019); whether the data collection is computerized (e.g., web-based questionnaires or lab-based computer tasks) and/or involves human interactions; the data collection protocols (which steps are necessary to collect the data) including all questions, tasks, and computer code (annotated such that it is clear for each section of code what it does); how the variables of interest are measured and coded (see Flake & Fried, 2020 for a paper on questionable measurement practices); which variables (e.g. sumscores) will be computed from the measured variables; which statistical models will be used and which hypotheses with respect to the parameters of these models will be tested; in classical statistics a power analysis (for starters read Cohen, 1992, andLakens, 2022) and in Bayesian hypothesis evaluation (Hoijtink et al., 2019) an updating plan (for starters read Rouder, 2014 andElsey, Filmer, &Stemerding, 2021); and, how to deal with missing data (see, for example, van Buuren, 2018) and data exclusion criteria.",intro,4,1,1,"The main issues to consider when planning data collection and analysis, are: the population of interest; the research design, e.g., a randomized controlled trial or a survey (see, for example, Burkholder et al., 2019); whether the data collection is computerized (e.g., web-based questionnaires or lab-based computer tasks) and/or involves human interactions; the data collection protocols (which steps are necessary to collect the data) including all questions, tasks, and computer code (annotated such that it is clear for each section of code what it does); how the variables of interest are measured and coded (see Flake & Fried, 2020 for a paper on questionable measurement practices); which variables (e.g. sumscores) will be computed from the measured variables; which statistical models will be used and which hypotheses with respect to the parameters of these models will be tested; in classical statistics a power analysis (for starters read Cohen, 1992, andLakens, 2022) and in Bayesian hypothesis evaluation (Hoijtink et al., 2019) an updating plan (for starters read Rouder, 2014 andElsey, Filmer, &Stemerding, 2021); and, how to deal with missing data (see, for example, van Buuren, 2018) and data exclusion criteria. Each of these issues is relevant when having a research project evaluated by a (medical) research ethics committee and when registering.",GOOD,0.042609424,484,NA,NA
b35,paper_782,"Larger follow up studies would be needed to affirm the presence or absence of such smaller effects (Lakens, 2022;Schäfer & Schwarz, 2019).",method,3,4,1,"Larger follow up studies would be needed to affirm the presence or absence of such smaller effects (Lakens, 2022;Schäfer & Schwarz, 2019).",GOOD,0.025941017,220,NA,NA
b32,paper_783,"Thus, we conducted a sensitivity analysis for the final sample size in each set of analyses (Lakens, 2022).",method,8,2,5,"We conducted an a priori power analysis based on our preplanned 2 × 2 analyses of variance (ANOVA). The a priori power analysis indicated that 128 participants would yield 80% power to detect medium-sized effects in a 2 × 2 design. Thus, we planned to collect 32 participants per cell for a total sample size of 192 across all six cells. However, given the constraints faced when collecting data, we collected 111 total participants (as further discussed in the Limitations section). Thus, we conducted a sensitivity analysis for the final sample size in each set of analyses (Lakens, 2022). The sensitivity analysis revealed that the final sample of 74 participants for the reverse order analyses yielded 80% power to detect large effects of η 2 > .10 (Cohen's d > 0.66) and 60% power to detect medium effects of η 2 > .06 (Cohen's d > 0.51). The final sample of 70 participants in the CI instructions analysis yielded 80% power to detect large effects of η 2 > .10 (Cohen's d > 0.68) and 60% power detect medium effects of η 2 > .07 (Cohen's d > 0.54).",GOOD,0.040068544,452,NA,NA
b39,paper_786,"Finally, Studies 1-3 consisted of small samples that were based on sample size justifications due to rules of thumb or effect size conventions that have since been called into question (Lakens, 2022).",discussion,54,1,8,"As with all studies, so did our studies have several limitations. First, we conducted the studies only in Norway and the United States, making generalization beyond these two countries difficult. Second, we did not have a truly neutral baseline condition in any of the studies, which does not allow us to draw strong conclusions about the absence of the experimental effect. Third, we did not measure the objects of the emotions, which would have allowed us to understand more about the concrete elicitors, nor did we measure other sources of intentions, which would have allowed us to examine the potential effects of the conditions on other predictors of intentions. Fourth, a longitudinal design with a pre-measure of intentions could have allowed us to examine changes in intentions more directly. Fifth, the use of a behavioral measure that assesses efforts toward climate change mitigation without being influenced by many external variables could have increased the intentionbehavior consistency. Sixth, ideally, different emotions should be measured with the same level of detail (number of items), which we did not do. Finally, Studies 1-3 consisted of small samples that were based on sample size justifications due to rules of thumb or effect size conventions that have since been called into question (Lakens, 2022). The experimental effects on intentions and behavior were rather small and our samples were not adequately powered to detect such effects. The meta-analytic approach can somewhat redeem this shortcoming, but future studies would need to focus on more valid sample size justifications such as selecting the smallest effect of interest. Furthermore, given how much people are exposed to media already, a single media exposure is unlikely to bring about lasting behavioral changes (Abraham et al., 2010;Landmann, 2020), particularly for behaviors with significance for a person's identity, group belonging, and ideology. Even short-term motivational changes are difficult to achieve. For example, a study comparing fear-and hopeinducing videos about climate change with neutral ones found no effect of condition on willingness to act (Ettinger et al., 2021).",GOOD,0.050029928,596,NA,NA
b17,paper_803,"Accordingly, we believe that future studies on this topic should urgently adopt practices such as conducting well-powered studies (e.g., Lakens, 2022b), replicating previous findings (e.g., Zwaan et al., 2018), and implementing preregistrations (e.g., Hardwicke & Wagenmakers, 2023) to generate valuable and interpretable findings on the embodied nature of language (see SS22 for a more detailed discussion; see also Solana, 2023).",results,6,9,2,"It is also worth noting that present results not only are in line with SS22, but also align with a growing number of studies, including preregistered and well-powered replication attempts (Montero-Melis et al., 2022;Morey et al., 2022), reanalyses of previously published work (Papesh, 2015;Witt et al., 2020), as well as other meta-analytic works (Winter et al., 2022), raising concern about key findings for the embodied view of language processing. Accordingly, we believe that future studies on this topic should urgently adopt practices such as conducting well-powered studies (e.g., Lakens, 2022b), replicating previous findings (e.g., Zwaan et al., 2018), and implementing preregistrations (e.g., Hardwicke & Wagenmakers, 2023) to generate valuable and interpretable findings on the embodied nature of language (see SS22 for a more detailed discussion; see also Solana, 2023).",GOOD,0.037267556,403,NA,NA
b16,paper_806,"Twelve adults (2 males, 10 females; convenience sample based on the heuristics justification (Lakens, 2022)) with a mean age of 22 years (range: 21-25 years) voluntarily took part in the study.",method,3,1,1,"Twelve adults (2 males, 10 females; convenience sample based on the heuristics justification (Lakens, 2022)) with a mean age of 22 years (range: 21-25 years) voluntarily took part in the study. To assure sufficient visual discrimination abilities, visual functions were assessed before the experiment. The participants' inclusion criteria were the following: normal or corrected-to-normal (with contact lenses) visual acuity (1.0 or better, decimal units), stereoscopic acuity of 40 arcsec or better (assessed using a Titmus stereotest, Stereo Optical Co., Chicago, IL). All participants were unaware of the specific purpose of the study. The study was approved by the Ethics Committee of the University of Latvia and was conducted in accordance with the Declaration of Helsinki.",BAD,0.040589039,349,NA,NA
b25,paper_810,"The practice is more solid for quantitative research (justify-n-qan) as researchers can conduct power analysis and calculate the minimum required sample size to acquire significant findings with a specific level of power and acceptable effect size [26,34].",method,15,1,2,"Justifying sample size before a study occurs is a well-known practice that explains how much the findings collected with a given sample size can be generalizable. The practice is more solid for quantitative research (justify-n-qan) as researchers can conduct power analysis and calculate the minimum required sample size to acquire significant findings with a specific level of power and acceptable effect size [26,34]. Support for these processes can come from software such as G*Power and other innovations for sample size computation [46]. For qualitative research (justify-n-qal), the most common justification for sample size is saturation [8] where researchers recruit participants until they reach saturation in their qualitative analysis (i.e., participants are no longer revealing new discussion topics). The second most common approach is to rely on previous studies, with researchers referring to a previous article. Lastly, the researchers might mention practical limitations or logistics.",GOOD,0.037419239,360,NA,NA
b30,paper_818,"The sample size in this study was determined by a combination of heuristic factors (i.e., orienting on the sample size of previous studies in the field such as [17,24,30] and resource constraints (i.e., the available number of elite athletes and time for doctoral research) rather than performing an a priori calculation of the sample size [31].",method,3,2,1,"The sample size in this study was determined by a combination of heuristic factors (i.e., orienting on the sample size of previous studies in the field such as [17,24,30] and resource constraints (i.e., the available number of elite athletes and time for doctoral research) rather than performing an a priori calculation of the sample size [31]. The sample size in the current study is constrained by (a) a limited number of available elite athletes, which is a common issue in studies investigating this cohort [1,32,33], (b) a time limit to complete the research for the doctoral thesis, and (c) the consequences arising from the measures against the spread of COVID-19 being present during the data collection (e.g., limited working hours and allowances, home confinement). In the current study, all measurements were taken between 10:30 and 12:00 AM, and the participants were asked not to consume ergogenic foods and drinks (e.g., caffeine, alcohol, vitamin complexes) that could affect their cognitive performance nearly 2-3 h before the assessments. All subjects were informed about the procedures, and each gave written consent. Prior to the assessments, the experimental setup and the application of cognitive tests were explained to the participants.",BAD,0.056368329,441,NA,NA
b37,paper_819,"Since this was an exploratory study, we determined the sample size using a heuristic sample selection, considering our budgetary restrictions (Lakens, 2022).",method,27,1,2,"One-hundred-and-fifty participants from the United States were recruited via the Internet platform Prolific (75 females, 75 males, Mage = 37.70, SD = 13.52). Since this was an exploratory study, we determined the sample size using a heuristic sample selection, considering our budgetary restrictions (Lakens, 2022). In line with the preregistration, data from six participants who used mobile phones were excluded from the analysis. Participants, all of whom were volunteers, were remunerated ($0.82). Before the study began, participants signed an informed consent form that briefly described the general purpose of the study; they were also told that they could withdraw at any point. The study was designed and conducted following ethical guidelines enforced by the University Ethical Committee and preregistered at the Open Science Framework (<https://doi.org/10.17605/OSF.IO/KWCBJ>).",BAD,0.048767678,375,NA,NA
b46,paper_823,"We collected as many participants as was logistically possible given resource constraints for each experiment (see Laken, 2022).",discussion,87,6,3,"An additional limitation of the current work may be the sample sizes, especially when manipulating perspective taking (Huang et al., 2021). A priori power analyses were unable to be conducted as no work has directly examined perspective taking and social tuning. We collected as many participants as was logistically possible given resource constraints for each experiment (see Laken, 2022). To try to rule out the possibility that any statistically significant findings were not a Type 1 error, we conducted six experiments to confirm the existence of our key findings. Over the six experiments, we demonstrate a proof of concept for an effect of perspective taking on the social tuning of explicit, but not implicit, self-views and attitudes. We report effect sizes for future researchers to conduct a priori power analyses and recommend larger sample sizes when within the resource constraints of researchers.",BAD,0.041383317,353,NA,NA
b4,paper_832,"Several methods of sample size justification/planning have been proposed (e.g., Kovacs et al., 2022;Lakens, 2022;Maier and Lakens, 2022).",intro,1,1,3,"Most psychological research using experimental approaches with null hypothesis significance testing assumes a population, selects a sample from it, and conducts statistical tests on the sampled data. How sample sizes are determined before experiments is crucial for discussing the research's inferential goals through statistical hypothesis testing. Several methods of sample size justification/planning have been proposed (e.g., Kovacs et al., 2022;Lakens, 2022;Maier and Lakens, 2022). Researchers should determine the sample size based on the justifications offered before collecting the data. Furthermore, they should discuss the results with respect to their sample size justifications. Moreover, based on these justifications, readers evaluate how informative the results are. Sample size justification is one of the factors used to determine what claims can be made based on the study's results and support its reliability.",GOOD,0.03809449,353,NA,NA
b4,paper_832,"If honestly reported, lacking a sample size justification may be benign (Lakens, 2022).",intro,1,2,2,"On the other hand, studies exist that have not justified (or could not justify) the sample size. If honestly reported, lacking a sample size justification may be benign (Lakens, 2022). However, a sample size justification is frequently required according to submission guidelines or reviewers' comments. As a result, manuscripts sometimes receive unfair evaluations simply because they lack sample size justification.",GOOD,0.026140402,260,NA,NA
b4,paper_832,"Here, the researchers used two justification patterns listed by Lakens (2022) resource constraints and heuristics.",intro,2,2,13,"The researchers then decided to use SPARKing before submitting to the next journal to increase the possibility of acceptance. In the manuscript, they created a new section called ""Sample Size Design."" They declared the following: We chose 10 participants per group as previous studies on weight perception have traditionally used a small sample size (e.g., Marsden et al., 1979;Amazeen and Turvey, 1996). Furthermore, we could not involve a large number of participants in the laboratory due to the COVID-19 lockdown. Furthermore, owing to an abundance of caution, they registered a protocol with Open Science Framework describing this sample size determination method. They included its URL in the same-named section of the manuscript (i.e., Sample Size Design). As a result, the reviewers found no fault with the sample size. Therefore, their paper was accepted for publication in the next journal. Congratulations! As seen above, nothing essential about the sample size changed. SPARKing has adjusted only superficial credibility. The sample size was justified by other reasons, although it was obtained through the stopping rule based on the researcher's convenience (i.e., the results supported their idea). Here, the researchers used two justification patterns listed by Lakens (2022) resource constraints and heuristics. The previous studies cited were chosen with great precision, although, in fact, the sample size of weight perception studies varies widely. However, many readers may have similar experiences with their work, especially in pandemic conditions.",BAD,0.050072435,485,NA,NA
b4,paper_832,"If authors do not have a specific and legitimate prior design related to establishing optimum sample size, they should disclose ""no justification"" (Lakens, 2022).",intro,5,1,7,"This paper aimed to review a new QRP related to the sample size justification, SPARKing. SPARKing potentially damages research credibility. Specifically, SPARKing, in conjunction with hacking pre-registration (i.e., PARKing : Yamada, 2018;Parsons et al., 2022), seems to ""unreasonably"" boost study reliability. At this time, no effective way to detect SPARKing is available. Of course, encouraging honest reporting of sample size justifications is desirable. Key ways to prevent the QRPs, including SPARKing, are transparency and honesty in reporting. If authors do not have a specific and legitimate prior design related to establishing optimum sample size, they should disclose ""no justification"" (Lakens, 2022). Moreover, if deviations from the research plan occur, the investigators should report them honestly and describe their reasons. We hope this paper will stimulate discussion within the research community concerning assessing the severity of SPARKing and seeking possible solutions.",GOOD,0.035967676,385,NA,NA
b46,paper_839,"While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46].",method,16,1,7,"Based on our financial resources to conduct this study, our original goal was to collect data from 150 Marshallese individuals. We collaborated with the primary social service agency that serves the Marshallese community in NWA; however, since it was closed to the public during the data collection period due to the pandemic, we were unable to recruit participants for our study via flyers, face-to-face community convenings, etc. The agency agreed that one of their staff members would contact all their adult clients via email to invite them to participate in this study. In the end, however, the staff member experienced multiple family illnesses and deaths due to COVID-19 after beginning the recruitment and data collection process and was unable to continue with recruitment after one month into the study. All other staff members were either tending to ill loved ones or focused on procuring donations and allocating groceries for the new Marshallese food pantry and were unable to assist with our recruitment. As a result, we collected data from less than half of our intended sample size. While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46]. According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",BAD,0.048391824,462,NA,NA
b46,paper_839,"According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",method,16,1,8,"Based on our financial resources to conduct this study, our original goal was to collect data from 150 Marshallese individuals. We collaborated with the primary social service agency that serves the Marshallese community in NWA; however, since it was closed to the public during the data collection period due to the pandemic, we were unable to recruit participants for our study via flyers, face-to-face community convenings, etc. The agency agreed that one of their staff members would contact all their adult clients via email to invite them to participate in this study. In the end, however, the staff member experienced multiple family illnesses and deaths due to COVID-19 after beginning the recruitment and data collection process and was unable to continue with recruitment after one month into the study. All other staff members were either tending to ill loved ones or focused on procuring donations and allocating groceries for the new Marshallese food pantry and were unable to assist with our recruitment. As a result, we collected data from less than half of our intended sample size. While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46]. According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",BAD,0,0,NA,NA
b46,paper_839,"While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46].",method,16,1,7,"Based on our financial resources to conduct this study, our original goal was to collect data from 150 Marshallese individuals. We collaborated with the primary social service agency that serves the Marshallese community in NWA; however, since it was closed to the public during the data collection period due to the pandemic, we were unable to recruit participants for our study via flyers, face-to-face community convenings, etc. The agency agreed that one of their staff members would contact all their adult clients via email to invite them to participate in this study. In the end, however, the staff member experienced multiple family illnesses and deaths due to COVID-19 after beginning the recruitment and data collection process and was unable to continue with recruitment after one month into the study. All other staff members were either tending to ill loved ones or focused on procuring donations and allocating groceries for the new Marshallese food pantry and were unable to assist with our recruitment. As a result, we collected data from less than half of our intended sample size. While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46]. According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",BAD,0,0,NA,NA
b46,paper_839,"According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",method,16,1,8,"Based on our financial resources to conduct this study, our original goal was to collect data from 150 Marshallese individuals. We collaborated with the primary social service agency that serves the Marshallese community in NWA; however, since it was closed to the public during the data collection period due to the pandemic, we were unable to recruit participants for our study via flyers, face-to-face community convenings, etc. The agency agreed that one of their staff members would contact all their adult clients via email to invite them to participate in this study. In the end, however, the staff member experienced multiple family illnesses and deaths due to COVID-19 after beginning the recruitment and data collection process and was unable to continue with recruitment after one month into the study. All other staff members were either tending to ill loved ones or focused on procuring donations and allocating groceries for the new Marshallese food pantry and were unable to assist with our recruitment. As a result, we collected data from less than half of our intended sample size. While resource constraints limit the amount of collected data, there is often a tradeoff between the costs of data collection and the value of having access to the data [46]. According to Lakens [46], even limited data are valuable in that they provide researchers with more knowledge about the research question, even when findings are not generalizable to the larger population.",BAD,0,0,NA,NA
b80,paper_854,"Cohen's d = 0.5 was set as the smallest effect size of interest (Lakens, 2022) for the present study because it has been specified in the literature to be the smallest difference individuals are able to detect in health-related quality of life outcomes (Norman et al., 2003).",method,7,1,3,"Participants were psychology undergraduate students at a university in Malaysia. An a priori power analysis using the pwr.f2.test function in R determined that to detect an effect size of f 2 = 0.06 (Cohen's d = 0.5), assuming alpha = 0.05, power = 0.8 and numerator degrees of freedom = 2, a sample size of 164 participants was required. Cohen's d = 0.5 was set as the smallest effect size of interest (Lakens, 2022) for the present study because it has been specified in the literature to be the smallest difference individuals are able to detect in health-related quality of life outcomes (Norman et al., 2003).",GOOD,0.031578013,335,NA,NA
b59,paper_857,"Separately, sample size determination also involves additional considerations beyond power analysis, such as the availability of resources and desired precision (Lakens, 2022).",intro,3,15,1,"Separately, sample size determination also involves additional considerations beyond power analysis, such as the availability of resources and desired precision (Lakens, 2022). Researchers should consider that sample sizes employed in power analyses do not have to be based on as-yet-unknown estimates of the effect size they are studying, but can instead be based on the researchers' Smallest Effect Size of Interest (SESOI: Lakens, Scheel, et al., 2018). Previous work has also pointed out ways in which standard power analyses may still under-power studies due to between-study heterogeneity. This may require that sample sizes be increased further. For a discussion of this issue as well as materials for performing power analyses that can account for this see McShane and Böckenholt (2014).",GOOD,0.033854933,347,NA,NA
b37,paper_858,"Powering a study to find an effect size estimated from previous research is a justification based on the ""expected effect size"" [38].",intro,4,2,7,"Closer attention to effect sizes would have made the RCIRCSA more useful. Specifically, the mere absence of a statistically significant effect tells us little about whether a publicly meaningful effect exists. Indeed, the authors reported that they powered the study to find an effect size ""determined based on the magnitude of effects observed in past studies"" [36] and did not report those calculations. Beyond this lack of transparency, there are two problems with powering the RCIRCA study to find previously reported effects. First, as we saw above, effect sizes in the literature are often overstated and so it is likely the RCIRCSA study was underpowered to find the effect they thought they were seeking. But, more importantly, the sample size justification did not fit with the RCIRCSA's inferential goal. Powering a study to find an effect size estimated from previous research is a justification based on the ""expected effect size"" [38]. In other words, the researchers are powering a study to find an effect of the size that research and theory predicts. However, that justification is only weakly (or perhaps not at all) related to the goal of determining whether it is safe to join trials.",GOOD,0.037946577,426,NA,NA
b37,paper_858,"Perhaps most fundamentally, law and psychology researchers must transparently report their effect sizes, their sample size justifications (which often hinge on an effect size), and any calculations underlying that work [38].",intro,5,2,2,"Researchers-when conducting research-also have an important role to play. Perhaps most fundamentally, law and psychology researchers must transparently report their effect sizes, their sample size justifications (which often hinge on an effect size), and any calculations underlying that work [38]. As we saw [29], such reporting does not always occur, yet is important in evaluating the legal and policy implications of psychological research. Similarly, researchers should candidly report both how effect sizes may or may not accumulate in the across time and people. By way of analogy, psychological researchers have prescribed [39] constraints on generality statements for research, which may be especially useful for lay readers to understand why a study may not apply to a context or population. The same level of candor and transparency should also be routine when presenting effect sizes in applied research. Finally, law and psychology researchers should consider engaging in increased metaresearch-research on its own methods and processes [40]. This work is underway, including the previously discussed survey of false memory researchers about the smallest effect size of interest in their work [15] and scrutiny of the hungry judges study finding several reasons its findings were likely overstated (e.g., [29]). However, much more is needed and can generally follow the discussion above. For instance, large preregistered replication projects of foundational studies can help provide more precise estimates of effect sizes and can be used as a basis to begin to estimate heterogeneity of effects across populations and contexts [12,41]. This work can help replace more impressionistic metaresearch in law and psychology which often relies on subjective judgments of researchers about what work has reached ""general acceptance"" (see e.g., [42]). It can also supplement this comment, which has relied on case studies rather than a systematic survey of the literature. And, to capitalize on these efforts [40], researchers should conduct regular audits (in psychology generally, see e.g., [43]) of reporting practices in law and psychology to see if reform efforts are working-that is, if researchers and practitioners actually begin to report effect sizes more transparently and more cautiously.",GOOD,0.051299458,607,NA,NA
b20,paper_862,"Given our sample size of 78 and 8 predictors in our final linear regression model, our designed study has 90% power to detect effects (f 2 ) of at least 0.273 [21].",method,7,1,2,"We performed a post-hoc sensitivity power analysis using GPower (Version 3.1.9.2, Universität Kiel, Germany) for linear multiple regression. Given our sample size of 78 and 8 predictors in our final linear regression model, our designed study has 90% power to detect effects (f 2 ) of at least 0.273 [21].",GOOD,0.027702736,263,NA,NA
b16,paper_871,"Para la obtención de la muestra se utilizó la fórmula de cálculo de proporciones para población infinita (al tratarse de más de 10 mil hogares en toda la ciudad, se asume una normalidad con distribución homogénea) (17) , en el cual se introdujo un nivel de confianza del 95%, una prevalencia esperada del 81% y un error aceptado del 5%; según lo reportado por Sánchez y Mejía (13) en 2019.",method,2,1,4,"Tipo y lugar de estudio: investigación de enfoque cuantitativo, de alcance descriptivo y transversal (16) . Se llevó a cabo en una población de una ciudad del estado de Yucatán, México. Unidad de análisis y muestra: se consideró como unidad de análisis a los hogares mexicanos, los cuales se componen por personas que habitan dentro de una casa-habitación y que tienen una relación consanguínea o filial. Para la obtención de la muestra se utilizó la fórmula de cálculo de proporciones para población infinita (al tratarse de más de 10 mil hogares en toda la ciudad, se asume una normalidad con distribución homogénea) (17), en el cual se introdujo un nivel de confianza del 95%, una prevalencia esperada del 81% y un error aceptado del 5%; según lo reportado por Sánchez y Mejía (13) en 2019. Finalmente se determinó un mínimo de 237 viviendas a encuestar, para lograr representatividad de la muestra. Criterios de inclusión y exclusión: se consideraron aptos para el estudio a todos los hogares con acceso a internet que estuvieran dentro de la zona urbana seleccionada, la cual se caracterizaba por una gran cantidad de farmacias del sistema público y privado, así como su alta densidad poblacional. La encuesta se hizo llegar a las personas que se identificaran como los administradores del hogar y/o cuidadores principales. Se excluyeron las zonas periurbanas colindantes, la población que llevaba viviendo menos de 6 meses (de acuerdo con la definición operativa de población flotante, ofrecida por el Consejo Nacional de Población en México) (18) en la zona delimitada y hogares donde habite solo una persona. Instrumentos: los datos fueron recabados en un periodo de tres meses, durante el año 2022, a través de una encuesta virtual de la compañía Google®. Dicha encuesta se distribuyó a través de redes sociales y mensajería instantánea para que las personas pudieran contestar desde sus hogares. Para los guardan (si poseen), así como la declaración de si creen que pueden tener medicamentos caducados, el mecanismo de disposición y finalmente, la autoevaluación sobre si los participantes creen que almacenan sus medicamentos de forma correcta. Análisis de datos: los datos obtenidos fueron exportados al software estadístico SPSS ® V 20, mediante el cual se calcularon medidas de tendencia central y dispersión (en cada una de las variables de la encuesta). Además, el análisis de riesgo se realizó con la medida del Odds Ratio (OR) y la prueba de Chi cuadrado (χ²) para independencia; esto con el fin de analizar la asociación entre los factores familiares y el almacenamiento de medicamentos. Aspectos éticos: la investigación mantuvo el anonimato de los datos y contó un consentimiento informado, en formato virtual, así como con la aprobación del Comité de Ética de la Facultad de Enfermería de la Universidad Autónoma de Yucatán. Se respetaron los principios bioéticos contenidos en la Declaración de Helsinki (22) y en la Ley General de Salud en materia de investigación para la salud en seres humanos de los Estados Unidos Mexicanos, artículo 100, título quinto (23) .",GOOD,0.092362608,972,NA,NA
b28,paper_876,"Alternatively, researchers may consider using the smallest effect size of interest (Lakens, 2022).",method,11,2,5,"As my design was mixed methods and I was unaware of any studies that have investigated momentary eco-anxiety experimentally, it was challenging to determine an ideal sample size. One of the more effective ways to ascertain sample size is to run an a-priori power analysis. This analysis often necessitates a determination of what effect size is expected to be important. Popularly, researchers base this on what effects have been found in past relevant research, which are unfortunately lacking for this study. Alternatively, researchers may consider using the smallest effect size of interest (Lakens, 2022). However, as this study focused on assessing the usefulness of a potential induction technique, I was hoping to see a medium-large effect (d = .50 ;Cohen, 1988). This anticipated effect size aligns with meta-analysis of mood induction procedures estimated a mean effect of rm = 0.481 across 250 experimental inductions (Westermann et al., 1996). An a-priori power analysis was conducted via G*Power 3.1.9.6 (Faul et al., 2007). This analysis revealed that in order to achieve 95% power and an effect size of .50, with a 5% margin of error, I required a minimum of 45 participants. However, since I wanted to be able to assess effectiveness of each video as well, I knew I would likely need more than that.",GOOD,0.056234005,475,NA,NA
b70,paper_877,"In addition, it is important to have good power to allow for exploration (Lakens, 2022).",method,17,2,2,"However, it is still essential to justify the required sample size. In addition, it is important to have good power to allow for exploration (Lakens, 2022). A previous simulation study has indicated that correlations generally stabilize with a sample of at least 250 individuals for a small effect size of .1 with 80% confidence (Schönbrodt & Perugini, 2013). Therefore, a sample size of 300 will be run to allow for the deletion of poor data. A sensitivity analysis was run in G*Power to explore the effect size that can be identified with this sample size. With alpha set at .05, 90%",GOOD,0.030584565,318,NA,NA
b19,paper_878,"These participant numbers represent an appropriate sample size for small population surveys (a minimum of 50%), based on resource constrains (Lakens, 2021).",method,4,6,4,"According to the annual report 2020-21, the TQI has ""received 24 new certification submissions"" (ACT TQI, 2021, p. 256). Fourteen teachers have undertaken the survey for current and former HALT applicants and twelve teachers have responded to the survey of the prospective applicants. These participant numbers represent an appropriate sample size for small population surveys (a minimum of 50%), based on resource constrains (Lakens, 2021). De-identified data collected from the online surveys are presented below providing tables and charts generated by the Qualtrics data analysis and presentation tools.",BAD,0.04724897,310,NA,NA
b35,paper_883,"The sample for this study was four of the five county executives who were in office in Tennessee as of January 1, 2022, using purposeful sampling to approach potential participants who have the desired characteristics for the study (Lakens, 2022), specifically that they were either female county executives in south-central Tennessee or were women county mayors in Tennessee.",method,6,1,1,"The sample for this study was four of the five county executives who were in office in Tennessee as of January 1, 2022, using purposeful sampling to approach potential participants who have the desired characteristics for the study (Lakens, 2022), specifically that they were either female county executives in south-central Tennessee or were women county mayors in Tennessee. Scholarly saturation was achieved due to the 80% participation rate. Participants were interviewed virtually via TEAMS, using an interview protocol of 29 standardized questions, to avoid loud noises and other interruptions and to keep the participant's information confidential. Before starting the interview, contributors signed a document confirming their agreement to participate in the study. The participants contributed to the study of their own free will and accord.",BAD,0.040371269,337,NA,NA
b48,paper_884,"The sample size was convenience-based and justified based on feasibility expectations given the researchers' access to the sample population, i.e., a resource constraints-based justification (49).",method,3,1,4,"After institutional ethical approval (ER38311849), 18 male participants were recruited using a convenience sampling approach that recruited eligible participants on a first-come, first-served basis. Participants represented a cross-section of strength and physique sports: weightlifting (n = 3), powerlifting (n = 12) and bodybuilding (n = 10). Some participants (n = 7) represented more than one sport (see Table 1 for a detailed descriptive profile of each participant). The sample size was convenience-based and justified based on feasibility expectations given the researchers' access to the sample population, i.e., a resource constraints-based justification (49). The mean duration of coaching experience at ≥ national standard was 10.9 (SD = 3.9) years. Fifteen participants had additional experience competing as an athlete at a minimum of national level in their respective sport. Education level ranged from no academic degree (n = 2) to Doctor of Philosophy (n = 10). Participants possessed a range of relevant sport-specific governing body certifications, with some holding additional Personal Fitness Training or Strength and Conditioning accreditations (e.g., National Strength and Conditioning Association). Each potential participant was screened for eligibility, and informed consent was obtained prior to taking part in the interview according to the principles of the Declaration of Helsinki (50).",BAD,0.048230667,453,NA,NA
b25,paper_889,"Sensitivity analyses indicate that the current sample size would be powered at 80% to detected an f 2 value of .13, which is small to medium (see Lakens, 2022 for a detailed discussion of sample size justifications when using secondary data).",method,3,2,1,"Sensitivity analyses indicate that the current sample size would be powered at 80% to detected an f 2 value of .13, which is small to medium (see Lakens, 2022 for a detailed discussion of sample size justifications when using secondary data). Participants provided written informed consent for the original studies in accordance with the Institutional Review Board of the University of Arizona. They also provided consent to have their data (including audio files) used for future studies.",GOOD,0.027642112,278,NA,NA
b59,paper_890,"Sample size justification was carried out based on resource constraints for a web research panel (Lakens, 2022), and a sensitivity analysis was conducted to identify effect sizes that the sample could detect with 80% power (Wang and Rhemtulla, 2021).",method,7,2,2,"Narrowing our scope and inferences to the second survey with a larger pool of participants, our study focuses on the results of the web research panel that employed randomized recruitment of geographically diverse respondents throughout Japan with specific age screening criteria to the span of emerging adults. Sample size justification was carried out based on resource constraints for a web research panel (Lakens, 2022), and a sensitivity analysis was conducted to identify effect sizes that the sample could detect with 80% power (Wang and Rhemtulla, 2021). The plan for the study was pre-registered prior to analysis on the Open Science Framework (see section ""Data availability statement"").",GOOD,0.030957303,318,NA,NA
b35,paper_896,"This study aimed to recruit 250-300 pharmacists, with this number determined both by sample sizes of our teams' earlier surveys of health professionals around MC and resourcing constraints [35][36][37].",method,5,1,4,"An on-line cross-sectional survey was conducted between May and December 2021. Participants met the inclusion criteria if they were registered pharmacists working in an Australian community pharmacy. Australian pharmacy organisations (The Pharmacy Guild of Australia and the Pharmaceutical Society of Australia) assisted with participant recruitment by promoting the survey through their professional education events, social media channels and private mailing groups. This study aimed to recruit 250-300 pharmacists, with this number determined both by sample sizes of our teams' earlier surveys of health professionals around MC and resourcing constraints [35][36][37]. As an incentive to complete the survey, pharmacists were given the chance to enter a draw to win one of three Apple watches after survey completion.",BAD,0.038729516,327,NA,NA
b35,paper_896,"The design of the questionnaire (see Online Resource 1 for a full copy) was informed by previous surveys of health professionals around cannabis-based medicines [1,32,33,35,36].",method,6,1,1,"The design of the questionnaire (see Online Resource 1 for a full copy) was informed by previous surveys of health professionals around cannabis-based medicines [1,32,33,35,36]. The questionnaire also uniquely queried Australian pharmacists' perspectives and knowledge of Pharmacist Only CBD products, given the recent legislative changes affecting these products. It contained five sections and a total of 51 items (Table 1) that took ~ 15-minutes to complete. The questionnaire was administered using a secure, webbased platform (REDCap ® 12.0.7, 2022, Vanderbilt University). Pharmacists were required to review the Participant Information Statement and complete an online checkbox to confirm informed consent before commencing the questionnaire. An adaptive algorithm and assortment of query formats were used to reduce respondent fatigue and maximise completion rates, including multiple choice, yes-no-unsure, true-false-unsure and 5-Point Likert Scale (e.g., Strongly Agree, Agree, Neutral, Disagree or Strongly Disagree) formats.",GOOD,0.035788332,398,NA,NA
b35,paper_896,"When Pharmacist Only CBD products become available, knowledge development in this area will be essential for optimal patient care and to actualise health professionals' roles to their full potential [33,36], particularly with pharmacists set to become the 'frontline suppliers' of Pharmacist Only CBD products.",discussion,16,2,7,"Lack of pharmacist confidence and comfort with MC supply was underlined by 'low' objective knowledge; twothirds of pharmacists who completed the Knowledge section of the survey achieved a score of < 60%. While these results may not entirely represent the whole survey cohort, many pharmacists also noted low perceived knowledge around regulatory changes affecting Pharmacist Only CBD products. This may reflect the fact that no Pharmacist Only CBD products are currently registered or marketed, meaning that pharmacists may not consider competency in this area a priority. The low levels of knowledge observed in this study are consistent with numerous surveys of pharmacists around cannabis-based medicines [31-34, 50, 51]. Indeed, the piecemeal approach to legalising MC has been identified as creating gaps in knowledge [51]. In one review, 58.9-81.9% of pharmacists were predicted to have a low perceived knowledge around MC [34], while only 5% of pharmacists in another survey of Californian pharmacists reported having 'professional level' knowledge around MC [33]. When Pharmacist Only CBD products become available, knowledge development in this area will be essential for optimal patient care and to actualise health professionals' roles to their full potential [33,36], particularly with pharmacists set to become the 'frontline suppliers' of Pharmacist Only CBD products.",GOOD,0.044817749,458,NA,NA
b36,paper_896,"This study aimed to recruit 250-300 pharmacists, with this number determined both by sample sizes of our teams' earlier surveys of health professionals around MC and resourcing constraints [35][36][37].",method,5,1,4,"An on-line cross-sectional survey was conducted between May and December 2021. Participants met the inclusion criteria if they were registered pharmacists working in an Australian community pharmacy. Australian pharmacy organisations (The Pharmacy Guild of Australia and the Pharmaceutical Society of Australia) assisted with participant recruitment by promoting the survey through their professional education events, social media channels and private mailing groups. This study aimed to recruit 250-300 pharmacists, with this number determined both by sample sizes of our teams' earlier surveys of health professionals around MC and resourcing constraints [35][36][37]. As an incentive to complete the survey, pharmacists were given the chance to enter a draw to win one of three Apple watches after survey completion.",BAD,0,0,NA,NA
b14,paper_899,"Thus, the probabilities provided by the Tables should only be used as reference, and only when similar assumptions to the ones employed here can be justified (Lakens, 2022).",discussion,4,5,3,"Also, it should be mentioned that the probabilities provided by the reference tables, do not correspond to the probability of successfully surpassing a BF threshold for a given experimental design. It is important to distinguish that the minimum effect size of interest discussed above, will in most cases differ to the observed difference in an experiment (for a similar argument see Dienes, 2019). Thus, the probabilities provided by the Tables should only be used as reference, and only when similar assumptions to the ones employed here can be justified (Lakens, 2022). To illustrate this, consider the study by Taikapi et al. ( 2022), who designed a within-subject experiment with sample size n = 15 to test the effects of a specific brain stimulation protocol on cortical excitability before and after stimulation. The authors were interested in testing for a minimum effect size of d = 0.7, and employed a Bayesian paired t-test, which reflected this prior assumption, for cortical excitability before brain stimulation and cortical excitability 45 minutes after brain stimulation. The t-test yielded a BF10 = 34.87. In general, brain stimulation studies are expected to result in data with high variability (Phylactou et al., 2022). Therefore, assuming high variability, we turn to Table 7 for reference. According to Table 7 a minimum effect size of d = 0.7, a BF10 ≥ 3 with such small sample size (n < 20) is less than 30% probable. However, this probability is only true if the observed effect size in the experiment is approximately d ≈ 0.7. In such case, if the experiment is repeated 10000 times, and the true effect size is d ≈ 0.7, then only 30% of the times a threshold of BF10 ≥ 3 will be reached. However, in the experiment of Traikapi et al., (2022), the difference before and after stimulation were consistent and large (d = 1.1), which eventually resulted in a higher BF. This is also discussed in previous work, which illustrated that the resulting BF in a given experiment relies heavily on the observed data and that, usually, assumptions reflected in the prior distributions have little impact on the resulting BF (Schönbrodt et al., 2017;van Ravenzwaaij & Etz, 2021;Wagenmakers et al., 2010).",GOOD,0.060014232,685,NA,NA
b60,paper_901,A sensitivity analysis was run to determine the predicted magnitude of the effect sizes needed to grant reliability to the observed findings [59][60][61].,results,7,1,5,"Collected data was wrangled in RStudio [57] and analysed with JASP [58]. Descriptive analyses were undertaken to ascertain reaction times and accuracy (see Table 1). Mean reaction times (RT) were computed for each condition and participant at a trial level by including only accurate responses. Additionally, participants' RT that were below 100 ms and 2.5 SD faster or slower than the mean RT per condition or those associated with timedout responses were rejected (2.94% of the data in the 2D modality and 1.86% of the data in the 3D version). A sensitivity analysis was run to determine the predicted magnitude of the effect sizes needed to grant reliability to the observed findings [59][60][61]. G*Power 3.1 software [62] was used to run this analysis which was conducted using F tests with α = 0.05, power = 0.80, and a total sample size of 36 people as the input parameters. Given such a sample size, alpha, and estimated power, the sensitivity analysis revealed that we could detect values as low as f = 0.18 (F critical = 2.27 (5, 170)); consequently, resulting F-values equal to or greater than the F critical value were deemed significant and reliable. (Note that all the significant F-values reported in the current study at the p < 0.05 level were also significant when contrasted with the F critical value.)",GOOD,0.062754487,489,NA,NA
b58,paper_904,The sensitivity power analysis can assess the minimal effect size to achieve 80% power when the sample size and alpha level are fixed 59 .,results,2,2,39,"EEG results. For P1, a 2 (race: White, Asian) × 2 (task demand: recognition, categorization) × 2 (hemisphere: O1, O2) repeated-measure ANOVA was conducted (see Fig. 2A). The main effect of task demand was significant, F(1, 26) = 9.926, p = 0.004, η 2 p = 0.276, with the categorization task (1.56 ± 0.48 µV) exhibiting a larger amplitude than the recognition task. The interaction between task demand and hemisphere was also significant F(1, 26) = 10.381, p = 0.003, η 2 p = 0.285, with the categorization task demonstrating a larger amplitude than the recognition task in both the left (1.24 ± 0.48 vs. 0.59 ± 0.40; p = 0.039) and right (1.88 ± 0.52 vs. 0.68 ± 0.41 µV; p < 0.001) hemispheres, but the variations between the two task-evoked P1 amplitudes were different, with a larger difference noted in the left hemisphere. Other main effects and the interaction did not reach significant levels (ps > 0.174, ηs 2 p < 0.070). For N170, a 2 (race: White, Asian) × 2 (task demand: recognition, categorization) × 2 (hemisphere) × 2 (site: P7/8, PO7/8) repeated-measure ANOVA was performed (see Fig. 2B). The main effect of task demand for N170 was significant F(1, 26) = 14.786, p = 0.001, η 2 p = 0.363, with recognition (-3.95 ± 0.64 µV) demonstrating a larger N170 amplitude than categorization (-2.65 ± 0.56 µV) task demand. The main effect of race and the interaction effect of race and task demand were not significant (ps > 0.215, ηs 2 p < 0.059). The interaction between the hemisphere and task demand was significant F(1, 26) = 8.856, p = 0.006, η 2 p = 0.254; further analysis found that the right hemisphere demonstrated larger N170 amplitudes than the left hemisphere for both categorization (-3.06 ± 0.66 vs. -2.23 ± 0.50; p = 0.024) and recognition tasks (-4.44 ± 0.75 vs. -3.24 ± 0.57; p = 0.001), but with differing magnitudes. The interaction between hemisphere and race was significant F(1, 26) = 8.225, p = 0.008, η 2 p = 0.240, with White faces (-4.02 ± 0.67 µV) evoking larger N170 amplitude than Asian faces (-3.79 ± 0.69 µV) in the right hemisphere (p = 0.022, η 2 p = 0.186), and the N170 amplitude of White faces (-2.71 ± 0.52 µV) and Asian faces (-2.76 ± 0.51 µV) was not significantly different at the left hemisphere (p = 0.713, η 2 p = 0.005). The interaction between site, task demand, and race was significant F(1, 26) = 8.016, p = 0.009, η 2 p = 0.236. Further separate analysis found that the interaction between site and race was significant for the recognition task F(1, 26) = 4.400, p = 0.046, η 2 p = 0.145, with a marginally significantly larger N170 amplitude for White (-4.81 ± 0.66 µV) than Asian (-4.55 ± 0.68 µV) faces at P7/8 (p = 0.090), whereas a non-significant difference of N170 amplitude was found for both White (-3.16 ± 0.64 µV) and Asian (-3.28 ± 0.63 µV) faces at PO7/8 (p = 0.458). The interaction between the site and race was not significant in the categorization task F(1, 26) = 3.067, p = 0.092, η 2 p = 0.106. Other post hoc separation analyses of site, task and race did not exhibit significant two-way interaction effects (ps > 0.147, ηs 2 p < 0.079). The interaction between task demand, race, and hemisphere was not significant F(1, 26) = 2.510, p = 0.125, η 2 p = 0.088, for the left hemisphere (P/PO7), the interaction between task demand and race was not significant F(1, 26) = 0.900, p = 0.352, η 2 p = 0.033. Regarding the right hemisphere (P/PO8), the interaction between task demand and race did not reach a significant level F(1, 26) = 0.153, p = 0.69, η 2 p = 0.006. The main effect of site was significant F(1, 26) = 39.848, p < 0.001, η 2 p = 0.605, with P7/8 (-4.01 ± 0.60 µV) demonstrating a larger N170 amplitude than PO7/8 (-2.58 ± 0.57 µV). The main effect of the hemisphere was significant F(1, 26) = 10.036, p = 0.004, η 2 p = 0.279, so larger N170 amplitudes were observed for the right (-3.86 ± 0.68 µV) versus (-2.74 ± 0.51 µV) the left hemisphere. The other main and interaction effects were not significant (ps > 0.071, ηs 2 p < 0.120). Analysis of P2 was conducted with a 2 (race: White, Asian) × 2 (task demand: recognition, categorization) × 3 (site: Fz, Cz, Pz) repeated-measured ANOVA (see Fig. 2C). A significant main effect for task demand was observed, F(1, 26) = 31.402, p < 0.001, η 2 p = 0.547, with the recognition (1.62 ± 0.27 µV) task exhibiting a larger P2 amplitude than the categorization (0.77 ± 0.24 µV) task. The main effect of race was significant, F(1, 26) = 4.377, p = 0.046, η 2 p = 0.144, with White faces (1.26 ± 0.25 µV) resulting in a larger P2 than Asian(1.13 ± 0.24 µV) faces. The main effect of site was also significant, F(2, 52) = 15.132, p < 0.001, η 2 p = 0.368, reflecting a larger amplitude of P2 at Fz than Pz (1.96 ± 0.36 vs. 1.72 ± 0.38 µV; p = 0.001), and Cz than Pz (1.72 ± 0.38 vs. -0.101 ± 0.26 µV; p < 0.001). No other main or interaction effects reached significance (ps > 0.057, ηs 2 p < 0.132). For P300, a 2 (race: White, Asian) × 2 (task demand: recognition, categorization) × 4 (site: Fz, Cz, Pz, Oz) repeated-measured ANOVA was performed (see Fig. 2C). The main effect of task demand was significant F(1, 26) = 16.538, p < 0.001, η 2 p = 0.389, revealing that the recognition task (0.81 ± 0.15 µV) demonstrated a larger P300 than the categorization task (0.39 ± 0.20 µV). The main effect of site was significant F(3, 78) = 4.071, p = 0.027, η 2 p = 0.135, reflecting a larger amplitude of P300 at Pz (1.41 ± 0.32 µV) than Fz (-0.32 ± 0.33 µV; p = 0.024) and Cz (0.84 ± 0.26 µV; p = 0.035). The interaction between the site and race was significant, F(3, 78) = 6.610, p = 0.003, η 2 p = 0.203. Post-hoc analysis found that for Asian faces, a smaller amplitude of P300 was found at Fz (-0.47 ± 0.34 µV) than Cz (0.69 ± 0.29 µV; p = 0.036) and Pz (1.34 ± 0.31 µV; p = 0.018); for White faces, a smaller amplitude of P300 was found at Fz (-0.18 ± 0.33 µV) than Cz (0.97 ± 0.25 µV; p = 0.020). The other main and interaction effects were not significant (ps > 0.100, ηs 2 p < 0.101). To confirm that the given number of participants in the study were allowed to detect of sufficient power, we conducted a sensitivity power analysis using G*Power software 58 . The sensitivity power analysis can assess the minimal effect size to achieve 80% power when the sample size and alpha level are fixed 59 . A repeated measurement of ANOVA Analysis with 27 participants in our study would be sensitive to the effects of f = 0.230 (η 2 p = 0.050) with 80% power (α = 0.05), revealing that an observed effect size larger than η 2 p = 0.050 would have 80% power to detect the effect.",GOOD,0.185230029,2484,NA,NA
b32,paper_907,"Across all studies, we did not perform a specific a-priori power analysis but mainly focused on collecting as many participants as possible (Lakens, 2022).",method,10,1,1,"Across all studies, we did not perform a specific a-priori power analysis but mainly focused on collecting as many participants as possible (Lakens, 2022). We tried to reach a sample size of at least n = 200 for all studies based on previous simulations on when correlation coefficients stabilize (Schönbrodt & Perugini, 2013). For Study 4, we registered that we would recruit a minimum of 600 participants to achieve enough power for smaller effects that we anticipated due to the focus on associations between self-re port and actual behavior. Study 6 was conducted as a screening study for a different unrelated project, and sample size determination was based on resources available for the different study (see Supplementary Material Section 1). We registered to collect 600 or 1200, depending on the outcome of the other unrelated study. Performing a two-tailed sensitivity analysis in G*Power 3.1 (Faul et al., 2009) with an alpha level of .05 suggests that we could detect correlation coefficients of magnitude between +/-.08 and .15 with 90% power across the studies (see Supplementary Figure S1).",BAD,0.053713821,420,NA,NA
b20,paper_908,"Instead, we used resource constraints to justify our sample size (Lakens, 2022).",method,8,1,3,"Before collecting the data, we preregistered our target sample size. Because we had no specific expectations about the effect sizes in this study, we were not convinced we could perform an accurate a priori sample size analysis. Instead, we used resource constraints to justify our sample size (Lakens, 2022). In coordination with the panel agency, we estimated that we could collect data from a maximum of 250 participants, given our budget and the number of panel members with children in the target group. However, we could potentially draw our conclusions with a smaller sample. Therefore, we registered two checkpoints (at N = 150 and N = 200) and checked whether we had found at least moderate evidence for all the vignettes. At the PARENTS' PERCEPTIONS OF PARENTAL CONSENT PROCEDURES 10 second checkpoint, we had gathered enough evidence to discontinue data collection at an early stage.",BAD,0.04280314,371,NA,NA
b33,paper_909,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34].",intro,5,1,1,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0.029274695,301,NA,NA
b33,paper_909,"Hence, they are motivated to manage the constraints that limit their performance [34].",intro,5,1,3,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34].",intro,5,1,1,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Hence, they are motivated to manage the constraints that limit their performance [34].",intro,5,1,3,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34].",intro,5,1,1,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Hence, they are motivated to manage the constraints that limit their performance [34].",intro,5,1,3,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34].",intro,5,1,1,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,"Hence, they are motivated to manage the constraints that limit their performance [34].",intro,5,1,3,"Nevertheless, some barriers may appear during task performance, such as lack of materials, scarce information, or workload-characteristics from healthcare working settings-which may negatively influence performance [34]. Some constraints that result from the performance of tasks, at certain times, such as stress from dealing with patients, the pressure exerted on individuals, the long working hours, and the complexity of the tasks, restrict the performance of the task; however, these constraints may encourage adaptive performance behaviors when individuals are intrinsically motivated. Hence, they are motivated to manage the constraints that limit their performance [34].",GOOD,0,0,NA,NA
b33,paper_909,These results are consistent with the theoretical [8] and empirical [34] literature.,discussion,12,2,4,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0.080923654,655,NA,NA
b33,paper_909,"For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation.",discussion,12,2,5,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,These results are consistent with the theoretical [8] and empirical [34] literature.,discussion,12,2,4,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,"For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation.",discussion,12,2,5,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,These results are consistent with the theoretical [8] and empirical [34] literature.,discussion,12,2,4,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,"For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation.",discussion,12,2,5,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,These results are consistent with the theoretical [8] and empirical [34] literature.,discussion,12,2,4,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b33,paper_909,"For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation.",discussion,12,2,5,"The findings show that job characteristics influence healthcare workers' happiness via two processes: affective (motivation) and behavioral (adaptive performance). That is, in line with the JCS of Hakman and Oldham [1,8], we show that autonomy, feedback, task variety, identity, and meaning are indeed motivating job characteristics that positively influence intrinsic motivation, that leads to more adaptive performance behaviors, making the individual feel better and happier. When the levels of these job characteristics are higher, workers' intrinsic motivation increases, making them more committed and engaged with work, leading to higher adaptive performance, and, as a result, happiness increases. These results are consistent with the theoretical [8] and empirical [34] literature. For example, Sonnentag et al. [34] showed that the same job characteristics were positively related to performance and satisfaction through intrinsic motivation. The JCM argues that these job characteristics are motivating and lead to positive outcomes because there is a positive effect of job characteristics on individuals' psychological states, such as the responsibility for the results to be achieved, or the relevance attributed by the individuals to their work and results. Accordingly, these characteristics are motivating once they lead to greater performance and well-being. Similarly, Millette and Gagné [43] showed that job characteristics were positively associated with intrinsic motivation, satisfaction, and performance. More recently, Zaman et al. [44] showed that job characteristics influenced motivation and, therefore, joy while working and performance. Therefore, we may conclude that autonomy, feedback, task variety, identity, and meaning influence healthcare workers' intrinsic motivation, which leads to more adaptive behaviors and in turn to increased Our findings also evidence that this serial mediation does not occur for extrinsic motivation. While some authors argued that some forms of extrinsic motivation would lead to positive outcomes, such as performance [15,25], others are in line with this result [1]. Hence, we may conclude that extrinsic motivation may be relevant to some outcomes (e.g., task performance) due to its inherent external rewards (e.g., salary, compensation) [45]; however, it is not sufficient to commit the individual to go further on his/her formal responsibilities and engage in adaptive behaviors that, in turn, result in happiness. Contrary to extrinsic motivation, some job characteristics make the individual take pleasure from",GOOD,0,0,NA,NA
b42,paper_912,"Due to the resource constraints as noted above, the small sample size of this study (n = 7) was justified (Lakens, 2022).",method,10,1,8,"The COVID-19 pandemic brought unprecedented changes to the agricultural industry in itself, even more so when it came to implementing this research. Pandemic restrictions were not conducive to in-depth learning and created barriers in recruiting and establishing relationships with the participants. The COVID-19 pandemic also forced many farmers into financial crisis and under undue stress. Four out of the seven farmers who participated had never used a digital agriculture app and had sporadic internet access. In the farm areas where the internet was sporadic, this made online training and general app usage difficult. Participants in north Surrey had better internet access and had more exposure to technology making their experience more streamlined. Under the COVID-19 pandemic conditions, the gaps in internet connectivity create inequitable results for using the app. Due to the resource constraints as noted above, the small sample size of this study (n = 7) was justified (Lakens, 2022). This small sample size was helpful as the study required the researchers to be on stand-by to address the specific concerns of each participant and troubleshoot for a period of 2 months. While the sample size can be considered relatively small. As noted by Davies and Dodd (2002) applying ""quantitative notions of rigor to qualitative research provides a poor instrument for evaluating qualitative research"". In a study on appropriate sample size for qualitative research, according to Boddy (2016), small sample size can be justified when it helps show direction for a future research field and it can be justified if it focuses on in depth-qualitative research. Moreover, other acceptable reasons for small sample size include the nature of the topic (Morse, 2000), and the amount of time spent with the participant (Marshall et al., 2013), which in the case of this study involved a novel use of an app and 2 months of trial. Finally, qualitative studies investigating theme saturation found that data saturation became evident at six individuals in studies that conduct qualitative in-depth interviews (Guest et al., 2006). As such, while the sample may seem small, the purpose and scope of this study justifies a sample of seven. Most importantly, another study on digital agriculture and the trial of a novel recording scheme also engaged with seven farms (Newton et al., 2020).",BAD,0.06169131,643,NA,NA
b22,paper_914,"As Lakens (2022) recently pointed out, there are many power analysis tools available, but learning to use them effectively takes time.",intro,1,2,4,"To address the challenges in finding a cost-efficient sample size while maintaining high statistical power, researchers can utilize power analysis tools to optimize their study designs. The mlpwr package provides a means to perform simulation-based power analysis for a broad class of applications (Zimmer & Debelak, in press). It fills two previously existing gaps in the literature by allowing for user-defined scenarios with multiple design parameters and explicitly accounting for the cost of study designs during the search algorithm. As Lakens (2022) recently pointed out, there are many power analysis tools available, but learning to use them effectively takes time. In response to this issue, we provide an introduction to the background and the applica-tion of the mlpwr package.",GOOD,0.032317675,331,NA,NA
b22,paper_914,"Although the sample size is still often only stated but not justified (Lakens, 2022), stating and justifying a sample size before conducting a study is arguably becoming common practice.",intro,3,1,10,"The recent replication crisis has put low statistical power and replicability of scientific research into focus (Button et al., 2013;Open Science Collaboration, 2015). Starting from the observation that most published research results might be wrong (Ioannidis, 2005;Simmons et al., 2011), there have been several developments to improve the replicability of scientific studies (Shrout & Rodgers, 2018). One of these are registered reports, in which research projects are reviewed and conditionally accepted based on sound methodology rather than on the statistical significance of the result. In registered reports, justification of sample size based on power analysis is usually mandatory. For example, in the journal Nature Human Behavior, the sample size should be large enough to achieve at least 95% statistical power (Nature Human Behaviour, 2022). Looking at recent developments, registered reports are indeed accompanied by more frequent justification of sample size (Soderberg et al., 2021). Another means to ensure replicability is through pre-registrations, where key properties of the planned research are fixed before data collection and statistical analyses. A study by Bakker et al. (2020) found that pre-registered stud-ies had larger sample sizes than earlier psychological studies. However, the study did not find that explicit recommendations for performing power analysis led to larger sample sizes. Although the sample size is still often only stated but not justified (Lakens, 2022), stating and justifying a sample size before conducting a study is arguably becoming common practice.",GOOD,0.057462664,497,NA,NA
b22,paper_914,"It is generally fast but sometimes unavailable, in particular for more complex or uncommon statistical hypothesis tests (Lakens, 2022).",intro,6,1,4,"Before we present our implementation to find optimal designs in these scenarios, we want to give an overview over power analysis methodology and implementations. Methodologically, two approaches can be distinguished for determining the power of a study. One is the analytical or formula-based approach. It is generally fast but sometimes unavailable, in particular for more complex or uncommon statistical hypothesis tests (Lakens, 2022). An alternative approach with higher availability but higher computational effort is the simulation-based approach. Both analytical and simulation-based approaches can be used in the presence of multiple design parameters.",GOOD,0.028703546,294,NA,NA
b22,paper_914,"For this reason, a common challenge is that analytical solutions are unavailable (Lakens, 2022).",intro,7,3,4,"The speed of analytical approaches makes them the first choice for simpler models. However, a slight change in the study design and the hypothesis test in question may require a different analytical treatment. This is because it can be difficult or even impossible to derive analytical formulas for more complex models (such as determining the power to test a random effect in a multilevel model, Cools et al., 2008). For this reason, a common challenge is that analytical solutions are unavailable (Lakens, 2022).",GOOD,0.028287793,286,NA,NA
b22,paper_914,"An important prerequisite for generating data and planning a study design in general is the determination of the expected size of the effect to be studied (Lakens, 2022).",method,16,4,3,"In most cases, the test of the hypothesis is more straightforward for applied researchers, as it is a standard use case of R packages and taught in many statistics courses. Generation of artificial data is however less often practiced and may be unfamiliar to applied researchers. An important prerequisite for generating data and planning a study design in general is the determination of the expected size of the effect to be studied (Lakens, 2022). There are many definitions of effect size that depend heavily on the statistical model used (e.g., Brysbaert & Stevens, 2018;Chalmers, 2022;Lorah, 2018). Options to determine an effect size include using the results of a meta-analysis, consulting with experts, or conducting a pilot study. To assist with data generation, many R packages offer special functions that can greatly help with this step. Examples are the simulate function in the lme4 package (Bates et al., 2015) and the simdata function in the mirt package (Chalmers, 2012). These functions can be used as a part of simulation functions to generate a data set with the desired study design parameters, such as a desired sample size. Because of this possibility, the mlpwr package is highly compatible with any existing artificial data generation functions in other packages and can be used in combination with them.",GOOD,0.042023953,461,NA,NA
b18,paper_915,"Pilot data, though not a requirement of Registered Reports, is especially useful to test and show reviewers that your planned design is feasible (pilot data is not typically useful for calculating power analyses as it introduces bias; see [18] for an explanation and [19] for recommendations).",intro,4,5,1,"Pilot data, though not a requirement of Registered Reports, is especially useful to test and show reviewers that your planned design is feasible (pilot data is not typically useful for calculating power analyses as it introduces bias; see [18] for an explanation and [19] for recommendations). As well as allowing you to check feasibility, pilot data may reveal unanticipated exclusion criteria for example, and will give you data to plan your data analysis steps in order to write your Stage 1 protocol and code (code is preferable to narrative explanations because it is more precise). If you are writing a meta-analysis, systematic review, or systematic map, you should pilot your searches to ensure that you will have sufficient studies included to provide a meaningful answer to your research question (s). This level of planning is one of the benefits of Registered Reports over vanilla preregistration; you do not just specify the topline design, but also detail all the processes and steps behind that design, so you won't end up wedded to a design that is infeasible in practice. Any research conducted prior to your Stage 1 submission should be noted as such (e.g., ""All steps in this search term identification section were completed prior to submitting the Stage 1 Registered Report"").",GOOD,0.039342742,436,NA,NA
b56,paper_917,"Final sample size was determined by the time allocated to the first author to collect the data (Lakens, 2022).",method,7,2,8,"sizes of raters varying from 9 to 2,733 per gender group (median = 58.5). None of these studies has attempted power calculations to estimate sample size. Moreover, in the current study, we intended to use multivariate techniques such as linear mixed effects modelling and multivariate multiple regression. To our knowledge, there is no precedent set to estimate statistical power using such techniques for multivariate outcomes such as body composition. While it is true that simulation techniques are available [see, e.g., (Kumle et al., 2021)], reliable simulations need reasonable estimates of the various sources of variance and covariance which we do not have. Therefore, our approach here is not to rely solely on inferential statistical methods that estimate probability, and which require adequate power. Instead, we will focus primarily on model building through testing changes in the Akaike information criterion (AIC; Akaike, 1973) to compare the adequacy of multiple models as a fit to our data (Burnham and Anderson, 2002;Wagenmakers and Farrell, 2004). Final sample size was determined by the time allocated to the first author to collect the data (Lakens, 2022).",BAD,0.046553592,432,NA,NA
b23,paper_919,For the study outcome to be clinically meaningful we have set the goal to detect or reject an arbitrary effect size of 0.25 (24).,method,10,1,3,"The main objective of this study is to assess if a Group x Time interaction effect exists for movement control of the spine, i.e., if movement control of the spine changes differently between groups over the course of the intervention. To the best of our knowledge, currently no studies have been performed in which spinal movement control, as defined in the current paper, is both trained and assessed before and after training, which complicates the estimation of an expected effect size. For the study outcome to be clinically meaningful we have set the goal to detect or reject an arbitrary effect size of 0.25 (24). In other words, if the effect size would be below 0.25, we would consider this result to be too small to be of interest. Differences in movement control of the spine in low back pain patients and healthy controls in terms of tracking error of a spinal movement controlled tracking task were reported by Willigenburg et al. (12). The tracking error in healthy controls was 0.332 degrees (SD 0.103) and in low back pain patients 0.422 degrees (SD 0.634), which is a large effect size (>0.8). No data about the expected effect of a sensor-based movement control intervention on these outcomes are available. However, a recent study from Matheve et al. demonstrated that low back pain patients can alter their movement behavior using a sensor-based intervention during a single session (14). Hence, we expect that low back pain patients will be able to perform equally well on movement control tasks by the end of the intervention as healthy controls without an intervention. If the static motor control group reaches 75% of the effect of the sensor-based movement control group, corresponding to an effect size of 0.25 (considering the effect of the intervention is equally large as the standard deviation of the effect), a total sample size of 54 (27 per group) would suffice to demonstrate a Group x Time interaction effect at a power of 95%. In case of a drop-out an additional patient will be recruited (with a maximum of 10 patients).",GOOD,0.059029578,608,NA,NA
b19,paper_920,"Our choice of power and error rate were based on common practice (Lakens 2022), but were ultimately arbitrary.",method,2,1,7,"With Department of Sport and Exercise Sciences ethics approval, 11 male university standard team sport players (age 20.0 ± 0.8 y, stature 181 ± 5 cm, body mass 82.3 ± 12.4 kg) participated in this study after providing written informed consent. Participants represented a range of team sports, including soccer, rugby and basketball. An a priori sample size calculation using G*Power 3. 1.9.6 (Faul et al. 2007) informed our sample recruitment. A sample size of nine was estimated to detect a one-tailed effect of d = 0.93 with a power of 80% and error rate of 5% using a paired-samples t test. The effect size of interest (d = 0.93) was estimated based on differences in maximal oxygen uptake for running and cycling in trained students (McArdle and Magel 1970), with the one-tailed option selected because of the systematically higher VO 2 values observed in running compared to cycling (Millet et al, 2009). Our choice of power and error rate were based on common practice (Lakens 2022), but were ultimately arbitrary. Twelve participants were initially recruited to account for participant drop out. Additional participants began testing before 9 complete sets of data had been collected-it was decided it was appropriate to complete the data collection for these additional participants. Participants first attended the laboratory on two separate occasions (temperature: 20.5 ± 1.1 compared to (cf.) 20.3 ± 1.1 °C, p = 0.518; humidity: 52.8 ± 3.3 cf. 52.1 ± 4.0%, p = 0.623; Pbar: 763 ± 11 cf. 764 ± 11 mmHg, p = 0.902; all ES < 0.2) completing incremental tests to exhaustion to establish speed (s VO 2max ) and power (p VO 2max ) corresponding to maximal oxygen uptake ( VO 2max ) during running (H/P Cosmos, Pulsar, Nussdorf-Traunstein, Germany) and cycling (Lode Excalibur Sport, Lode Medical Technology, Groningen, The Netherlands), respectively. The protocols started at 100 W (cycling) or 8 km h -1 with a 1% incline (running) and increased by 20 W min -1 (cycling) and speed by 0.5 km h -1 min -1 (running) until volitional exhaustion. Volitional exhaustion was defined as either the point at which participants could no longer maintain a cycling cadence of 50 rev min -1, or the speed of the treadmill. Expired air was collected continuously throughout each exhaustive trial using a pre-calibrated metabolic cart (Quark RMR, Cosmed, Cosmed.SRL, Italy). Oxygen uptake ( VO 2 ), was recorded breath-by-breath and later averaged over 30 s, with heart rate (HR) collected via telemetry (Garmin Premium HR, Garmin Ltd, Kansas, USA). VO 2max was accepted as the highest V O 2max averaged over 30 s.",GOOD,0.223640914,860,NA,NA
b20,paper_921,"Often, it is difficult to determine the effect size for a priori power analyses and so researchers can consider initially conducting a hypothesis-generating or exploratory study followed by a high-powered replication study that confirms the results (see more information about sample size justification and a priori power analyses in Lakens, 2022).",discussion,14,13,3,"In sum, the results of the p-curve analyses and the fact that there are many published reports with results that are not statistically significant should give everyone pause about the reliability and accuracy of results based on the emotional Stroop task in the eating disorders literature. Accordingly, there is a need for high-powered (i.e., at least 80%; Bakker et al., 2016) pre-registered research. Often, it is difficult to determine the effect size for a priori power analyses and so researchers can consider initially conducting a hypothesis-generating or exploratory study followed by a high-powered replication study that confirms the results (see more information about sample size justification and a priori power analyses in Lakens, 2022). The research conducted by Tabri and Palmer (2020) using the emotional Stroop task in the context of eating disorders provides an example of this approach with pre-registration.",GOOD,0.034024927,367,NA,NA
b23,paper_925,"The final data was collected from 582 (the sample size would provide sufficient power according to justifications for the sample size in a study (Daniël, 2022) and the priori sample size (n = 105) calculation conducted via Gpower [effect size was set at 0.2 (medium), α = 0.5, β = 0.95)] senior high school students in Henan, China in September 2021.",method,3,1,1,"The final data was collected from 582 (the sample size would provide sufficient power according to justifications for the sample size in a study (Daniël, 2022) and the priori sample size (n = 105) calculation conducted via Gpower [effect size was set at 0.2 (medium), α = 0.5, β = 0.95)] senior high school students in Henan, China in September 2021. The participants, with a mean age of 16.8 (SD = 0.9) years, included 289 girls (48.3%) and 310 boys (51.7%). Among these participants, 347 were from Grade 12, 176 from Grade 11, and 72 from Grade 10. The study was approved by the Ethics Committee of the institution and carried out following the approved guidelines and regulations. The written informed assent was obtained from the participants and the written informed consent was obtained from the student's guardians. And all participants were paid for their participation.",GOOD,0.043968954,398,NA,NA
b16,paper_926,"Psychophysics utilizes precise measurement techniques and focuses on effects that are generally large and stable across participants (e.g., Baker et al., 2018); thus, our sample sizes, which were no different from those in previous similar studies (e.g., Gheorghiu et al., 2016), were deemed adequate in this context (Lakens, 2022).",method,3,1,4,"In Experiment 1, a total of 21 participants were tested, but three of them were excluded due to poor overall performance (below 55%). This left 18 participants in the sample (two males, age range: 19-29 years, M = 22, SD = 2). In Experiment 2, there were 26 participants (eight males): 14 younger adults (age range: 20-27 years; M = 22, SD = 2) and 12 older adults (age range: 60-69 years; M = 65, SD = 3). Psychophysics utilizes precise measurement techniques and focuses on effects that are generally large and stable across participants (e.g., Baker et al., 2018); thus, our sample sizes, which were no different from those in previous similar studies (e.g., Gheorghiu et al., 2016), were deemed adequate in this context (Lakens, 2022).",GOOD,0.040337009,384,NA,NA
b22,paper_930,This sample size is deemed sufficient for an 11-item scale (23).,results,6,1,5,Internal consistency was evaluated for all items in the scale. The analysis yielded an alpha estimate of 0.84. A principalcomponents factor analysis was then conducted on the 11 items. One hundred and thirty-five first responder sample. This sample size is deemed sufficient for an 11-item scale (23).,BAD,0.033656461,245,NA,NA
b13,paper_932,"Across all three replication experiments, we chose sample sizes that would ensure high statistical power to detect not only effect sizes of the magnitude originally reported by Smith et al. (2019), but also to detect minimum effect sizes of interest (Lakens, 2022).",method,4,1,5,"Experiments 1 and 3 were conducted independently at Trent University in collaboration with the University of Waterloo and Bishop's University, while Experiment 2 was conducted by a group of researchers at Texas A&M University-Commerce in collaboration with the US Army Research Laboratory. After data collection was completed for Experiment 1, the two groups of researchers became aware of each others' mutual interest in replicating Smith et al.'s (2019) experiments. Both groups of researchers thus decided to conduct an international multiuniversity and pre-registered replication of Smith et al.'s experiments to better understand the true nature of the effect of posture on cognition. As a result, the reader will notice slight differences across the study protocols (e.g., whether demographic information was collected or not). Across all three replication experiments, we chose sample sizes that would ensure high statistical power to detect not only effect sizes of the magnitude originally reported by Smith et al. (2019), but also to detect minimum effect sizes of interest (Lakens, 2022). Table 1 summarizes the originally reported effect sizes of the Smith et al. experimental paradigms, as well as the minimum effect sizes of interest (in addition to their corresponding statistical power) for the current replication work. Statistical power was calculated for repeated trials (Goulet & Cousineau, 2019) using SuperPower (Lakens & Caldwell, 2021; Supplemental Materials on <https://osf.io/kwrjd>). The minimum effect sizes of interest in the replication experiments fall within the upper end of the small range. This is a fraction of the effect sizes originally reported in Smith et al., which were in the medium-to-large range. Consequently, the minimum effect sizes of interest provide lower boundaries for estimating statistical power.",GOOD,0.054226373,542,NA,NA
b64,paper_934,"Small sample sizes result in greater chance of type II error (Lakens, 2022).",discussion,24,3,2,"Our samples were also small, which limited the ability of our analysis to identify significant differences in depression symptoms scores between a number of groups. Small sample sizes result in greater chance of type II error (Lakens, 2022). This means that the visible difference in depression symptoms scores may in fact represent real effects despite not reaching statistical significance (see Wasserstein, Schirm, & Lazar, 2019). It is for this reason that we emphasize effect sizes.",GOOD,0.028033367,277,NA,NA
b26,paper_940,"However, post hoc power analysis has limitations: its outcome depends on the detected effect size, which may not reflect the true effect size in population [27].",method,10,1,4,"Due to the sampling plan, it was only possible to perform a post hoc power analysis. According to the power analysis conducted using the G*Power 3.1 software [26] (for the current single-group model of analysis, which included four variables, two-point measurements with the assumed average correlation between the measurements of 0.50, non-sphericity correction = 1.00, and alpha = 0.95), the current sample size of 85 participants had 0.44, 0.91, 0.99, and 1.00 power to detect an effect size of f = 0.10, 0.91, f = 0.18, f = 0.25, and f = 0.40, respectively. It was assumed that f = 0.10 indicated a small effect, f = 0.18 indicated a small-to-medium effect, f = 0.25 indicated a medium effect, and f = 0.40 indicated a large effect. However, post hoc power analysis has limitations: its outcome depends on the detected effect size, which may not reflect the true effect size in population [27].",GOOD,0.038645105,426,NA,NA
b35,paper_941,"We determined our effect size of interest based on Cohen's heuristic, which defines a medium effect size as something visible to the naked eye of the careful observer [34,35] and the guidelines proposed by Lakens [36] who suggest calculating an experimental sample size based on the minimum effect size of interest.",method,6,2,5,"The sample size was determined a priori, with a power analysis conducted with the ""pwr"" package on R [32,33]. To compute sample size, we assumed a power of 0.8, and a significance level of p = 0.05 (two-tailed). Effect size was set at r = 0.3, as this should indicate a medium effect size. We defined r = 0.3 as the smallest effect size of interest, in the context of this study. We determined our effect size of interest based on Cohen's heuristic, which defines a medium effect size as something visible to the naked eye of the careful observer [34,35] and the guidelines proposed by Lakens [36] who suggest calculating an experimental sample size based on the minimum effect size of interest. The results of our power analysis suggested a sample of 85 participants.",GOOD,0.033859208,362,NA,NA
b42,paper_943,Simulation-based power analyses (43) for hypotheses H1a through H1g (19) and H3a through H3g (20) were conducted after data collection but before data was accessed.,method,20,1,1,"Simulation-based power analyses (43) for hypotheses H1a through H1g (19) and H3a through H3g (20) were conducted after data collection but before data was accessed. The simulation-based approach was chosen due to the absence of parameters in the literature on COVID-19 related changes in solitude. In order to obtain the power estimates for hypotheses H1a through H1g, data were used from 1167 participants who provided ESM data when alone at T0 but did not re-enter the study at T1. Similarly, data from 1026 participants who provided ESM data when alone at T0 but did not re-enter the study at T2 were used to obtain the power estimates for hypotheses H3a through H3g. For all hypotheses, data were simulated based on a range of plausible increases or decreases from T0 to T1 or T2 according to the direction of the hypothesis. For H1b through H1g and H3b through H3g, data were simulated for increases or decreases of 1%, 5%, 10%, 15% and 20%. For H1a and H3a, increases of 25%, 30%, 35%, 40%, 45% and 50% were added based on previous reports of sharp decreases in adolescent social interactions from T0 to T1 in the same sample (2). For every effect size, 1000 datasets were simulated and analysed with linear regression models (H1a and H3a) or linear mixedeffects regression models (H1b through H1g and H3b through H3g). The proportion of datasets in which each null hypothesis was rejected at α < .007 (Bonferroni-corrected, initial α = .05) was taken as the power of a specific effect size. For H1a, there was sufficient power to detect increases of 40% and above. For H1b through H1g, there was sufficient power to detect increases or decreases of 5% and above. For H3A, there was sufficient power to detect increases of 30% and above. For H3b through H3g, there was sufficient power to detect in-or decreases of 5% and above.",GOOD,0.334715657,649,NA,NA
b48,paper_945,"Women have been shown to vary regarding learning and memory in general and sleep-dependent consolidation specifically across the menstrual cycle 46 and the high time intensity of sleep research constrains the possibility of increasing power by increasing the sample size 47,48 .",method,2,1,3,"Participants. Twenty-five healthy young men aged between 18 and 30 years (24.20 ± 3.53) took part in the study. Women have been shown to vary regarding learning and memory in general and sleep-dependent consolidation specifically across the menstrual cycle 46 and the high time intensity of sleep research constrains the possibility of increasing power by increasing the sample size 47,48 . Therefore, men were chosen for this study to reduce the variance in the sample, which optimised statistical power under these constraints 49 . Notably, simply adding twenty-five women to our sample would not enable us to robustly detect differences between men and women since this would be a between-within as well as a higher order interaction effect that would most likely require many more participants to detect 50 . Participants were non-smokers, fluent in English, not currently under medication and did not have any physical or mental disorders. They all reported having a regular sleep schedule, going to bed before midnight (11:18 p.m. ± 47 min) and waking up before 8:00 a.m. (7:41 a.m. ± 47 min). In addition, participants did not work during night shifts and were not diagnosed with sleep disorders and they did not travel across time zones. Finally, they did not report any stressful events such as exams or deadlines before or during the experiment. The experiment was approved by the UCL ethics committee (ID number: 8951/002) and all research was performed in accordance with relevant guidelines and regulations. Written informed consent was obtained from each participant before starting the experiment. Participants were compensated financially for their participation.",BAD,0.058542425,516,NA,NA
b13,paper_947,"Finally, Lakens (2022) claims that one of the methods for determining the sample size consists in planning for results precision.",intro,1,11,1,"Finally, Lakens (2022) claims that one of the methods for determining the sample size consists in planning for results precision. In more detail, the sample size based on precision requires the collection of data to achieve the desired width of the confidence interval around the parameter estimate. This decision should be taken considering previous experimental results and the nature of the test itself. This approach will be considered in this study, taking into account the particular characteristics of BMAT results.",GOOD,0.027639659,276,NA,NA
b48,paper_957,"Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51].",method,17,1,2,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0.063484467,564,NA,NA
b48,paper_957,"It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51].",method,17,1,3,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α).",method,17,1,5,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51].",method,17,1,8,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51].",method,17,1,2,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51].",method,17,1,3,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α).",method,17,1,5,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51].",method,17,1,8,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51].",method,17,1,2,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51].",method,17,1,3,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α).",method,17,1,5,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51].",method,17,1,8,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51].",method,17,1,2,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51].",method,17,1,3,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α).",method,17,1,5,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b48,paper_957,"However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51].",method,17,1,8,"Given the well-known challenges in pre-screening and recruiting people with subclinical ED symptomatology in the healthy population, a compromise power analysis was performed to determine the minimum number of participants that would generate reliable ndings. Compromise power analysis represents a novel concept in statistics, which can be determined using the G*Power software package [49][50][51]. It is applicable in uncontrollable situations (e.g., working with samples affected by diseases and/or attrition), or when N is too small to satisfy conventional levels of alpha (α) and beta (β) (1-power) [49][50][51]. As both were the case in our situation, we used this method to determine the number of participants we would need for our study, as well as for the study's statistical power. Following Lakens [50] recommendations, besides justifying sample restrictions, a compromise power analysis should also report a justi cation of the expected effect size and the desired ratio of Type I and Type II errors (q= β/α). Regarding the effect size, based on previous literature on the topic (i.e., [15,52], we expected moderate and large effect sizes for the power analysis described below. On the other hand, it is commonly assumed that the error ratio (q) is equal to four; considering most studies are designed to have 80% power (β equal to 0.20) and α < 0.05 [53][54][55][56]. However, in compromise power analysis is preferably to set q = 1, balancing an equal Type I and Type II error risks (β/α) since both are considered equally serious [49][50][51]. As Faul et al. [49] noted, the bene t of balanced Type I and Type II error risks often offsets the costs of violating signi cance level conventions.",GOOD,0,0,NA,NA
b64,paper_965,"Thus, we recruited as many participants as possible until the available subject pool was exhausted [63].",intro,12,3,8,"We predicted that participants in the gratitude conditions would report higher levels of gratitude, connectedness, elevation, humility, and improvement motivation, as well as lower levels of general negative affect (e.g., anger and frustration) relative to controls. We also hypothesized that expressing gratitude may give rise to specific negative socially oriented emotions such as feelings of indebtedness, guilt, embarrassment, shame, and discomfort. Employees from four French companies and one French-Canadian company completed Study 1 online (N = 224, 71.4% female, M age = 37.17, SD = 9.08). Most participants had completed college (46.4%), with 21.0% of participants having completed graduate school, 20.5% having completed some college, 7.1% having completed high school, and 4.9% of participants having not completed high school. Information about ethnicity was not collected due to regulations from the Commission Nationale Informatiques et Libertés (CNIL). We aimed to recruit 100 participants per condition [62]. However, our sample was composed of busy working adults who are notoriously difficult to enlist, and we could not reach this target. Thus, we recruited as many participants as possible until the available subject pool was exhausted [63]. No participants were excluded, and we did not conduct analyses until the final sample size was collected. We performed a sensitivity power analysis using GPower [64]. Because we had a directional a priori hypothesis, we used a one-tailed test. The power analysis indicated that the smallest effect size that this sample could detect with 80% power was Cohen's d = 0.38 (equivalent to Cohen's f = 0.19). The study was translated into French by a professional translation service (Gengo), and then reviewed by several native French speakers who made minor adjustments.",BAD,0.058711853,562,NA,NA
b38,paper_966,"First, unless the meta-analysis is only comprised of Registered Report studies (Chambers & Tzavella, 2021) it is very likely that the observed summary effect size is inflated due to publication bias (Ioannidis, 2008;6 Kvarven et al., 2020;Lakens, 2022;Schäfer & Schwarz, 2019).",intro,1,5,2,"Despite the utility of sunset plots there are some limitations associated with a single effect size approach. First, unless the meta-analysis is only comprised of Registered Report studies (Chambers & Tzavella, 2021) it is very likely that the observed summary effect size is inflated due to publication bias (Ioannidis, 2008;6 Kvarven et al., 2020;Lakens, 2022;Schäfer & Schwarz, 2019). Using Jacob Cohen's suggested threshold levels for a small/medium/large effect as an alternative should be avoided as a first option if possible as these thresholds were only suggested as fallback for when the effect size distribution is unknown (Cohen, 1988). What actually constitutes a small/medium/large effect differs according to subfield (e.g., Gignac & Szodorai, 2016;Quintana, 2016), study population/context (Kraft, 2020), and is also likely to be influenced by publication bias (Nordahl-Hansen et al., 2022).",GOOD,0.037916878,406,NA,NA
b38,paper_966,"thresholds are essentially moot points as the true effect size is unknown when testing hypotheses (Lakens, 2022).",intro,2,1,1,"thresholds are essentially moot points as the true effect size is unknown when testing hypotheses (Lakens, 2022). Alternatively, researchers can determine the range of effect sizes a study design can reliably detect and evaluate whether this range includes meaningful effect sizes. In most cases, determining what constitutes a meaningful effect is not a straightforward task, as typical effect sizes vary from fieldto-field and researchers can understandably draw different conclusions. Presenting statistical power assuming a range of true effect sizes, instead of a single true effect size, allows readers to transparently evaluate study-level statistical power according to their own assumptions and what is reasonable for a given research field.",GOOD,0.030986366,312,NA,NA
b38,paper_966,"Indeed, a future meta-analysis has been proposed as potential justification for performing studies including small samples due to resource limitations, such as undergraduate student research projects or when collecting data from rare populations (Lakens, 2022;Quintana, 2021).",intro,12,1,2,"One advantage of meta-analysis is that while individual included studies may not have sufficient statistical power to reliably detect a wide range of effect sizes, the synthesis of several of these studies into a summary effect size can increase statistical power. Indeed, a future meta-analysis has been proposed as potential justification for performing studies including small samples due to resource limitations, such as undergraduate student research projects or when collecting data from rare populations (Lakens, 2022;Quintana, 2021). However, under typical circumstances for the psychological sciences, meta-analysis is not a straightforward remedy for synthesizing underpowered studies, especially those that can only reliably detect large effect sizes, as increases in overall power via meta-analysis may be modest.",GOOD,0.031027705,327,NA,NA
b38,paper_966,"Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022).",intro,14,1,3,"The main purpose of the metameta package is to determine the range of effect sizes that can be reliably detected for a body of studies. This tutorial used an 80% power criterion to determine reliability, however, other power levels can be used when justified. Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022). While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022). For example, when working with rare study populations or when collecting observations is expensive, it can be difficult to design studies that can detect small effect sizes due to resource limitations as the use of large sample sizes is unrealistic in these cases. Alternatively, in other situations, error rates less than 20% are warranted or more realistic. A benefit of the metameta package is that by presenting power for a range of effects, the reader judge what they consider to be appropriate power based on the research question at hand and the available resources.",GOOD,0.044290667,456,NA,NA
b38,paper_966,"While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022).",intro,14,1,4,"The main purpose of the metameta package is to determine the range of effect sizes that can be reliably detected for a body of studies. This tutorial used an 80% power criterion to determine reliability, however, other power levels can be used when justified. Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022). While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022). For example, when working with rare study populations or when collecting observations is expensive, it can be difficult to design studies that can detect small effect sizes due to resource limitations as the use of large sample sizes is unrealistic in these cases. Alternatively, in other situations, error rates less than 20% are warranted or more realistic. A benefit of the metameta package is that by presenting power for a range of effects, the reader judge what they consider to be appropriate power based on the research question at hand and the available resources.",GOOD,0,0,NA,NA
b38,paper_966,"Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022).",intro,14,1,3,"The main purpose of the metameta package is to determine the range of effect sizes that can be reliably detected for a body of studies. This tutorial used an 80% power criterion to determine reliability, however, other power levels can be used when justified. Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022). While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022). For example, when working with rare study populations or when collecting observations is expensive, it can be difficult to design studies that can detect small effect sizes due to resource limitations as the use of large sample sizes is unrealistic in these cases. Alternatively, in other situations, error rates less than 20% are warranted or more realistic. A benefit of the metameta package is that by presenting power for a range of effects, the reader judge what they consider to be appropriate power based on the research question at hand and the available resources.",GOOD,0,0,NA,NA
b38,paper_966,"While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022).",intro,14,1,4,"The main purpose of the metameta package is to determine the range of effect sizes that can be reliably detected for a body of studies. This tutorial used an 80% power criterion to determine reliability, however, other power levels can be used when justified. Indeed, the 80% power convention does not have a strong empirical basis, but rather, reflected the personal preference of Jacob Cohen (Cohen, 1988;Lakens, 2022). While a 20% Type II error rate (i.e., 80% statistical power) can be a good starting point judging the evidential value of a study, or body of studies, one should consider whether other Type II error rates for the research question at hand are more appropriate (Lakens, 2022;Maier & Lakens, 2022). For example, when working with rare study populations or when collecting observations is expensive, it can be difficult to design studies that can detect small effect sizes due to resource limitations as the use of large sample sizes is unrealistic in these cases. Alternatively, in other situations, error rates less than 20% are warranted or more realistic. A benefit of the metameta package is that by presenting power for a range of effects, the reader judge what they consider to be appropriate power based on the research question at hand and the available resources.",GOOD,0,0,NA,NA
b57,paper_969,"Justifying the choice of sample sizes is essential in an adequate study design as it provides transparency about the criterion used to determine when data collection is completed 58,59 .",method,7,1,2,"The sample size (i.e., the final number of individuals included in a study) should be determined in the study design phase as it is crucial for answering the research question. Justifying the choice of sample sizes is essential in an adequate study design as it provides transparency about the criterion used to determine when data collection is completed 58,59 . Ideally, justifications should account for potential drop-out and data censoring (e.g., due to compromised/missing data or behavioral performance). Data drop-out/censoring may be considerable for fNIRS research, depending on the context (e.g., for clinical or infant studies).",GOOD,0.030291767,310,NA,NA
b57,paper_969,"In reality, however, sampling plans may often be determined by external factors such as resource constraints (e.g., time, funding or sample characteristics), which reflect another, albeit limited justification for sample size 58 .",method,7,2,3,"Transparent sample size justifications are pivotal to allow evaluating the precision and hence robustness of effects that a study is trying to detect. Different justification procedures exist: conducting an a priori power or sensitivity analysis is conventionally recommended as best practice for confirmatory research when employing inferential statistics 3 . In reality, however, sampling plans may often be determined by external factors such as resource constraints (e.g., time, funding or sample characteristics), which reflect another, albeit limited justification for sample size 58 . Often resource constraints and heuristics result in too small sample sizes, leading to lower statistical power and thus over or underestimation of true effects due to imprecision in the outcome estimate. This is why in particular small studies are more likely to yield outcome estimates that are not as reliable and should thus not be a basis for future sample size calculations 60 .",BAD,0.051254419,357,NA,NA
b57,paper_969,"Alternatively, an effect size of interest without prior empirical results could be also the smallest effect size of interest (SESOI), which expresses the smallest relevant effect the researchers care about (e.g., clinical significance, theoretical implication 58 ).",method,7,9,1,"Alternatively, an effect size of interest without prior empirical results could be also the smallest effect size of interest (SESOI), which expresses the smallest relevant effect the researchers care about (e.g., clinical significance, theoretical implication 58 ). Researchers should justify their choice of SESOI and may use anchors specific to their research question. For instance, when powering for group or condition differences, other measures (e.g., clinical or behavioral) can be used as an anchor for a SESOI. To provide an example from clinical research, it has for instance been estimated that improvements in depressive symptoms may need to exceed a standardized mean difference (SMD) of at least 0.24 to be considered a clinically meaningful change 68 . If the goal of fNIRS recordings in the context of a depression trial is to detect accompanying hemodynamic correlates as a potential biomarker that can indicate clinically relevant improvements of depressive symptoms, the SESOI for hypothesized hemodynamic changes assessed by fNIRS should be at least SMD = 0.24 to be sufficiently sensitive and thus informative. An alternative anchor for SESOIs of hemodynamic effects may be effect sizes that are observed for behavioral effects. For instance, rates of anticipated non-responders (e.g., percentages of successful trials) can inform the lower effect size boundary. By using sampling plans that are based on SESOIs it is more likely to detect meaningful effects. Moreover, in the case of non-significant/inconclusive results, they are more likely to yield evidence for the absence of an effect through follow-up equivalence tests (or their Bayesian equivalent) that allow rejecting the alternative hypothesis 46,69,70 . In the context of hemodynamic brain imaging, such equivalence tests have been recently discussed for fMRI applications 71 .",GOOD,0.054317834,542,NA,NA
b22,paper_974,Our sample size was imposed by resource constraints [23].,method,3,1,2,"A total of 45 younger adults (mean age = 26.38, SD = 4.44) and 45 community-dwelling older adults (mean age = 65.67, SD = 7.36) from Chieti and Pescara (Italy) took part in the study. Our sample size was imposed by resource constraints [23]. Before beginning the experimental session, all participants completed the forward and backward digit span of the Wechsler Adult Intelligence Scale-Revised (WAIS-R) [24], the Positive and Negative Affective Scale (PANAS) [25] to assess current mood, and the FAS [26] for verbal fluency. Older adults also completed the Mini Mental State Examination (MMSE) [27] to screen for general cognitive abilities. Neuropsychological and demographic data are reported in Table 1. Older and younger adults differed for years of education and verbal fluency. However, it should be noted that this is very typical of the older Italian population, and MMSE scores confirmed that older adults did not show significant cognitive impairments. All participants reported being in good physical and mental health, had no known memory deficits, and normal or corrected-to-normal visual acuity. Exclusion criteria included history of severe head trauma, stroke, neurological disease, severe medical illness, or alcohol or substance abuse. The study was approved by the Department of Psychological, Health and Territorial Sciences ethic committee and all participants gave written informed consent in accordance with the Declaration of Helsinki [28] prior to inclusion in the study. Note: Positive and Negative Affect Schedule [25]; Digit Span-Forward and the Digit Span-Backward [24]; FAS [26]; and Mini Mental State Examination [27]. *** = p < 0.001.",BAD,0.060129383,549,NA,NA
b50,paper_985,"Other than apriori power analysis, these can include other types of justifications such as accuracy or resource constraints 50 .",intro,16,1,3,"Justify sample size. Sample size justification statements are critical for judging the informational value of a study. Other than apriori power analysis, these can include other types of justifications such as accuracy or resource constraints 50 . McKay et al. 51 provide an example of sample size justification where the sample size was based on a prior study, with adjustments to the p-value for sequential analysis. In light of steps 3 and 4 that raise the need for initial characterization of long-term studies with multiple groups, it is critical that these studies also have sufficient sample sizes for reliable estimation of effect sizes.",GOOD,0.031486917,305,NA,NA
b27,paper_989,"Nevertheless, future research should include more households to overcome the limitation associated with small sample sizes and preferably conduct a priori power analyses to ascertain that the effect sizes of interest can be detected with sufficient statistical power given the set target sample size (Lakens, 2022).",discussion,12,1,8,"The current work offers a novel perspective on the role of technology-induced social norms in supporting societal sustainable development, providing several interesting avenues for future research. Admittedly, however, the study is not without limitations. For example, while the longitudinal sample size is comparable with other similar studies (e.g., Skarin et al., 2021;Shavit et al., 2021), the relatively few data points in the second wave of data collection limit the ability to identify small and moderate effect sizes with sufficient statistical power. Unfortunately, the small sample size in the second wave of data collection was impossible to fully circumvent, as participants were free to choose whether to complete the follow-up survey. Still, considering that the surveys were distributed several months apart and that the response rate during the first wave of data collection was not substantially larger than that of the second wave of data collection, the results should reasonably have some predictive validity. Indeed, of the 153 households who could download the app and reply to the initial survey, 59 households (38.56%) decided to do so, corresponding to 66 residents as some households had more than one adult who replied to the initial survey. Subsequently, 18 of those 66 residents (27.27%) also completed the follow-up survey, with these response rates being common in studies relying on similar designs (Kirkwood and Walton, 2014;Siegrist et al., 2015). Nevertheless, future research should include more households to overcome the limitation associated with small sample sizes and preferably conduct a priori power analyses to ascertain that the effect sizes of interest can be detected with sufficient statistical power given the set target sample size (Lakens, 2022). Moreover, as no mediating or moderating variables were documented in the current study, scholars addressing similar topics should optimally try to elucidate the specific psychological mechanisms driving the obtained effects, while simultaneously examining the replicability, generalizability, and boundary conditions of the current results. For example, several studies have shown that certain individual-differences factors may shape individuals' sustainability-related responses (Luchs and Mooradian, 2012;Folwarczny and Otterbring, 2021;Vizcaíno et al., 2021) as well as the extent to which they are influenced by social norms (Cialdini et al., 1999;Perugini and Gallucci, 2001;Stankov, 2011). Thus, future research should preferably include measures related to, for example, people's personality traits, cultural values, and the importance put on environmental protection to shed further light on whether, when, and why social norms may foster sustainable responses with a clear social connotation. As such, while this empirical investigation constitutes the first preliminary step for examining the theorizing proposed herein, more research is needed to gain a deeper understanding of the studied outcomes and build a nomological network of cumulative evidence.",GOOD,0.073533317,784,NA,NA
b28,paper_995,"Sampling was constrained by time but rule of thumb was applied to judge the adequacy of the sample size (Wilson Van Voorhis and Morgan, 2007;Lakens, 2022).",method,3,2,3,"Participants and procedure. The target sample size was 300 participants, to increase representation across the populations reached and to ensure the power of the anticipated regression analysis. Sampling was constrained by time but rule of thumb was applied to judge the adequacy of the sample size (Wilson Van Voorhis and Morgan, 2007;Lakens, 2022). A total of 402 individuals responded to the survey, however, 82 were excluded for failing to complete the first questionnaire (APQ), leaving a final sample size of 320 participants.",BAD,0.036371906,292,NA,NA
