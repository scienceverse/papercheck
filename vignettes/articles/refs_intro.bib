@article{aczel2018quantifying,
  title={Quantifying support for the null hypothesis in psychology: An empirical investigation},
  author={Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F and van den Bergh, Don and Wagenmakers, Eric-Jan},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={1},
  number={3},
  pages={357--366},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{goos2024assessing,
  title={Assessing Reliable and Valid Measurement as a Prerequisite for Informative Replications in Psychology},
  author={Goos, Cas and Bakker, Marjan and Wicherts, Jelte M and Nuijten, Mich{\`e}le B},
  year={2024},
  publisher={OSF}
}


@article{tendeiro2024diagnosing,
  title={Diagnosing the misuse of the Bayes Factor in applied research},
  author={Tendeiro, Jorge N and Kiers, Henk AL and Hoekstra, Rink and Wong, Tsz Keung and Morey, Richard D},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={7},
  number={1},
  pages={25152459231213371},
  year={2024},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{dexter2017narrative,
  title={Narrative review of statistical reporting checklists, mandatory statistical editing, and rectifying common problems in the reporting of scientific articles},
  author={Dexter, Franklin and Shafer, Steven L},
  journal={Anesthesia \& Analgesia},
  volume={124},
  number={3},
  pages={943--947},
  year={2017},
  publisher={LWW}
}


@article{thibault2024evaluation,
  title={An evaluation of reproducibility and errors in published sample size calculations performed using G* Power},
  author={Thibault, Robert T and Zavalis, Emmanuel A and Mali{\v{c}}ki, Mario and Pedder, Hugo},
  journal={medRxiv},
  pages={2024--07},
  year={2024},
  publisher={Cold Spring Harbor Laboratory Press}
}


@article{van2024potential,
  title={The potential of preregistration in psychology: Assessing preregistration producibility and preregistration-study consistency.},
  author={van den Akker, Olmo R and Bakker, Marjan and van Assen, Marcel ALM and Pennington, Charlotte R and Verweij, Leone and Elsherif, Mahmoud M and Claesen, Aline and Gaillard, Stefan DM and Yeung, Siu Kit and Frankenberger, Jan-Luca and others},
  journal={Psychological Methods},
  year={2024},
  publisher={American Psychological Association}
}


@misc{GROBID,
    title = {GROBID},
    howpublished = {\url{https://github.com/kermitt2/grobid}},
    publisher = {GitHub},
    year = {2008--2025},
    archivePrefix = {swh},
    eprint = {1:dir:dab86b296e3c3216e2241968f0d63b68e8209d3c}
}

@article{lakens_justification_2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  editor = {van Ravenzwaaij, Don},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2025-06-10},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/cristian/Zotero/storage/JBVP3ZHF/Lakens - 2022 - Sample Size Justification.pdf}
}


@article{wilkinson_fair,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {1--9},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  urldate = {2021-11-05},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders---representing academia, industry, funding agencies, and scholarly publishers---have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {/Users/cristian/Zotero/storage/UHWCWVPU/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;/Users/cristian/Zotero/storage/JJBXHHJ5/sdata201618.html}
}

@article{sharpe_transparency_2024,
  title = {Editor Bias and Transparency in Psychology's Open Science Era},
  author = {Sharpe, Donald},
  year = {2024},
  journal = {American Psychologist},
  volume = {79},
  number = {7},
  pages = {883--892},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/amp0001224},
  abstract = {In this open science era, psychology demands researchers be transparent in their research practices. In turn, researchers might ask if journal editors are being equally transparent in their editorial practices. Editor bias is when editors fail to be fair and impartial in their handling of articles. Editor bias can arise because of identity---who authors are---or because of content---what authors write. Proposed solutions to editor bias include masking author identity and increasing editor diversity. What is needed is greater transparency. By being more transparent, editors would be in a better position to encourage others to embrace open science. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Ethics,Open Science,Psychology,Publication Bias,Scientific Communication},
  file = {/Users/cristian/Zotero/storage/5HF8NPXJ/2025-32402-001.html}
}




@article{
  sharpe_why_2013,
  title={Why the resistance to statistical innovations? Bridging the communication gap.},
  volume={18},
  ISSN={1939-1463, 1082-989X},
  url={http://doi.apa.org/getdoi.cfm?doi=10.1037/a0034177},
  DOI={10.1037/a0034177},
  number={4},
  journal={Psychological Methods},
  author={Sharpe, Donald},
  year={2013},
pages={572â€“582} }
