<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Introducing Papercheck • papercheck</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Open_Sans-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Fira_Code-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introducing Papercheck">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-160147578-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160147578-1');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">papercheck</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Unreleased version">0.0.0.9053</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/papercheck.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item"><a class="nav-link" href="../articles/index.html">Articles</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/scienceverse/papercheck/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Introducing Papercheck</h1>
            <h3 data-toc-skip class="subtitle">An Automated Tool to
Check for Best Practices in Scientific Articles</h3>
            
      
      <small class="dont-index">Source: <a href="https://github.com/scienceverse/papercheck/blob/HEAD/vignettes/articles/intro.Rmd" class="external-link"><code>vignettes/articles/intro.Rmd</code></a></small>
      <div class="d-none name"><code>intro.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="the-problem">The Problem<a class="anchor" aria-label="anchor" href="#the-problem"></a>
</h2>
<p>Metascientific research reveals substantial opportunities to improve
how scientists design studies, report results, and implement open
science practices. For example, researchers often use invalid and
unreliable measures <span class="citation">(Goos et al. 2024)</span>,
misinterpret non-significant results <span class="citation">(Aczel et
al. 2018)</span>, write insufficiently specific preregistrations <span class="citation">(Akker et al. 2024)</span>, make mistakes in power
analyses <span class="citation">(Thibault et al. 2024)</span>, and
misuse Bayes factors <span class="citation">(Tendeiro et al.
2024)</span>.</p>
<p>Despite the growing availability of best practices, their adoption
remains slow <span class="citation">(Sharpe 2013)</span>. While
scientists are responsible for staying informed, time constraints and
the rapid pace of new developments pose significant challenges. Peer
reviewers face similarly have a lack of time, and may overlook the
absence of best practices. Checklists have been proposed to promote
adherence, but in practice they have limited impact <span class="citation">(Dexter and Shafer 2017)</span>, as researchers might
be unaware of them, and even when used, evaluating adherence still
requires considerable expertise.</p>
</div>
<div class="section level2">
<h2 id="a-solution">A Solution<a class="anchor" aria-label="anchor" href="#a-solution"></a>
</h2>
<p>Human factors research suggests that automation, where researchers
stay in the loop, can offer a partial solution to automatically check if
best practices are followed. For example, algorithms could detect
passages that describe an a priori power analysis, and whether
researchers check whether it is fully reported and specifies the alpha
level, desired power, the effect size metric, and a justification for
the effect size <span class="citation">(Lakens 2022)</span>.</p>
<p>Algorithms are also useful for automating straightforward but time
consuming tasks. For example, a reviewer could manually check if all
p-values in a manuscript are reported exactly (e.g., p = 0.007 instead
of p &lt; .05), and check whether authors unknowingly cite retracted
papers . However, these tasks can be easily automated (as is for example
done by the reference manager <a href="https://www.zotero.org/blog/retracted-item-notifications/" class="external-link">Zotero</a>).</p>
<p>Recent progress in machine learning and artificial intelligence has
made it increasingly viable to implement automated checks with
sufficient accuracy, supporting broader adoption and adherence to best
practices in scientific manuscripts. For example, the GROBID machine
learning library <span class="citation">(<span>“GROBID”</span>
2008--2025)</span> can turn scientific PDFs into structured text files
from which specific content can be extracted. While many automated
checks for best practices will require only simple text matching based
on regular expression, more complex text extraction is possible by using
large language models (LLMs).</p>
<p>As a team, we have a specific philosophy on how to create useful
automated checks. First, automated checks should make users aware of a
potential points of improvement, and can point to sources to learn how
to implement improvements, but the user decides whether and how to
incorporate suggestions. Second, algorithms should be validated against
community-annotated ground truth where possible, and the error rates
should be transparently communicated. We use open software and tools,
and as much as possible, open data. For each research practice to detect
or check, domain experts should establish what best practices are, and
automated checks should continually be revised and updated based on
criticism. Any use of AI must be minimal, optional, and based on open
source tools that guarantee data privacy. As a matter of principle, we
start the development modules without the use of LLMs. Finally, the
creation of modules should be open to the entire scientific community,
such that any domain expert who wants to contribute an automated check
can easily do so.</p>
</div>
<div class="section level2">
<h2 id="papercheck">PaperCheck<a class="anchor" aria-label="anchor" href="#papercheck"></a>
</h2>
<p><img src="https://scienceverse.github.io/papercheck/logo.png" style="float:right; width: 10em;" fig-alt="Papercheck Logo, a green hexagon with PAPAERCHECK in art deco font"></p>
<p>In this post, we introduce <a href="https://scienceverse.github.io/papercheck/">Papercheck</a>, a new
tool that leverages text search, code, and large language models to
extract and supplement information from scientific documents (including
manuscripts, submitted or published articles, or preregistration
documents) and provides automated suggestions for improvement.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/scienceverse/papercheck" class="external-link">papercheck</a></span><span class="op">)</span></span></code></pre></div>
<p>Inspired by practices in software development, where automated checks
(e.g., CRAN checks for R packages) are used to identify issues before
release, Papercheck aims to screen scientific manuscripts to identify
potential issues or areas for improvement and guide researchers in
adopting best practices.</p>
<p>Papercheck is modular, with each module focused on a specific
practice that the scientific community wants to improve, and for which
automation is viable. This modularity allows the tool to be extended and
customized by the community, enabling the development of specialized
modules tailored to the standards and best practices of different
scientific fields. It can also offer an overarching platform to
integrate existing tools (e.g., checks for retracted articles,
Statcheck, etc).</p>
</div>
<div class="section level2 callout-note">
<h2 id="an-example-of-the-retractionwatch-module">An example of the “retractionwatch” module<a class="anchor" aria-label="anchor" href="#an-example-of-the-retractionwatch-module"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paper</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/read.html">read</a></span><span class="op">(</span><span class="fu"><a href="../reference/demoxml.html">demoxml</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/module_run.html">module_run</a></span><span class="op">(</span><span class="va">paper</span>, <span class="st">"retractionwatch"</span><span class="op">)</span></span></code></pre></div>
<p>You cited some papers in the Retraction Watch database (as of
2025-05-20). These may be retracted, have corrections, or expressions of
concern.</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="1%">
<col width="23%">
<col width="5%">
<col width="1%">
<col width="16%">
<col width="5%">
<col width="1%">
<col width="5%">
<col width="3%">
<col width="1%">
<col width="6%">
<col width="23%">
<col width="1%">
<col width="0%">
<col width="0%">
<col width="0%">
</colgroup>
<thead><tr class="header">
<th align="left">xref_id</th>
<th align="left">ref</th>
<th align="left">doi</th>
<th align="left">bibtype</th>
<th align="left">title</th>
<th align="left">journal</th>
<th align="left">year</th>
<th align="left">authors</th>
<th align="left">retractionwatch</th>
<th align="left">type</th>
<th align="left">contents</th>
<th align="left">text</th>
<th align="left">section</th>
<th align="right">div</th>
<th align="right">p</th>
<th align="right">s</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">b0</td>
<td align="left">Gino F, Wiltermuth SS (2014). “Retracted: Evil Genius?
How Dishonesty Can Lead to Greater Creativ…</td>
<td align="left">10.1177/0956797614520714</td>
<td align="left">Article</td>
<td align="left">Retracted: Evil Genius? How Dishonesty Can Lead to
Greater Creativity</td>
<td align="left">Psychological Science</td>
<td align="left">2014</td>
<td align="left">F Gino, S S Wiltermuth</td>
<td align="left">Retraction</td>
<td align="left">bibr</td>
<td align="left">(Gino &amp; Wiltermuth, 2014)</td>
<td align="left">Although intentional dishonestly might be a successful
way to boost creativity (Gino &amp; Wiltermuth…</td>
<td align="left">intro</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr></tbody>
</table>
<p>Showing 1 of 1 rows</p>
</div>
<p>In addition to the initial modules already introduced on the <a href="https://scienceverse.github.io/papercheck/">Papercheck
website</a>, we will demonstrate Papercheck’s capabilities through four
practical scenarios in an upcoming series of blog posts:</p>
<ol style="list-style-type: decimal">
<li><p>Identifying adherence to the American Psychological Association’s
Journal Article Reporting Standards (JARS), such as reporting exact
<em>p</em>-values and including effect sizes alongside their
corresponding statistical test results;</p></li>
<li><p>Reviewing the content of data repositories, such listing if data
and/or code files are shared, and if the repository includes a README
file, code book, or data dictionary;</p></li>
<li><p>Extracting sample sizes from AsPredicted preregistrations and
checking the preregistered sample size for consistency with the final
study sample size reported in the manuscript;</p></li>
<li><p>Detecting the presence of a priori power analyses and checking if
they are fully reported to make them reproducible.</p></li>
</ol>
<p>These examples are intended to demonstrate the breadth of practices
that can be automatically checked. The automated checks can be performed
as part of an individual or metascientific workflow. In the individual
workflow, an author or editor can run selected modules on a single paper
and receive a report highlighting potential areas for improvement and
explanations or links to further resources. In the metascientific
workflow, a single module can be run on a batch of hundreds of papers,
producing tabular data that can be used to address metascientific
questions, such as how prevalent a practice is in a specific field. For
the individual workflow our philosophy is that error rates can be higher
than in the meta-scientific workflow, as the user will check whether to
implement each recommendation or not. For a tool to be used in a
metascientific workflow, the module needs to be shown to have low error
rates when run on manually coded ground truth files.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># run the module on 250 open access papers from Psychological Science</span></span>
<span><span class="va">rw</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/module_run.html">module_run</a></span><span class="op">(</span><span class="va">psychsci</span>, <span class="st">"retractionwatch"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">rw</span><span class="op">$</span><span class="va">summary</span>, <span class="va">rw_Retraction</span> <span class="op">&gt;</span> <span class="fl">0</span> <span class="op">|</span> <span class="va">rw_Correction</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##                  id rw_Correction rw_Retraction</span></span>
<span><span class="co">## 1 09567976221150616             1             0</span></span>
<span><span class="co">## 2 09567976231194221             1             0</span></span>
<span><span class="co">## 3 09567976241249183             0             1</span></span>
<span><span class="co">## 4 09567976241260247             3             0</span></span></code></pre>
<div class="section level2">
<h2 id="future-plans">Future Plans<a class="anchor" aria-label="anchor" href="#future-plans"></a>
</h2>
<p>There are more possible modules than our team can create, and we
welcome anyone who wants to collaborate with us on building and
validating new modules. In addition to the modules demonstrated in the
four blog posts, future modules could be developed to check if open data
and materials are Findable, Accessible, Interoperable, and Reusable
(FAIR) <span class="citation">(Wilkinson et al. 2016)</span>, to perform
checks on specific types of articles (e.g., meta-analyses), check
journal-specific reporting guidelines, or perform checks on the
references. We see great potential for new modules; the only limitation
is your own creativity and your engineering skills.</p>
<p>We are excited to continue developing Papercheck in close
collaboration with the broader scientific community and welcome
feedback, suggestions, and contributions. You can explore Papercheck on
GitHub: <a href="https://scienceverse.github.io/papercheck/" class="uri">https://scienceverse.github.io/papercheck/</a>. If you want
to contribute to new papercheck modules, validate existing modules in
your own scientific discipline, or explore the use of Papercheck for
metascientific projects, reach out to <a href="mailto:lisa.debruine@glasgow.ac.uk?subject=Papercheck">Lisa
DeBruine</a> or <a href="mailto:d.lakens@tue.nl?subject=Papercheck">Daniel Lakens</a>.</p>
</div>
<div class="section level2">
<h2 id="papercheck-team">Papercheck Team<a class="anchor" aria-label="anchor" href="#papercheck-team"></a>
</h2>
<p>Papercheck is developed by a collaborative team of researchers,
consisting of (from left to right in the picture below) <a href="https://debruine.github.io" class="external-link">Lisa DeBruine</a> (developer and
maintainer) and <a href="https://sites.google.com/site/lakens2/Home" class="external-link">Daniël Lakens</a>
(developer), <a href="https://research.vu.nl/en/persons/rene-bekkers" class="external-link">René Bekkers</a>
(collaborator and PI of Transparency Check), <a href="https://ssreplicationcentre.com/author/cristian-mesquida/" class="external-link">Cristian
Mesquida</a> (postdoctoral researcher), and Max Littel and Jakub Werner
(research assistants).</p>
<p><img src="papercheck_team.png" fig-alt="The faces of the papercheck team"></p>
<p>Papercheck is a component of a broader research project, Research
Transparency Check, led by René Bekkers, and funded by a <a href="https://tdcc.nl/tdcc-ssh-challenge-projects/research-transparency-check/" class="external-link">TDCC-SSH
grant</a> from the Dutch Research Council (NWO). Our funded project
priorities are described in more detail in our grant proposal, available
here: <a href="https://osf.io/cpv4d/" class="external-link uri">https://osf.io/cpv4d/</a>.</p>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-aczel2018quantifying" class="csl-entry">
Aczel, Balazs, Bence Palfi, Aba Szollosi, Marton Kovacs, Barnabas
Szaszi, Peter Szecsi, Mark Zrubka, Quentin F Gronau, Don van den Bergh,
and Eric-Jan Wagenmakers. 2018. <span>“Quantifying Support for the Null
Hypothesis in Psychology: An Empirical Investigation.”</span>
<em>Advances in Methods and Practices in Psychological Science</em> 1
(3): 357–66.
</div>
<div id="ref-van2024potential" class="csl-entry">
Akker, Olmo R van den, Marjan Bakker, Marcel ALM van Assen, Charlotte R
Pennington, Leone Verweij, Mahmoud M Elsherif, Aline Claesen, et al.
2024. <span>“The Potential of Preregistration in Psychology: Assessing
Preregistration Producibility and Preregistration-Study
Consistency.”</span> <em>Psychological Methods</em>.
</div>
<div id="ref-dexter2017narrative" class="csl-entry">
Dexter, Franklin, and Steven L Shafer. 2017. <span>“Narrative Review of
Statistical Reporting Checklists, Mandatory Statistical Editing, and
Rectifying Common Problems in the Reporting of Scientific
Articles.”</span> <em>Anesthesia &amp; Analgesia</em> 124 (3): 943–47.
</div>
<div id="ref-goos2024assessing" class="csl-entry">
Goos, Cas, Marjan Bakker, Jelte M Wicherts, and Michèle B Nuijten. 2024.
<span>“Assessing Reliable and Valid Measurement as a Prerequisite for
Informative Replications in Psychology.”</span>
</div>
<div id="ref-GROBID" class="csl-entry">
<span>“GROBID.”</span> 2008--2025. <a href="https://github.com/kermitt2/grobid" class="external-link uri">https://github.com/kermitt2/grobid</a>; GitHub.
</div>
<div id="ref-lakens_justification_2022" class="csl-entry">
Lakens, Daniël. 2022. <span>“Sample <span>Size
Justification</span>.”</span> Edited by Don van Ravenzwaaij.
<em>Collabra: Psychology</em> 8 (1): 33267. <a href="https://doi.org/10.1525/collabra.33267" class="external-link">https://doi.org/10.1525/collabra.33267</a>.
</div>
<div id="ref-sharpe_why_2013" class="csl-entry">
Sharpe, Donald. 2013. <span>“Why the Resistance to Statistical
Innovations? Bridging the Communication Gap.”</span> <em>Psychological
Methods</em> 18 (4): 572–82. <a href="https://doi.org/10.1037/a0034177" class="external-link">https://doi.org/10.1037/a0034177</a>.
</div>
<div id="ref-tendeiro2024diagnosing" class="csl-entry">
Tendeiro, Jorge N, Henk AL Kiers, Rink Hoekstra, Tsz Keung Wong, and
Richard D Morey. 2024. <span>“Diagnosing the Misuse of the Bayes Factor
in Applied Research.”</span> <em>Advances in Methods and Practices in
Psychological Science</em> 7 (1): 25152459231213371.
</div>
<div id="ref-thibault2024evaluation" class="csl-entry">
Thibault, Robert T, Emmanuel A Zavalis, Mario Malički, and Hugo Pedder.
2024. <span>“An Evaluation of Reproducibility and Errors in Published
Sample Size Calculations Performed Using g* Power.”</span>
<em>medRxiv</em>, 2024–07.
</div>
<div id="ref-wilkinson_fair" class="csl-entry">
Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,
Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.
2016. <span>“The <span>FAIR Guiding Principles</span> for Scientific
Data Management and Stewardship.”</span> <em>Scientific Data</em> 3 (1):
1–9. <a href="https://doi.org/10.1038/sdata.2016.18" class="external-link">https://doi.org/10.1038/sdata.2016.18</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Lisa DeBruine, Daniel Lakens.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
