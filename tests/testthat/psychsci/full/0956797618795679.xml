<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mind the Depth: Visual Perception of Shapes Is Better in Peripersonal Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elvio</forename><surname>Blini</surname></persName>
							<email>elvio.blini@inserm.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Integrative Multisensory Perception Action &amp; Cognition Team (ImpAct)</orgName>
								<orgName type="laboratory" key="lab2">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">Lyon Neuroscience Research Center (CRNL)</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country>France;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Lyon</orgName>
							</affiliation>
							<affiliation key="aff4">
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Farnè</surname></persName>
							<email>alessandro.farne@inserm.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Integrative Multisensory Perception Action &amp; Cognition Team (ImpAct)</orgName>
								<orgName type="laboratory" key="lab2">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">Lyon Neuroscience Research Center (CRNL)</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country>France;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Lyon</orgName>
							</affiliation>
							<affiliation key="aff4">
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Hospices Civils de Lyon</orgName>
								<orgName type="institution" key="instit2">Neuro-Immersion Platform</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clément</forename><surname>Desoche</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Hospices Civils de Lyon</orgName>
								<orgName type="institution" key="instit2">Neuro-Immersion Platform</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Romeo</forename><surname>Salemme</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Integrative Multisensory Perception Action &amp; Cognition Team (ImpAct)</orgName>
								<orgName type="laboratory" key="lab2">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">Lyon Neuroscience Research Center (CRNL)</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country>France;</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Hospices Civils de Lyon</orgName>
								<orgName type="institution" key="instit2">Neuro-Immersion Platform</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Kabil</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Hospices Civils de Lyon</orgName>
								<orgName type="institution" key="instit2">Neuro-Immersion Platform</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fadila</forename><surname>Hadj-Bouziane</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Integrative Multisensory Perception Action &amp; Cognition Team (ImpAct)</orgName>
								<orgName type="laboratory" key="lab2">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">Lyon Neuroscience Research Center (CRNL)</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country>France;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Lyon</orgName>
							</affiliation>
							<affiliation key="aff4">
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Lyon Neuroscience Research Center</orgName>
								<orgName type="laboratory">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">ImpAct Team</orgName>
								<address>
									<addrLine>16 Avenue du Doyen Jean Lépine</addrLine>
									<postCode>69500</postCode>
									<settlement>Bron</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Lyon Neuroscience Research Center</orgName>
								<orgName type="laboratory">INSERM U1028</orgName>
								<orgName type="institution" key="instit1">CNRS UMR5292</orgName>
								<orgName type="institution" key="instit2">ImpAct Team</orgName>
								<address>
									<addrLine>16 Avenue du Doyen Jean Lépine</addrLine>
									<postCode>69500</postCode>
									<settlement>Bron</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mind the Depth: Visual Perception of Shapes Is Better in Peripersonal Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9AF85287C9D72D34AAD8BD90BFE4CA08</idno>
					<idno type="DOI">10.1177/0956797618822990</idno>
					<note type="submission">Received 12/4/17; Revision accepted 7/4/18</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>peripersonal space</term>
					<term>depth</term>
					<term>multisensory integration</term>
					<term>perception</term>
					<term>visual streams</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We regret having omitted reference in our article to a study by <ref type="bibr" target="#b19">O'Connor, Meade, Carter, Rossiter, and Hester (2014)</ref>. The experiments we presented were designed by adapting the task O'Connor et al. originally used to test spatial sensitivity to reward. The following sentence is therefore being added to the beginning of the Materials and Apparatus section (p</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b28">Serino, Canzoneri, &amp; Avenanti, 2011;</ref><ref type="bibr" target="#b29">Teneggi, Canzoneri, di Pellegrino, &amp; Serino, 2013)</ref><p>, possibly coordinating automatic defensive behavior whenever necessary <ref type="bibr" target="#b12">(Graziano &amp; Cooke, 2006)</ref>. Furthermore, objects lying in proximity to the body might more often be candidates for manipulation, and thus, the enhanced PPS processing might reflect an attempt to maximize prehension efficiency or any voluntary action toward these objects <ref type="bibr" target="#b1">(Brozzoli, Ehrsson, &amp; Farnè, 2014;</ref><ref type="bibr" target="#b2">Brozzoli, Gentile, &amp; Ehrsson, 2012)</ref>. The functional linkage between PPS and actions, supported by neurophysiological and anatomical evidence from primate work (for a review, see <ref type="bibr" target="#b15">Makin, Holmes, Brozzoli, &amp; Farnè, 2012)</ref>, prompted the idea that visual processing in PPS would mainly rely on the dorsal visual stream, optimized for action, whereas visual processing beyond it, in extrapersonal space (EPS), would mainly rely on the ventral stream, optimized for perception <ref type="bibr" target="#b18">(Milner &amp; Goodale, 2008;</ref><ref type="bibr" target="#b21">Previc, 1990)</ref>.</p><p>This presupposed division of labor indicates that object detection would be more efficient for stimuli appearing close to the body, in light of the recruitment of parietal networks tapping on magnocellular processing <ref type="bibr" target="#b18">(Milner &amp; Goodale, 2008)</ref>. This has been generally confirmed <ref type="bibr" target="#b5">(de Gonzaga Gawryszewski et al., 1987;</ref><ref type="bibr" target="#b20">Plewan &amp; Rinkenauer, 2017)</ref>. In contrast, object discrimination would be more efficient for stimuli appearing far from the body, in light of the enhanced reliance on a ventral, parvocellular pathway <ref type="bibr" target="#b11">(Goodale &amp; Milner, 1992)</ref>. Because retinal size scales with physical distance, it appears sound to ascribe perceptual processing in EPS to a subset of neurons that present higher spatial resolution <ref type="bibr" target="#b11">(Goodale &amp; Milner, 1992)</ref>. However, to be appropriate, automatic defensive reactions to objects in the PPS require the brain to quickly discern whether objects are indeed harmful (e.g., bees) or not (e.g., ladybugs). Similarly, voluntary appetitive actions on objects in the PPS would require discriminating between the shapes of the objects. We therefore hypothesized that object discrimination may also benefit from PPS processing. To date, whether object-discrimination abilities are superior in PPS or EPS remains unanswered.</p><p>Here, we capitalized on immersive virtual environments that, compared with 2-D settings, provide clear depth percepts. We presented geometric shapes either close (50 cm, within PPS) or far (300 cm, in EPS) from healthy volunteers engaged in a shape-discrimination task (depth was thus irrelevant and orthogonal to the task at hand). Our aims were (a) to compare discrimination abilities in PPS and EPS when retinal-size scaling is artificially teased apart, (b) to explore the determinants of any depth-related difference (i.e., perspective vs. binocular cues), and (c) to model the spatial distribution of discrimination abilities in depth.</p><p>In the first experiment, we found that discrimination abilities are superior for stimuli presented in PPS compared with stimuli presented outside PPS, despite far stimuli having the same retinal size (thus looking bigger). In Experiment 2, we found that this advantage persists in a 2-D setting exploiting perspective cues (i.e., in the context of the Ponzo illusion), thus showing that binocular depth cues are not necessary in order to highlight an advantage for PPS. Experiment 3 further replicated results from the first experiment, ruling out a potential confound related to upper/lower visual field covariance with depth-that is, stimuli were presented at the same height (at fixation). In Experiment 4, retinal size was naturally scaled as a function of distance, allowing us to estimate the typical strength of the PPS advantage in more ecological settings. Finally, in Experiment 5, we presented shapes at six different distances and found that benefits over performance follow a sigmoidal trend, closely mirroring that found in studies using multisensory integration to probe PPS boundaries <ref type="bibr" target="#b4">(Canzoneri et al., 2012;</ref><ref type="bibr" target="#b6">Ferri et al., 2015;</ref><ref type="bibr" target="#b29">Teneggi et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Participants were healthy volunteers who were enrolled in the study after we obtained informed written consent. They were all students of the University Claude Bernard of Lyon, were recruited through web advertising, and were paid for their participation. None of the participants had a history of neurologic or psychiatric disorders, and the vision of all participants was normal or corrected to normal.</p><p>We had no prior beliefs or pilot data to estimate a realistic effect size. We recruited 20 participants for Experiment 1 because this number reflects the average sample size in similar PPS studies. Once results were obtained, a power analysis (paired-samples t test, Cohen's d = 0.6, α = .05, one-tailed) indicated a minimum of 19 participants to reach a power of .8 (the effect size from Experiment 1 was computed as if reflecting a between-participants design, and thus, this power analysis revealed itself to be conservative). About 20 participants were thus enrolled for each of the following experiments, except for Experiment 2, which was performed concurrently with a parallel experiment that required a larger sample size. The recruitment was made independently for each of the five experiments, but recruitments for Experiments 3 and 4 were made in parallel, and a few participants completed both experiments; those participants always performed Experiment 3 before Experiment 4. In no case were optional stopping procedures applied; the experiments ended either because the prespecified number of participants was reached or (in Experiment 2) because other experiments running in parallel stopped as well. Thus, the significance of the results was never considered as a criterion to stop or continue data collection. A summary of demographic information for each experiment is reported in</p><p>Table 1. The study followed the Declaration of Helsinki standards and was approved by the Institut National de la Santé et de la Recherche Médicale (INSERM) Ethics Committee (IRB00003888, No. 16-341).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and apparatus</head><p>We adapted the task designed by O'Connor, <ref type="bibr" target="#b19">Meade, Carter, Rossiter, and Hester (2014)</ref>, which was originally employed to test spatial sensitivity to reward, reported to be reduced in far relative to near space. In Experiments 1, 3, 4, and 5, participants wore a virtual-reality headset (Oculus Rift; <ref type="url" target="https://www.oculus.com">https://www.oculus.com</ref>). The experiments were implemented within Unity (Version 5.1.2; Unity Technologies, San Francisco, CA) and Oculus Runtime (Version 0.6; Facebook Technologies Ireland, Dublin, Ireland) software, which were used to create the virtual environment, display experimental stimuli on the head-mounted display, and record participants' responses. The experiments were run on a computer with an Intel Core i7 processor, AMD Fire Pro M6000 graphics card, and Windows 7 operating system. The scene was rendered in Oculus Rift DK2 software, with a resolution of 960 × 1,080 per eye, a frequency of 75 Hz, and a field of view equal to 106°.</p><p>In Experiment 2, participants faced a 15-in. screen at a distance of approximately 57 cm. The open-source software OpenSesame (<ref type="url" target="http://osdoc.cogsci.nl/">http://osdoc.cogsci.nl/</ref>) was used to display experimental stimuli and record participants' responses. Stimuli were obtained with professional designing software (SolidWorks; Dassault Systèmes, Waltham, MA). The rendering of an empty room was designed to introduce depth cues by exploiting a Ponzo-like illusion. A very similar empty room was also created and presented as a virtual environment in <ref type="table" target="#tab_1">Experiments 1</ref>, <ref type="table" target="#tab_5">3</ref>, <ref type="table">4</ref>, and <ref type="table">5.</ref> Across all experiments, we obtained different distance conditions by presenting red, green, or blue shapes (cubes or spheres) at different positions. Shapes were presented close to (50 cm) or far from (300 cm) the observer in the virtual environment. Note that only close shapes were within reachable distance. In Experiment 2, shapes were presented in either the bottom or upper part of the grid, providing 2-D perspective cues; thus, shapes presented in the bottom of the grid were illusorily perceived to be closer to participants. Finally, in Experiment 5, shapes were presented at six equidistant points, ranging from 50 to 300 cm.</p><p>The retinal size of the shapes (≈14° of visual angle in the 3-D experiments, ≈2.2° in the 2-D experiment) was kept constant across distances and shapes, resulting in the more distant shapes being larger (Experiments 1 and 3) or appearing illusorily larger because of the perspective (Experiment 2). In Experiments 4 and 5, retinal size was naturally scaled: Farther shapes had the same real dimensions as closer ones, and thus retinal size was smaller.</p><p>In Experiment 1, closer shapes appeared in the bottom part of the visual field (below the fixation cross), and farther ones appeared in the upper visual field. In Experiment 2, the Ponzo-like illusion display imposed the same up-down arrangement by design (to allow a proper depth illusion). We ruled out this potential confound in Experiments 3, 4, and 5, in which all shapes were presented at the same height as the fixation cross. For all experiments, a further rendering included a cross, which was used as a fixation point across all trials. The position of the cross was midway between close and distant shapes (175 cm). Participants provided responses to object shape by means of keyboard presses (B and N keys on a standard QWERTY keyboard) using the index and middle fingers of their dominant hand. Figure <ref type="figure" target="#fig_0">1</ref> depicts the main features manipulated in each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants sat in a dark, quiet room, with their head restrained by a chin rest. Each trial was composed of a first fixation phase (500 ms), followed by the presentation of a stimulus randomly chosen among the combination of shape (cube or sphere), color (red, green, blue), and distance (close or far). Stimuli were presented up to a maximum of 750 ms and were replaced by feedback (text presented for 1,000 ms) as soon as a response was provided. Participants were told that responses slower than 500 ms and faster than 100 ms would be discarded, to discourage anticipations or slow responses; they were asked to respond as quickly and accurately as possible using their index finger to indicate a cube and middle finger for a sphere if responding with their rightdominant hand (the opposite finger assignment was given to left-handers). In our design, distance was therefore irrelevant to the task and orthogonal to the response. All participants underwent a brief 24-trial practice block before starting the experiment, which consisted of another four blocks of trials. In Experiments 2 to 4, there were 60 trials each (240 trials overall). In Experiment 5, each of the four blocks was composed of 108 trials (432 overall). In Experiment 1, the whole procedure was repeated twice (i.e., four blocks of 60 trials each × 2), with a postural manipulation defining the two sessions: We asked participants to place their unseen nondominant hand in two different positions, namely, close to the chin rest (about 10 cm from their body) or farther away (roughly 50 cm from their body and therefore close to where the near virtual shape was presented). The order for hand position was counterbalanced across participants. We dropped this factor in the subsequent experiments because it had no effect on performance. The hand was therefore kept at about 10 cm from the body in the subsequent experiments.</p><p>About halfway through and after each experiment exploiting virtual reality (i.e., Experiments 1, 3, 4, and 5), we asked participants whether they had perceived two different distances and then to provide an approximate estimation for each of them. We used estimated distances given after the experiment (to allow adjustments after the initial response) to check for the presence of an effective depth perception. Several authors (for a review, see <ref type="bibr" target="#b24">Renner, Velichkovsky, &amp; Helmert, 2013)</ref> have found that explicit distance judgments are often underestimated by up to about 75% of the intended depths. Although here we probed the effect of distance implicitly, as it was task irrelevant, we use the labels "close" and "far" throughout the text and refrain from linearly mapping unities of the virtual environment to real distances.</p><p>The raw data, the full analysis pipeline, and additional graphical depictions for all experiments can be found in the Supplemental Material available online. Data, excluding practice trials, were analyzed with the opensource software R (R Core <ref type="bibr" target="#b23">Team, 2008)</ref>. Accuracy and response times (the latter for responses that were both accurate and given within the window of 100-500 ms)</p><p>Experiment 1 Close Far Experiment 2 Close Far (Illusorily) Experiment 3 Close Far Experiment 4 Close Far Experiment 5 D1 D2 D3 D4 D5 D6</p><p>Close Far exploited a 3-D virtual-reality setting. Shapes were presented at the fixation level, and their position on the transverse axis was kept constant, but retinal size varied, being naturally scaled as a function of distance. Experiment 5 exploited a 3-D virtual-reality setting. Shapes were presented at the fixation level and at six different distances (50, 100, 150, 200, 250, and 300 cm, labeled D1 to D6 here). Retinal size was scaled as a function of distance. (continued) were analyzed through mixed-effects multiple regression models <ref type="bibr" target="#b0">(Baayen, Davidson, &amp; Bates, 2008)</ref>. A great advantage of mixed-effects models is that they are based on single-trial data (rather than on averaged data), they do not assume independence among observations, and the model-fitting procedure takes into account the covariance structure of the data, including random effects (i.e., individual variability). Models had a logistic link function, appropriate for binary variables, when assessing accuracy.</p><p>As a first step, we defined a model containing the random effects. Linear mixed models generalize best when one includes the most complex random structure that does not prevent model convergence <ref type="bibr" target="#b17">(Matuschek, Kliegl, Vasishth, Baayen, &amp; Bates, 2017)</ref>. Random effects were introduced sequentially, and their effect on model fit was assessed using likelihood tests (i.e., we compared the residuals of each model and chose the one with significantly lower deviance as assessed by a chisquare test). A random intercept for participant was included in all models. We then tested the contribution of random slopes for distance, hand position (Experiment 1 only), color of the presented shape, and shape. The latter variable (i.e., the presented shape, cube, or sphere) also indicates the response effector (i.e., index or middle finger), as contingencies were blocked for each participant, and thus indexes differences in discrimination performance of cubes compared with spheres and of responses with one effector over another. Finally, we also tested n-way interactions of random slopes that were previously retained in the models.</p><p>The models with the final random-effects structure were then used to evaluate the role of fixed effects. We used a stepwise Type 2 approach and likelihood tests to assess whether the improvements in model fit were statistically significant. Parametric bootstrapping was used to obtain 95% confidence intervals (CIs) for the beta coefficients and thus to evaluate the distribution of estimated mean differences between the levels of a factor. Additional analyses (e.g., analyses of variance, t tests) were also performed and are reported in the Supplemental Material in the Robustness Checks sections. All the robustness checks fully confirmed the results from the main inferential approach.</p><p>In Experiment 5, we explicitly required models to have only a random slope and fixed effect for distance. This allows obtaining, for each participant, estimates of the performance that are weighted by the random effects themselves and by the participant-specific and group-specific variances (e.g., noise; <ref type="bibr" target="#b0">Baayen et al., 2008)</ref>. We used such random slopes as dependent variables and evaluated which curve (among linear, logarithmic, exponential, and sigmoidal) best described their relationship with depth (the independent variable). The models' formulas are reported in Table <ref type="table" target="#tab_3">2</ref>. Nonlinear least-squares estimations were obtained using the nls() function in R, and goodness of fit was evaluated by means of both root-mean-square error (RMSE) and the Akaike information criterion (AIC). The first is a measure of dispersion of residuals, whereas the latter is best used for model comparison and accounts for both goodness of fit and complexity of the models. Because the fourth model (sigmoidal) included two more parameters, the AIC introduced a more severe penalization aimed at decreasing the chances of overfitting noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>Preliminary selection of random effects. The null models included random slopes for hand position and shape when accuracy was assessed. The best matrix of random effects for response times was more complex because it included a further random slope for distance and the Distance × Shape interaction. We used these specifications to test the contribution of fixed effects through a chi-square test for goodness of fit.</p><p>Accuracy. Neither distance, χ 2 (1, N = 20) = 0.04, p = .84, nor hand position, χ 2 (1, N = 20) = 2.7, p = .10, improved model fit. In addition, fit did not improve when the Distance × Hand Position interaction (and main effects) was tested against the model including only the two main effects, χ 2 (1, N = 20) = 1, p = .315. Thus, none of our manipulations, or the interaction, had substantial effects on the odds of producing an accurate response. Indeed, accuracy was quite high for both the close position (M = 89%, SD = 7.88%) and the far position (M = 89%, SD = 8.47%).</p><p>Response times. Response times were considered for accurate and fast (&lt; 500 ms) responses only, and 82.5% of the observations met this prespecified criterion (close: M = 83.3%, SD = 10.5%; far: M = 81.75%, SD = 10.3%). Response times markedly differed across viewing  Discussion. In this experiment, visual stimuli were presented in an immersive 3-D setting using a virtual-reality headset. Despite the retinal size of different shapes being kept constant, and the farther ones being-and appearing-much bigger, we observed a response advantage to objects presented in PPS, even if they looked smaller (see Fig. <ref type="figure" target="#fig_3">2</ref>). Whether participants placed their unseen nondominant hand close to, or far from, the more proximal virtual shape had no role in modulating the distance effect. This suggests that when only proprioception is available, the shape-discrimination advantage in PPS is not hand centered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>Preliminary selection of random effects. No random slope improved model fit when accuracy was assessed; random slopes for distance and shape, together with their interaction, were selected when the role of fixed effects over response times was assessed.</p><p>Accuracy. Accuracy was high for both the close condition (M = 93.8%, SD = 5.3%) and far condition (M = 94.4%, SD = 4%). Distance, χ 2 (1, N = 32) = 1.52, p = .217, did not improve model fit. Thus, it had no effect on the odds of producing an accurate response.</p><p>Response times. Response times were considered for accurate and fast (&lt; 500 ms) responses only, and 83% of the observations met this prespecified criterion (close: M = 82.8%, SD = 14%; far: M = 83.3%, SD = 11.4%). Adding distance as a main effect improved model fit, χ 2 (1, N = 32) = 6.11, p = .0134, Cohen's d = 0.68, 95% CI = [0.17 Discussion. The perception of depth allowed by the virtual-reality headset is due to both binocular cues (ocular disparity) and related ocular vergence, as well as to perspective cues. To isolate the role played by perspective cues in this experiment, we presented stimuli on a 2-D screen, using the rendering of an empty room as a background (Ponzo illusion). We still observed the advantage for shapes that appeared-illusorily-closer to participants, indicating that perspective cues alone are sufficient for the PPS advantage to emerge (see Fig. <ref type="figure" target="#fig_3">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3</head><p>Preliminary selection of random effects. No random slope improved model fit when accuracy was assessed; a random slope for shape was instead introduced when the role of fixed effects over response times was assessed.</p><p>Accuracy. Accuracy was high for both the close position (M = 92.9%, SD = 4.4%) and far position (M = 93%, SD = 4.6%). Distance, χ 2 (1, N = 21) = 0.01, p = .911, did not improve model fit. Thus, it had no effect on the odds of producing an accurate response.</p><p>Response times. Response times were considered for accurate and fast (&lt; 500 ms) responses only, and 85.8% of the observations met this prespecified criterion (close: M = 86%, SD = 9%; far: M = 85.6%, SD = 10.5%). Adding distance as a main effect improved model fit, χ 2 (1, N = 21) = 10.17 Discussion. In both Experiments 1 and 2, depth covaried with the height of stimuli in the visual field, such as in ecological situations in which closer objects usually appear in the lower hemifield <ref type="bibr" target="#b21">(Previc, 1990)</ref>. Nevertheless, even when shapes were presented along the same gaze line in Experiment 3 (and hence, such a potential confound was ruled out), the advantage for stimuli in PPS was confirmed (see Fig. <ref type="figure" target="#fig_3">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 4</head><p>Preliminary selection of random effects. The random slope for distance improved model fit when accuracy was assessed; a further random slope for shape, together with its interaction term with distance, was included when response times were assessed. We used these specifications to test the contribution of fixed effects through a chi-square test for goodness of fit.</p><p>Accuracy. Accuracy was high for the close position (M = 89.2%, SD = 5.6%) and slightly, but not significantly, lower for the far position (M = 86.6%, SD = 8%). Distance, χ 2 (1, N = 21) = 3.38, p = .066, did not improve model fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>times. Response times were considered for accurate and fast (&lt; 500 ms) responses only, and 78.4% of the observations met this prespecified criterion (close: M = 82.8%, SD = 13.2%; far: M = 74%, SD = 16.5%). Adding distance as a main effect improved model fit, χ 2 (1, N = 21) = 28.9, p &lt; .001, Cohen's d = 1.71, 95% CI = [0.99, 2.44]. Response times were faster for close objects (M = 375.8 ms, SD = 21) compared with far objects (M = 397.9 ms, SD = 18.6), β = 20.76, SE = 2.33, 95% CI = [16.01, 25.8]. Results, depicted in Figure 2, were confirmed by a two-tailed t test for dependent samples, t(20) = 7.86, p &lt; .001. Far shapes were, on average, discriminated more slowly than close shapes (mean difference = 22.05 ms, 95% CI = [16.2, 27.91]).</p><p>Discussion. This experiment, performed in ecologically veridical conditions in which farther objects appeared smaller than closer ones, demonstrates that the natural distance scaling of size substantially enhances the PPS advantage (see Fig. <ref type="figure" target="#fig_3">2</ref>). As in the previous experiments, results cannot be ascribed to speed/accuracy trade-offs.</p><p>Experiment 5</p><p>Psychophysical modeling. Random slopes (for both accuracy and response times) were fitted for each participant and for the group average to four different equations (see Table <ref type="table" target="#tab_3">2</ref>). At the group level, a sigmoidal trend emerged when we assessed both accuracy (sigmoidal AIC = -8.5; exponential AIC = -7.94) and response times (sigmoidal AIC = 36.44; exponential AIC = 36.95; linear AIC = 37.37). At the individual participant level, the sigmoidal trend obtained the best performance for all participants and for both response times and accuracy when using the RMSE as an index of goodness of fit. The AIC was less conclusive. When response times were assessed, the AIC still favored the sigmoidal trend for 11 participants out of 20, but for the remaining participants, the exponential curve was preferred. The results when fitting accuracy were similar, but the exponential curve was favored for 11 participants; of the remaining participants, 8 showed a sigmoidal trend, and only 1 showed a logarithmic trend. Results are summarized in Table <ref type="table" target="#tab_5">3</ref>.</p><p>Discussion. To model the spatial tuning of shape discrimination as a function of depth in Experiment 5, we presented shapes, not corrected for retinal size, at six different, equidistant points ranging from 50 cm to 300 cm. The fit to empirical data for several theoretical curves (sigmoidal, linear, logarithmic, and exponential) was then contrasted. A sigmoidal trend emerged at the group level when we assessed both accuracy and response times (see Fig. <ref type="figure" target="#fig_5">3</ref>). Thus, the PPS advantage follows a sigmoidal trend, similar to what is commonly observed in studies using multisensory integration paradigms to assess the PPS boundary (e.g., <ref type="bibr" target="#b4">Canzoneri et al., 2012;</ref><ref type="bibr" target="#b6">Ferri et al., 2015;</ref><ref type="bibr" target="#b29">Teneggi et al., 2013)</ref>, except that here, only the visual modality was involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Throughout the same discrimination task, the features of different visual shapes were progressively stripped of important depth cues:  ventral/dorsal dichotomy alone, although extensively supported by physiological and neuropsychological studies, cannot readily account for the PPS-dependent advantage in visual shape discrimination. It is beyond dispute that this dichotomy is not so strict <ref type="bibr" target="#b18">(Milner &amp; Goodale, 2008;</ref><ref type="bibr" target="#b32">Zachariou et al., 2015)</ref>, and the dorsal pathway contains object representations that are, to some extent, independent from ventral ones <ref type="bibr" target="#b8">(Freud, Culham, Plaut, &amp; Behrmann, 2017;</ref><ref type="bibr" target="#b9">Freud, Ganel, et al., 2017;</ref><ref type="bibr" target="#b10">Freud, Plaut, &amp; Behrmann, 2016;</ref><ref type="bibr" target="#b22">Quinlan &amp; Culham, 2007;</ref><ref type="bibr" target="#b31">Wang, Li, Zhang, &amp; Chen, 2016</ref>) and might contribute to perception. Additional candidate regions are a set of inferior parietal and premotor areas <ref type="bibr" target="#b3">(Brozzoli et al., 2011;</ref><ref type="bibr" target="#b7">Fogassi et al., 1996;</ref><ref type="bibr" target="#b12">Graziano &amp; Cooke, 2006;</ref><ref type="bibr" target="#b26">Rizzolatti et al., 1983)</ref> that are known to preferentially respond to stimuli presented in PPS. The latter neural network, which also includes the putamen <ref type="bibr" target="#b13">(Graziano &amp; Gross, 1993)</ref>, contains a majority of neurons with bimodal (i.e., visual and tactile) receptive fields coding for PPS <ref type="bibr" target="#b3">(Brozzoli et al., 2011;</ref><ref type="bibr" target="#b7">Fogassi et al., 1996)</ref>, together with unimodal (visual) neurons. This network seems thus ideally suited to subserve the advantage in discriminating close versus far objects reported here.</p><p>Whereas future studies may tease apart the contribution of unisensory versus multisensory neurons in driving this advantage for PPS, here we disclose that depth per se, even when completely irrelevant for the situation at hand, helps to determine people's visual perception of shapes, independently of physical size. In addition, we found that the sigmoidal performance curve, considered the fingerprint of the multisensorydefined boundary of PPS, can actually also be found for merely unimodal visual stimuli. The visual modality alone, therefore, can capture functional features of PPS that were previously thought to be exquisitely multisensory. Although we cannot state, at present, the extent to which visual and multisensory PPSs overlap, these findings open up new considerations in the evergrowing field of multisensory research: The convergence of multiple senses might not be a necessary feature to explain behavioral advantages in close space or even to probe PPS. We thus urge researchers conducting future studies to be depth aware, to better frame human visual abilities that are not homogeneously distributed in the three dimensions of the space around us.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The main features of each experiment. Experiment 1 exploited a 3-D virtual-reality setting. Shapes were presented close to (50 cm) or far away from (300 cm) participants, below the fixation cross; this resulted in close shapes always being perceived to be lower than farther ones. Retinal size was kept constant. The proprioceptive input coming from the position of the hand was manipulated to be close to or far from the closer shape. Experiment 2 exploited a Ponzo illusion in a 2-D display. Shapes were presented in the lower (close) or upper (far) visual field. Retinal size was kept constant. Experiment 3 exploited a 3-D virtual-reality setting. Unlike in Experiment 1, shapes were presented at the fixation level, and their position on the transverse axis and retinal size were kept constant. Experiment 4</figDesc><graphic coords="5,52.85,488.58,58.34,93.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (continued)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>2 (1, N = 20) = 17.52, p &lt; .001, Cohen's d = 1.17, 95% CI = [0.48, 1.86] (see Fig. 2), participants being faster in categorization when shapes appeared close (M = 365.8 ms, SD = 15.06) rather than far (M = 375.4 ms, SD = 15.76), β = 13.24, SE = 1.65, 95% CI = [9.5, 16.5]. We observed no main effect of hand position, χ 2 (1, N = 20) = 1.05, p = .30, Cohen's d = -0.16, 95% CI = [-0.8, 0.48], or the Distance × Hand Position interaction, χ 2 (1, N = 20) = 0.1, p = .753. In other words, results point toward the presence of a clear advantage for stimuli appearing in PPS, whereas the proprioceptive information coming from the hands exerted no main or modulatory effects.Results were confirmed by a two-way analysis of variance, which yielded a main effect of distance, F(1, 19) = 27.34, p &lt; .001, η p 2 = .59, but no effects of hand position, F(1, 19) = 0.54, p = .47, η p 2 = .028. The interaction between the two was not significant, F(1, 19) = 0.03, p = .858, η p 2 = .002. Far shapes were, on average, discriminated more slowly than close shapes (mean difference = 9.62 ms, 95% CI = [5.77, 13.47]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results from Experiments 1 to 4: box-and-whisker plots depicting the mean gain in response time as a function of distance (interindividual variability of the peripersonal-space advantage, calculated by subtracting response times to close objects from response times to far objects, in ms). In each plot, the vertical length of the box represents the interquartile range, the thick horizontal line represents the median, and the whiskers indicate the full range of values. Dots outside the whiskers represent values exceeding 1.5 times the interquartile range.</figDesc><graphic coords="7,62.36,193.75,52.94,81.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>, p = .001, Cohen's d = 0.72, 95% CI = [0.08, 1.37]. The response times were faster for close objects (M = 371.6 ms, SD = 18.33) compared with far objects (M = 376.7 ms, SD = 20.0), β = 5.0, SE = 1.57, 95% CI = [1.92, 8.05]. Results, depicted in Figure2, were confirmed by a two-tailed t test for dependent samples, t(20) = 3.34, p = .003. Far shapes were, on average, discriminated more slowly than close shapes (mean difference = 5.15 ms, 95% CI = [1.93, 8.37]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results from Experiment 5, in which we presented shapes at six different depths. Group-wise predicted sigmoidal curves are shown for mean accuracy (left panel) and mean response time (RT) advantage (right panel) as a function of distance (labeled here from D1, close, to D6, far). Error bars show standard errors of the mean. The y-axes refer to the odds of providing a correct response (accuracy) and the relative RT advantage observed with respect to participant-specific mean performance.</figDesc><graphic coords="9,67.85,493.88,226.82,162.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Demographic Information for the Five Experiments</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Age (years)</cell></row><row><cell>Experiment</cell><cell>Sample size</cell><cell>Left-handed (n)</cell><cell>M</cell><cell>SD</cell></row><row><cell>1</cell><cell>20 (10 female)</cell><cell>2</cell><cell cols="2">23.4 3.13</cell></row><row><cell>2</cell><cell>32 (16 female)</cell><cell>2</cell><cell cols="2">21.8 2.52</cell></row><row><cell>3</cell><cell>21 (10 female)</cell><cell>6</cell><cell cols="2">23.9 2.06</cell></row><row><cell>4</cell><cell>21 (11 female)</cell><cell>6</cell><cell cols="2">24.6 2.58</cell></row><row><cell>5</cell><cell>20 (10 female)</cell><cell>0</cell><cell>24</cell><cell>3.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Models Contrasted in Experiment 5</figDesc><table><row><cell>Curve</cell><cell>Equation</cell></row><row><cell>Linear</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Results From Experiment 5 : The table gives values for group means, fitted with the relative equations. The number of participants who favored each model is reported in parentheses.</figDesc><table><row><cell></cell><cell>Root-mean-square</cell><cell>Akaike information</cell></row><row><cell>Measure and curve</cell><cell>error (RMSE)</cell><cell>criterion (AIC)</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell></row><row><cell>Linear</cell><cell>0.13 (n = 0)</cell><cell>-1.72 (n = 0)</cell></row><row><cell>Logarithmic</cell><cell>0.21 (n = 0)</cell><cell>4.28 (n = 1)</cell></row><row><cell>Exponential</cell><cell>0.08 (n = 0)</cell><cell>-7.94 (n = 11)</cell></row><row><cell>Sigmoidal</cell><cell>0.05 (n = 20)</cell><cell>-8.5 (n = 8)</cell></row><row><cell>Response time</cell><cell></cell><cell></cell></row><row><cell>Linear</cell><cell>3.3 (n = 0)</cell><cell>37.37 (n = 0)</cell></row><row><cell>Logarithmic</cell><cell>5.6 (n = 0)</cell><cell>43.69 (n = 0)</cell></row><row><cell>Exponential</cell><cell>3.19 (n = 0)</cell><cell>36.95 (n = 9)</cell></row><row><cell>Sigmoidal</cell><cell>2.19 (n = 20)</cell><cell>36.44 (n = 11)</cell></row></table><note><p>Note</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Action Editor</head><p><rs type="person">Philippe G. Schyns</rs> served as action editor for this article.</p></div>
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Pascal Mamassian</rs> for constructive feedback in an early stage of this study.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>F. Hadj-Bouziane and A. Farnè contributed equally to this study. A. Farnè, F. Hadj-Bouziane, and E. Blini designed the experiments. C. Desoche, A. Kabil, and R. Salemme programmed the tasks. E. Blini collected the data, performed statistical analyses, and wrote the first draft of the manuscript.</p><p>All the authors discussed and reviewed the manuscript and approved the final version for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/0956797618795679">http://  journals.sagepub.com/doi/suppl/10.1177/0956797618795679</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices</head><p>This study was not preregistered. The raw data, the full analysis pipeline, and additional graphical depictions of the experiments and results are available in the Supplemental Material available online.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixedeffects modeling with crossed random effects for subjects and items</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2007.12.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multisensory representation of the space near the hand: From perception to action and interindividual interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brozzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ehrsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farnè</surname></persName>
		</author>
		<idno type="DOI">10.1177/1073858413511153</idno>
	</analytic>
	<monogr>
		<title level="j">The Neuroscientist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="122" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">That&apos;s near my hand! Parietal and premotor coding of hand-centered space contributes to localization and self-attribution of the hand</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brozzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ehrsson</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2660-12.2012</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14573" to="14582" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">fMRI adaptation reveals a cortical mechanism for the coding of space near the hand</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brozzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ehrsson</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1172-11.2011</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9023" to="9031" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic sounds capture the boundaries of peripersonal space representation in humans</title>
		<author>
			<persName><forename type="first">E</forename><surname>Canzoneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Magosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serino</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0044306</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">44306</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Movements of attention in the three spatial dimensions and the meaning of &quot;neutral&quot; cues</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Gonzaga Gawryszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Umiltá</surname></persName>
		</author>
		<idno type="DOI">10.1016/0028-3932(87)90040-6</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion-inducing approaching sounds shape the boundaries of multisensory peripersonal space</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tajadura-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Väljamäe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vastano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Costantini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2015.03.001</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="468" to="475" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coding of peripersonal space in inferior premotor cortex (area F4)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fogassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luppino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="141" to="157" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The large-scale organization of shape processing in the ventral and dorsal pathways</title>
		<author>
			<persName><forename type="first">E</forename><surname>Freud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Culham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrmann</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.27576</idno>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27576</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three-dimensional representations of objects in dorsal cortex are dissociable from those in ventral cortex</title>
		<author>
			<persName><forename type="first">E</forename><surname>Freud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ganel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shelef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrmann</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhv229</idno>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="422" to="434" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What&apos; is happening in the dorsal visual pathway</title>
		<author>
			<persName><forename type="first">E</forename><surname>Freud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.08.003</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="773" to="784" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Milner</surname></persName>
		</author>
		<idno type="DOI">10.1016/0166-2236(92)90344-8</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parieto-frontal interactions, personal space, and defensive behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Graziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Cooke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2005.09.009</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="845" to="859" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bimodal map of space: Somatosensory receptive fields in the macaque putamen with corresponding visual receptive fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Graziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Gross</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00228820</idno>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="96" to="109" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Functional and dynamic properties of visual peripersonal space</title>
		<author>
			<persName><forename type="first">E</forename><surname>Làdavas</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1364-6613(00)01814-3</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="17" to="22" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Keeping the world at hand: Rapid visuomotor processing for hand-object interactions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Makin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brozzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farnè</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00221-012-3089-5</idno>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="421" to="428" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coding of visual space during motor preparation: Approaching objects rapidly modulate corticospinal excitability in hand-centered coordinates</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Makin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brozzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farnè</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2955-09.2009</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="11841" to="11851" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Balancing Type I error and power in linear mixed models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matuschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2017.01.001</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two visual systems re-viewed</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2007.10.005</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="774" to="785" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Behavioral sensitivity to reward is reduced for far objects</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rossiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hester</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797613503663</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="271" to="277" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple reaction time and size-distance integration in virtual 3D space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Plewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rinkenauer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-016-0769-y</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="653" to="663" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Functional specialization in the lower and upper visual fields in humans: Its ecological origins and neurophysiological implications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Previc</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X00080018</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral &amp; Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="519" to="542" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">fMRI reveals a preference for near viewing in the human parieto-occipital cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Culham</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2007.02.029</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="167" to="187" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="http://www.R-project.org" />
		<title level="m">R: A language and environment for statistical computing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The perception of egocentric distances in virtual environments-A review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Velichkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Helmert</surname></persName>
		</author>
		<idno type="DOI">10.1145/2543581.2543590</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The space around us</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fogassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.277.5323.190</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="page" from="190" to="191" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deficits in attention and movement following the removal of postarcuate (area 6) and prearcuate (area 8) cortex in macaque monkeys</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pavesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="655" to="673" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">To blink or not to blink: Fine cognitive tuning of the defensive peripersonal space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Sambo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Iannetti</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.0607-12.2012</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12921" to="12927" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fronto-parietal areas necessary for a multisensory representation of peripersonal space in humans: An rTMS study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Serino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Canzoneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avenanti</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_00006</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2956" to="2967" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Social modulation of peripersonal space boundaries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Teneggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Canzoneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Di Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serino</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2013.01.043</idno>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="406" to="411" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth: The forgotten dimension in multisensory research</title>
		<author>
			<persName><forename type="first">N</forename><surname>Van Der Stoep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farnè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.1163/22134808-00002525</idno>
	</analytic>
	<monogr>
		<title level="j">Multisensory Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="493" to="524" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The role of parieto-occipital junction in the interaction between dorsal and ventral streams in disparity-defined near and far space processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0151838</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">151838</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Common dorsal stream substrates for the mapping of surface texture to object parts and visual spatial processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zachariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Nikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Safiullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_00871</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2442" to="2461" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
